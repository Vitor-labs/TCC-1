{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.util import load_logs\n",
    "from numpy import where\n",
    "from pandas import DataFrame, Series, concat, read_csv\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "MIN_SAMPLES: Final[int] = 0\n",
    "LOGS_PATH: Final[str] = \"../reports/logs_sequence.log\"\n",
    "logger = JSONLogger(path=LOGS_PATH, reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = read_csv(\"../data/PAMAP2/x_train_data.csv\")\n",
    "X_test = read_csv(\"../data/PAMAP2/x_test_data.csv\")\n",
    "y_train = read_csv(\"../data/PAMAP2/y_train_data.csv\")\n",
    "y_test = read_csv(\"../data/PAMAP2/y_test_data.csv\")\n",
    "\n",
    "X_train[\"activity\"] = y_train  # First 80% of the data\n",
    "X_test[\"activity\"] = y_test  # Last 20% of the data\n",
    "\n",
    "# MIN_SAMPLES = X_train[\"activity\"].value_counts().min()\n",
    "MIN_SAMPLES = X_train[\"activity\"].value_counts().sort_values().iloc[3]\n",
    "\n",
    "models: dict[int, dict] = {}\n",
    "training_data: DataFrame\n",
    "testing_data: DataFrame\n",
    "train_targets: Series\n",
    "test_targets: Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(nu: float, gamma: float) -> float:\n",
    "    \"\"\"\n",
    "    Objective function to optimize F1-Score on the test set.\n",
    "\n",
    "    Args:\n",
    "        nu (float): nu param to evaluate.\n",
    "        gamma (float): gamma param to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: F1-Score on the test set of this iteration.\n",
    "    \"\"\"\n",
    "    oc_svm = OneClassSVM(kernel=\"rbf\", nu=nu, gamma=gamma).fit(training_data)\n",
    "\n",
    "    f1_train = f1_score(\n",
    "        train_targets,\n",
    "        where(oc_svm.predict(training_data) == 1, False, True),\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    f1_test = f1_score(\n",
    "        test_targets,\n",
    "        where(oc_svm.predict(testing_data) == 1, False, True),\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    print(f\"\\nF1 Score (Train): {f1_train}\\nF1 Score (Test): {f1_test}\")\n",
    "    return float(f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New optimizer is now aware of 0 points.\n",
      "\n",
      "F1 Score (Train): 0.34336147352264007\n",
      "F1 Score (Test): 0.7795303711685515\n",
      "\n",
      "F1 Score (Train): 0.4098903372646389\n",
      "F1 Score (Test): 0.8666179163369635\n",
      "\n",
      "F1 Score (Train): 0.4745762711864407\n",
      "F1 Score (Test): 0.9608962619220839\n",
      "\n",
      "F1 Score (Train): 0.36149253731343284\n",
      "F1 Score (Test): 0.8015597031120053\n",
      "\n",
      "F1 Score (Train): 0.39085860743272105\n",
      "F1 Score (Test): 0.8417199044799836\n",
      "\n",
      "F1 Score (Train): 0.46001893341748185\n",
      "F1 Score (Test): 0.9355503786030874\n",
      "\n",
      "F1 Score (Train): 0.49738588967867003\n",
      "F1 Score (Test): 0.9969123413447807\n",
      "\n",
      "F1 Score (Train): 0.49738588967867003\n",
      "F1 Score (Test): 0.9969123413447807\n",
      "\n",
      "F1 Score (Train): 0.49318801089918257\n",
      "F1 Score (Test): 0.9895836939069856\n",
      "\n",
      "F1 Score (Train): 0.4753495217071376\n",
      "F1 Score (Test): 0.9615931855046034\n",
      "\n",
      "F1 Score (Train): 0.4761846455246725\n",
      "F1 Score (Test): 0.9634481719209901\n",
      "\n",
      "F1 Score (Train): 0.49629106322854116\n",
      "F1 Score (Test): 0.9933680482205001\n",
      "\n",
      "F1 Score (Train): 0.4902591599642538\n",
      "F1 Score (Test): 0.983982867507266\n",
      "\n",
      "F1 Score (Train): 0.4941468605888613\n",
      "F1 Score (Test): 0.9891372003262542\n",
      "\n",
      "F1 Score (Train): 0.48907201719813687\n",
      "F1 Score (Test): 0.98173039885142\n",
      "\n",
      "F1 Score (Train): 0.45108102906268044\n",
      "F1 Score (Test): 0.9213848031151801\n",
      "\n",
      "F1 Score (Train): 0.44155081261014295\n",
      "F1 Score (Test): 0.9136801682046622\n",
      "\n",
      "F1 Score (Train): 0.49216524216524216\n",
      "F1 Score (Test): 0.9860041178017959\n",
      "\n",
      "F1 Score (Train): 0.4885222381635581\n",
      "F1 Score (Test): 0.9812790593607418\n",
      "\n",
      "F1 Score (Train): 0.4622926093514329\n",
      "F1 Score (Test): 0.9369979386301576\n",
      "\n",
      "F1 Score (Train): 0.4908961085326669\n",
      "F1 Score (Test): 0.9864525232475425\n",
      "\n",
      "F1 Score (Train): 0.4345757335448057\n",
      "F1 Score (Test): 0.8997679507493068\n",
      "\n",
      "F1 Score (Train): 0.4158530757151635\n",
      "F1 Score (Test): 0.8778772560912598\n",
      "\n",
      "F1 Score (Train): 0.45991667718722384\n",
      "F1 Score (Test): 0.9441874828894523\n",
      "\n",
      "F1 Score (Train): 0.4833021317712422\n",
      "F1 Score (Test): 0.9753855660502111\n",
      "\n",
      "F1 Score (Train): 0.46137865911237014\n",
      "F1 Score (Test): 0.9382017555321369\n",
      "\n",
      "F1 Score (Train): 0.48714260025175327\n",
      "F1 Score (Test): 0.9799233361568926\n",
      "\n",
      "F1 Score (Train): 0.4723729649728663\n",
      "F1 Score (Test): 0.9543567796459316\n",
      "\n",
      "F1 Score (Train): 0.4898336414048059\n",
      "F1 Score (Test): 0.9835329351788897\n",
      "New optimizer is now aware of 29 points.\n",
      "\n",
      "F1 Score (Train): 0.3319800124921924\n",
      "F1 Score (Test): 0.7875223277239822\n",
      "\n",
      "F1 Score (Train): 0.4865578492558809\n",
      "F1 Score (Test): 0.9769141644378512\n",
      "\n",
      "F1 Score (Train): 0.4479643847990193\n",
      "F1 Score (Test): 0.9265342215867294\n",
      "\n",
      "F1 Score (Train): 0.4661841776890442\n",
      "F1 Score (Test): 0.9497488432417274\n",
      "\n",
      "F1 Score (Train): 0.47230788207721724\n",
      "F1 Score (Test): 0.9583868079244935\n",
      "\n",
      "F1 Score (Train): 0.37314088944245\n",
      "F1 Score (Test): 0.8300557531386992\n",
      "\n",
      "F1 Score (Train): 0.4305111821086262\n",
      "F1 Score (Test): 0.9021592464404101\n",
      "\n",
      "F1 Score (Train): 0.4540581929555896\n",
      "F1 Score (Test): 0.9359709052019878\n",
      "\n",
      "F1 Score (Train): 0.4689672293942403\n",
      "F1 Score (Test): 0.9497488432417274\n",
      "\n",
      "F1 Score (Train): 0.4759279676589489\n",
      "F1 Score (Test): 0.9623178584472641\n",
      "\n",
      "F1 Score (Train): 0.49756298079746314\n",
      "F1 Score (Test): 0.9975928867783759\n",
      "\n",
      "F1 Score (Train): 0.39327754928378955\n",
      "F1 Score (Test): 0.8559502153960357\n",
      "New optimizer is now aware of 41 points.\n",
      "\n",
      "F1 Score (Train): 0.4883693117263649\n",
      "F1 Score (Test): 0.9831070009116857\n",
      "New optimizer is now aware of 42 points.\n",
      "New optimizer is now aware of 42 points.\n",
      "New optimizer is now aware of 42 points.\n",
      "New optimizer is now aware of 42 points.\n",
      "New optimizer is now aware of 42 points.\n",
      "New optimizer is now aware of 42 points.\n",
      "New optimizer is now aware of 42 points.\n",
      "New optimizer is now aware of 42 points.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len((activities := X_train[\"activity\"].unique()))):\n",
    "    training = X_train[X_train[\"activity\"].isin(activities[:i])].head(MIN_SAMPLES)\n",
    "    testing = X_test[X_test[\"activity\"].isin(activities[i : i + 1])].head(MIN_SAMPLES)\n",
    "\n",
    "    training.loc[:, \"isNovelty\"], testing.loc[:, \"isNovelty\"] = False, True\n",
    "    sampled_data = training.sample(n=int(0.2 * len(training)), random_state=42)\n",
    "    novelty = concat([testing, sampled_data])\n",
    "\n",
    "    training_data = training.drop(columns=[\"isNovelty\"])\n",
    "    train_targets = training[\"isNovelty\"]\n",
    "    # only current activity (as novelty)\n",
    "    testing_data = novelty.drop(columns=[\"isNovelty\"])\n",
    "    test_targets = novelty[\"isNovelty\"]\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=objective_function,\n",
    "        pbounds={\"nu\": (0.01, 0.5), \"gamma\": (1e-4, 1e-1)},\n",
    "        random_state=42,\n",
    "    )\n",
    "    load_logs(optimizer, logs=[LOGS_PATH])\n",
    "    print(\"New optimizer is now aware of {} points.\".format(len(optimizer.space)))\n",
    "\n",
    "    optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "    optimizer.maximize(init_points=5, n_iter=25)\n",
    "\n",
    "    models[i] = optimizer.max  # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCC-1-pCv1QtoV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
