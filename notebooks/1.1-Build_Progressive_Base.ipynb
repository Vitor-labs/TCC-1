{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Final, Literal\n",
    "\n",
    "from numpy import argmax, concatenate, vstack\n",
    "from pandas import DataFrame, concat, read_csv, set_option, to_datetime\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "set_option(\"display.max_columns\", None)\n",
    "\n",
    "COLUMNS: Final[list[str]] = [\n",
    "\t\"timestamp\",\n",
    "\t\"activity\",\n",
    "\t\"heart_rate\",\n",
    "\t*[\n",
    "\t\tf\"IMU_{body_part}_{suffix}\"\n",
    "\t\tfor body_part in [\"hand\", \"chest\", \"ankle\"]\n",
    "\t\tfor suffix in [\n",
    "\t\t\t\"temp_C\",\n",
    "\t\t\t*[\n",
    "\t\t\t\tf\"{scalar}_{axis}\"\n",
    "\t\t\t\tfor scalar in [\"acc16g_ms^-2\", \"acc6g_ms^-2\", \"gyro_rad/s\", \"mag_ŒºT\"]\n",
    "\t\t\t\tfor axis in [\"x\", \"y\", \"z\"]\n",
    "\t\t\t],\n",
    "\t\t\t*[f\"orient_{x}\" for x in range(1, 5)],\n",
    "\t\t]\n",
    "\t],\n",
    "]\n",
    "IMU_COLUMNS: Final[list[str]] = [\n",
    "\tcol\n",
    "\tfor col in COLUMNS\n",
    "\tif col.startswith(\"IMU_\") and \"acc6g_ms^-2\" not in col and \"orient\" not in col\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_w_log(path: Path, filename: str) -> tuple[DataFrame, str]:\n",
    "\t\"\"\"\n",
    "\tThe IMU sensory data contains the following columns:\n",
    "\t- 1 temperature (¬∞C)\n",
    "\t- 2...4 3D-acceleration data (ms^-2), scale: ¬±16g, resolution: 13-bit\n",
    "\t- 5...7 3D-acceleration data (ms^-2), scale: ¬±6g, resolution: 13-bit*\n",
    "\t- 8...10 3D-gyroscope data (rad/s)\n",
    "\t- 11...13 3D-magnetometer data (ŒºT)\n",
    "\t- 14...17 orientation (invalid in this data collection)\n",
    "\n",
    "\t* This accelerometer is not precisely calibrated with the first one. Moreover, due\n",
    "\tto high impacts caused by certain movements (e.g. during running) with acceleration\n",
    "\tover 6g, it gets saturated sometimes. Therefore, the use of the data from the first\n",
    "\taccelerometer (with the scale of ¬±16g) is recommended.\n",
    "\n",
    "\tArgs:\n",
    "\t\tpath: Directory path containing the data file.\n",
    "\t\tfilename: Name of the file to read.\n",
    "\n",
    "\tReturns:\n",
    "\t\tTuple containing the cleaned DataFrame and subject ID (last 2 chars of filename).\n",
    "\t\"\"\"\n",
    "\tprint(f\"Reading: {filename}\", end=\"\\r\")\n",
    "\tdf = read_csv(os.path.join(path, filename), sep=r\"\\s+\", header=None)\n",
    "\tdf.columns = COLUMNS\n",
    "\treturn (\n",
    "\t\tdf.loc[\n",
    "\t\t\t:,\n",
    "\t\t\t~df.columns.str.contains(\"orient\") & ~df.columns.str.contains(\"acc6g\"),\n",
    "\t\t],\n",
    "\t\tfilename.split(\".\")[0][-2:],\n",
    "\t)\n",
    "\n",
    "\n",
    "def handle_nans(df: DataFrame) -> DataFrame:\n",
    "\t\"\"\"\n",
    "\tHandles NaN values in the sensor data with a time-series-aware strategy.\n",
    "\n",
    "\t- First, forward-fills to propagate the last valid observation.\n",
    "\t- Then, uses linear interpolation for short gaps.\n",
    "\t- Finally, drops any rows where sensor data is still missing.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdf: The input DataFrame with potential NaN values.\n",
    "\n",
    "\tReturns:\n",
    "\t\tDataFrame with NaNs handled.\n",
    "\t\"\"\"\n",
    "\tdf = df.copy()\n",
    "\t# For IMU data: linear interpolation for short gaps, drop for long gaps\n",
    "\tfor col in IMU_COLUMNS:\n",
    "\t\t# Forward fill first (sensor readings typically persist briefly)\n",
    "\t\tdf.loc[:, col] = df[col].ffill(limit=2)\n",
    "\t\t# Only interpolate if gap is ‚â§ 5 samples (0.05s at 100Hz)\n",
    "\t\t# IMU gaps can be interpolated without significant information loss.\n",
    "\t\tdf.loc[:, col] = df[col].interpolate(\"linear\", limit=5, limit_direction=\"both\")\n",
    "\t# Drop rows where ANY IMU sensor still has NaN (likely sensor disconnection)\n",
    "\treturn df.dropna(subset=IMU_COLUMNS)\n",
    "\n",
    "\n",
    "def normalize_features(\n",
    "\tX_train: DataFrame,\n",
    "\tX_val: DataFrame | None = None,\n",
    "\tX_test: DataFrame | None = None,\n",
    "\tscaler: RobustScaler | None = None,\n",
    "\tforce_refit: bool = False,\n",
    ") -> tuple[DataFrame, DataFrame | None, DataFrame | None, RobustScaler]:\n",
    "\t\"\"\"Normalizes IMU features using RobustScaler.\n",
    "\n",
    "\tArgs:\n",
    "\t\tX_train: Training feature DataFrame.\n",
    "\t\tX_val: Optional validation feature DataFrame.\n",
    "\t\tX_test: Optional test feature DataFrame.\n",
    "\t\tscaler: Optional pre-fitted scaler. If None, fits new one on X_train.\n",
    "\t\tforce_refit: If True, ignores existing scaler and refits. Defaults to False.\n",
    "\n",
    "\tReturns:\n",
    "\t\t(normalized X_train, normalized X_val, normalized X_test, fitted scaler).\n",
    "\t\"\"\"\n",
    "\tif scaler is None or force_refit:\n",
    "\t\tscaler = RobustScaler().fit(X_train[IMU_COLUMNS])\n",
    "\n",
    "\tX_train = X_train.copy()\n",
    "\tX_train.loc[:, IMU_COLUMNS] = scaler.transform(X_train[IMU_COLUMNS])\n",
    "\n",
    "\tX_val_norm = None\n",
    "\tif X_val is not None:\n",
    "\t\tX_val_norm = X_val.copy()\n",
    "\t\tX_val_norm.loc[:, IMU_COLUMNS] = scaler.transform(X_val[IMU_COLUMNS])\n",
    "\n",
    "\tX_test_norm = None\n",
    "\tif X_test is not None:\n",
    "\t\tX_test_norm = X_test.copy()\n",
    "\t\tX_test_norm.loc[:, IMU_COLUMNS] = scaler.transform(X_test[IMU_COLUMNS])\n",
    "\n",
    "\treturn X_train, X_val_norm, X_test_norm, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: Path) -> tuple[DataFrame, DataFrame]:\n",
    "\t\"\"\"Load and preprocess all PAMAP2 protocol data.\n",
    "\n",
    "\tReturns:\n",
    "\t\ttuple: (data DataFrame, labels DataFrame)\n",
    "\t\"\"\"\n",
    "\tdata, labels = [], []\n",
    "\tfor df, subject in [  # all protocol files\n",
    "\t\tread_w_log(path, filename)\n",
    "\t\tfor filename in os.listdir(path)\n",
    "\t\tif filename.endswith(\".dat\")\n",
    "\t]:  # dropping rope jumping (24) cause only subject 9 does this activity\n",
    "\t\tdf = handle_nans(df[~df[\"activity\"].isin([0, 24])])\n",
    "\t\tdf[\"subject\"] = str(subject)\n",
    "\t\tdf[\"timestamp\"] = to_datetime(df[\"timestamp\"], unit=\"s\").dt.time\n",
    "\t\tdf[\"id\"] = df[\"subject\"] + \"_\" + df[\"timestamp\"].astype(str)\n",
    "\n",
    "\t\tdata.append(df.drop(columns=[\"activity\", \"heart_rate\"]))\n",
    "\t\tlabels.append(df[[\"id\", \"activity\"]])  # Index & Activity\n",
    "\n",
    "\tdata, labels = concat(data), concat(labels)\n",
    "\tdata[\"subject\"] = data[\"subject\"].astype(\"category\")\n",
    "\tlabels[\"activity\"] = labels[\"activity\"].astype(\"category\")\n",
    "\n",
    "\treturn data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_order(\n",
    "\tdata: DataFrame,\n",
    "\tlabels: DataFrame,\n",
    "\tmethod: Literal[\"pca\", \"statistical\", \"variance\", \"frequency\"] = \"pca\",\n",
    ") -> list[int]:\n",
    "\t\"\"\"\n",
    "\tOrder activities by distinctiveness for incremental learning.\n",
    "\n",
    "\tStrategy: Start with most common/representative activities, then add\n",
    "\tprogressively more distinct ones. This mimics realistic deployment where\n",
    "\tyou start with common cases and gradually encounter edge cases.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdata: Feature DataFrame\n",
    "\t\tlabels: Labels DataFrame with 'id' and 'activity' columns\n",
    "\t\tmethod: Method to use for activity representation:\n",
    "\t\t\t- 'pca': Use principal components (captures main movement patterns)\n",
    "\t\t\t- 'statistical': Use mean, std, and quartiles (robust statistics)\n",
    "\t\t\t- 'variance': Use variance and energy (good for dynamic activities)\n",
    "\t\t\t- 'frequency': Simple frequency-based ordering (most common first)\n",
    "\n",
    "\tReturns:\n",
    "\t\tOrdered list of activity IDs for incremental learning.\n",
    "\t\"\"\"\n",
    "\tprint(f\"\\nüîç Ordering activities for incremental learning using '{method}'...\")\n",
    "\tdf = data.merge(labels, how=\"left\", on=\"id\")\n",
    "\n",
    "\tif method == \"frequency\":\n",
    "\t\t# Simple: most common activities first\n",
    "\t\tactivity_counts = df[\"activity\"].value_counts()\n",
    "\t\tordered = activity_counts.index.tolist()\n",
    "\t\tprint(\"\\nüìä Activity ordering (most ‚Üí least frequent):\")\n",
    "\t\tfor i, act in enumerate(ordered, 1):\n",
    "\t\t\tprint(f\"  {i}. Activity {act}: {activity_counts[act]:,} samples\")\n",
    "\t\treturn ordered\n",
    "\n",
    "\t# Compute activity representations\n",
    "\tactivity_stats = {}\n",
    "\tactivity_counts = {}\n",
    "\tfor activity in df[\"activity\"].unique():\n",
    "\t\tactivity_data = df[df[\"activity\"] == activity][IMU_COLUMNS]\n",
    "\t\tactivity_counts[activity] = len(activity_data)\n",
    "\n",
    "\t\tif method == \"pca\":\n",
    "\t\t\tactivity_stats[activity] = (\n",
    "\t\t\t\tPCA(n_components=min(10, len(IMU_COLUMNS), len(activity_data)))\n",
    "\t\t\t\t.fit_transform(activity_data)\n",
    "\t\t\t\t.mean(axis=0)\n",
    "\t\t\t)\n",
    "\t\telif method == \"statistical\":\n",
    "\t\t\tactivity_stats[activity] = concatenate(\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\tactivity_data.mean().values,\n",
    "\t\t\t\t\tactivity_data.std().values,\n",
    "\t\t\t\t\tactivity_data.quantile(0.25).values,\n",
    "\t\t\t\t\tactivity_data.quantile(0.75).values,\n",
    "\t\t\t\t]\n",
    "\t\t\t)\n",
    "\t\telif method == \"variance\":\n",
    "\t\t\tactivity_stats[activity] = concatenate(\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\tactivity_data.var().values,\n",
    "\t\t\t\t\tactivity_data.abs().mean().values,\n",
    "\t\t\t\t]\n",
    "\t\t\t)\n",
    "\tactivities = list(activity_stats.keys())\n",
    "\tdistances = squareform(\n",
    "\t\tpdist(vstack([activity_stats[act] for act in activities]), metric=\"euclidean\")\n",
    "\t)\n",
    "\t# Greedy ordering: Start with most central (least distinct) activity\n",
    "\t# then add activities that are progressively more distinct\n",
    "\tordered = []\n",
    "\tremaining = set(range(len(activities)))\n",
    "\t# Start with the activity with minimum average distance to all others (most \"central\")\n",
    "\tavg_distances = distances.mean(axis=1)\n",
    "\tfirst_idx = argmax(\n",
    "\t\t[\n",
    "\t\t\t-avg_distances[i] if i in remaining else float(\"-inf\")\n",
    "\t\t\tfor i in range(len(activities))\n",
    "\t\t]\n",
    "\t).astype(int)\n",
    "\tordered.append(first_idx)\n",
    "\tremaining.remove(first_idx)\n",
    "\t# Greedily add activities with moderate distance to already selected ones\n",
    "\twhile remaining:\n",
    "\t\t# For each remaining activity, compute min distance to selected activities\n",
    "\t\tmin_dists = [\n",
    "\t\t\tmin([distances[idx, sel_idx] for sel_idx in ordered])\n",
    "\t\t\tif idx in remaining\n",
    "\t\t\telse float(\"inf\")\n",
    "\t\t\tfor idx in range(len(activities))\n",
    "\t\t]\n",
    "\t\t# Choose the one with minimum distance (closest to existing set)\n",
    "\t\t# This creates a gradual progression rather than jumping to extremes\n",
    "\t\tnext_idx = argmax(\n",
    "\t\t\t[\n",
    "\t\t\t\t-min_dists[i] if i in remaining else float(\"-inf\")\n",
    "\t\t\t\tfor i in range(len(activities))\n",
    "\t\t\t]\n",
    "\t\t).astype(int)\n",
    "\t\tordered.append(next_idx)\n",
    "\t\tremaining.remove(next_idx)\n",
    "\n",
    "\tprint(\"\\nüìä Incremental learning order:\")\n",
    "\tfor i, act in enumerate(ordered_activities := [activities[i] for i in ordered], 1):\n",
    "\t\tprint(f\"  {i}. Activity {act}: {activity_counts[act]:,} samples\")\n",
    "\n",
    "\treturn ordered_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalDataset:\n",
    "\t\"\"\"\n",
    "\tManages data splits for incremental/continual learning experiments.\n",
    "\n",
    "\tUsage:\n",
    "\t>>> dataset = IncrementalDataset(data, labels, initial_activities=3, test_size=2)\n",
    "\t>>> for fold_idx in range(dataset.n_folds):\n",
    "\t>>> \tX_train, y_train, X_test, y_test = dataset.get_fold(fold_idx, val_split=0.2)\n",
    "\t>>> \t# Train and evaluate model\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tdata: DataFrame,\n",
    "\t\tlabels: DataFrame,\n",
    "\t\tinitial_activities: int = 3,\n",
    "\t\ttest_size: int = 2,\n",
    "\t\tactivity_order: list[int] | None = None,\n",
    "\t\tordering_method: Literal[\"pca\", \"statistical\", \"variance\", \"frequency\"] = \"pca\",\n",
    "\t):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize incremental dataset manager.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tdata: Feature DataFrame\n",
    "\t\t\tlabels: Labels DataFrame with 'id' and 'activity' columns\n",
    "\t\t\tinitial_activities: Number of activities in first training set\n",
    "\t\t\ttest_size: Number of activities to test on in each fold\n",
    "\t\t\tactivity_order: Optional pre-defined activity ordering. If None, computed automatically.\n",
    "\t\t\tordering_method: Method for automatic activity ordering\n",
    "\t\t\"\"\"\n",
    "\t\tself.data = data\n",
    "\t\tself.labels = labels\n",
    "\t\tself.df = data.merge(labels, how=\"left\", on=\"id\")\n",
    "\n",
    "\t\tself.initial_activities = initial_activities\n",
    "\t\tself.test_size = test_size\n",
    "\n",
    "\t\tself.activity_order = (  # Get or compute activity ordering\n",
    "\t\t\tget_activity_order(data, labels, method=ordering_method)\n",
    "\t\t\tif activity_order is None\n",
    "\t\t\telse activity_order\n",
    "\t\t)\n",
    "\t\tself.n_activities = len(self.activity_order)\n",
    "\t\t# Calculate number of folds\n",
    "\t\t# Fold 0: train on first 3, test on next 2\n",
    "\t\t# Fold 1: train on first 5, test on next 2\n",
    "\t\t# ...\n",
    "\t\tself.n_folds = (self.n_activities - initial_activities) // test_size\n",
    "\t\tprint(\"\\nüì¶ Incremental Dataset Configuration:\")\n",
    "\t\tprint(f\"  Total activities: {self.n_activities}\")\n",
    "\t\tprint(f\"  Initial training activities: {initial_activities}\")\n",
    "\t\tprint(f\"  Test size per fold: {test_size}\")\n",
    "\t\tprint(f\"  Number of folds: {self.n_folds}\")\n",
    "\t\tprint(f\"  Activity order: {self.activity_order}\")\n",
    "\n",
    "\tdef get_fold(\n",
    "\t\tself,\n",
    "\t\tfold_idx: int,\n",
    "\t\tval_split: float = 0.2,\n",
    "\t\tnormalize: bool = True,\n",
    "\t\tscaler: RobustScaler | None = None,\n",
    "\t) -> tuple[\n",
    "\t\tDataFrame,\n",
    "\t\tDataFrame | None,\n",
    "\t\tDataFrame | None,\n",
    "\t\tDataFrame,\n",
    "\t\tDataFrame,\n",
    "\t\tDataFrame,\n",
    "\t\tRobustScaler | None,\n",
    "\t]:\n",
    "\t\t\"\"\"\n",
    "\t\tGet train/val/test split for a specific fold.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tfold_idx: Fold index (0 to n_folds-1)\n",
    "\t\t\tval_split: Proportion of training data to use for validation\n",
    "\t\t\tnormalize: Whether to normalize features\n",
    "\t\t\tscaler: Optional pre-fitted scaler to use\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\t(X_train, X_val, X_test, y_train, y_val, y_test, scaler)\n",
    "\t\t\"\"\"\n",
    "\t\tif fold_idx >= self.n_folds:\n",
    "\t\t\traise ValueError(f\"fold_idx {fold_idx} >= n_folds {self.n_folds}\")\n",
    "\t\t# Determine which activities go into train vs test\n",
    "\t\tn_train_activities = self.initial_activities + fold_idx * self.test_size\n",
    "\t\ttrain_activities = self.activity_order[:n_train_activities]\n",
    "\t\ttest_activities = self.activity_order[\n",
    "\t\t\tn_train_activities : n_train_activities + self.test_size\n",
    "\t\t]\n",
    "\t\tprint(f\"\\nüîÑ Fold {fold_idx}:\")\n",
    "\t\tprint(f\"  Training activities: {train_activities}\")\n",
    "\t\tprint(f\"  Test activities: {test_activities}\")\n",
    "\t\t# Split data\n",
    "\t\ttrain_mask = self.df[\"activity\"].isin(train_activities)\n",
    "\t\ttest_mask = self.df[\"activity\"].isin(test_activities)\n",
    "\n",
    "\t\ttrain_val_df = self.df[train_mask].copy()\n",
    "\t\ttest_df = self.df[test_mask].copy()\n",
    "\t\t# Temporal split for train/val\n",
    "\t\tsplit_idx = int(len(train_val_df) * (1 - val_split))\n",
    "\t\ttrain_df = train_val_df.iloc[:split_idx].copy()\n",
    "\t\tval_df = train_val_df.iloc[split_idx:].copy()\n",
    "\n",
    "\t\t# Separate features and labels\n",
    "\t\tfeature_cols = [col for col in self.data.columns if col != \"id\"]\n",
    "\n",
    "\t\tX_train = train_df[feature_cols].reset_index(drop=True)\n",
    "\t\tX_val = val_df[feature_cols].reset_index(drop=True)\n",
    "\t\tX_test = test_df[feature_cols].reset_index(drop=True)\n",
    "\n",
    "\t\ty_train = train_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\t\ty_val = val_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\t\ty_test = test_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\n",
    "\t\tprint(f\"  Train samples: {len(X_train):,}\")\n",
    "\t\tprint(f\"  Val samples: {len(X_val):,}\")\n",
    "\t\tprint(f\"  Test samples: {len(X_test):,}\")\n",
    "\n",
    "\t\tif normalize:\n",
    "\t\t\tX_train, X_val, X_test, scaler = normalize_features(\n",
    "\t\t\t\tX_train, X_val, X_test, scaler=scaler\n",
    "\t\t\t)\n",
    "\n",
    "\t\treturn X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "\n",
    "\tdef save_fold(self, fold_idx: int, output_dir: Path, **kwargs):\n",
    "\t\t\"\"\"Save a specific fold to disk.\"\"\"\n",
    "\t\tX_train, X_val, X_test, y_train, y_val, y_test, scaler = self.get_fold(\n",
    "\t\t\tfold_idx, **kwargs\n",
    "\t\t)\n",
    "\n",
    "\t\tfold_dir = output_dir / f\"fold_{fold_idx}\"\n",
    "\t\tfold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\t\tX_train.to_csv(fold_dir / \"X_train.csv\", index=False)\n",
    "\t\tX_val.to_csv(fold_dir / \"X_val.csv\", index=False)\n",
    "\t\tX_test.to_csv(fold_dir / \"X_test.csv\", index=False)\n",
    "\n",
    "\t\ty_train.to_csv(fold_dir / \"y_train.csv\", index=False)\n",
    "\t\ty_val.to_csv(fold_dir / \"y_val.csv\", index=False)\n",
    "\t\ty_test.to_csv(fold_dir / \"y_test.csv\", index=False)\n",
    "\n",
    "\t\tif scaler is not None:\n",
    "\t\t\twith open(fold_dir / \"scaler.pkl\", \"wb\") as f:\n",
    "\t\t\t\tpickle.dump(scaler, f)\n",
    "\n",
    "\t\tprint(f\"‚úÖ Saved fold {fold_idx} to {fold_dir}\")\n",
    "\n",
    "\tdef save_all_folds(self, output_dir: Path, **kwargs):\n",
    "\t\t\"\"\"Save all folds to disk.\"\"\"\n",
    "\t\tfor fold_idx in range(self.n_folds):\n",
    "\t\t\tself.save_fold(fold_idx, output_dir, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: subject109.dat\n",
      "üîç Ordering activities for incremental learning using 'pca'...\n",
      "\n",
      "üìä Incremental learning order:\n",
      "  1. Activity 16: 175,353 samples\n",
      "  2. Activity 13: 104,944 samples\n",
      "  3. Activity 3: 189,845 samples\n",
      "  4. Activity 5: 98,199 samples\n",
      "  5. Activity 17: 238,656 samples\n",
      "  6. Activity 1: 192,507 samples\n",
      "  7. Activity 2: 185,051 samples\n",
      "  8. Activity 4: 238,722 samples\n",
      "  9. Activity 6: 164,600 samples\n",
      "  10. Activity 12: 117,216 samples\n",
      "  11. Activity 7: 188,107 samples\n",
      "\n",
      "üì¶ Incremental Dataset Configuration:\n",
      "  Total activities: 11\n",
      "  Initial training activities: 3\n",
      "  Test size per fold: 2\n",
      "  Number of folds: 4\n",
      "  Activity order: [16, 13, 3, 5, 17, 1, 2, 4, 6, 12, 7]\n",
      "\n",
      "üîÑ Fold 0:\n",
      "  Training activities: [16, 13, 3]\n",
      "  Test activities: [5, 17]\n",
      "  Train samples: 376,113\n",
      "  Val samples: 94,029\n",
      "  Test samples: 336,855\n",
      "‚úÖ Saved fold 0 to ..\\data\\PAMAP2\\incremental\\fold_0\n",
      "\n",
      "üîÑ Fold 1:\n",
      "  Training activities: [16, 13, 3, 5, 17]\n",
      "  Test activities: [1, 2]\n",
      "  Train samples: 645,597\n",
      "  Val samples: 161,400\n",
      "  Test samples: 377,558\n",
      "‚úÖ Saved fold 1 to ..\\data\\PAMAP2\\incremental\\fold_1\n",
      "\n",
      "üîÑ Fold 2:\n",
      "  Training activities: [16, 13, 3, 5, 17, 1, 2]\n",
      "  Test activities: [4, 6]\n",
      "  Train samples: 947,644\n",
      "  Val samples: 236,911\n",
      "  Test samples: 403,322\n",
      "‚úÖ Saved fold 2 to ..\\data\\PAMAP2\\incremental\\fold_2\n",
      "\n",
      "üîÑ Fold 3:\n",
      "  Training activities: [16, 13, 3, 5, 17, 1, 2, 4, 6]\n",
      "  Test activities: [12, 7]\n",
      "  Train samples: 1,270,301\n",
      "  Val samples: 317,576\n",
      "  Test samples: 305,323\n",
      "‚úÖ Saved fold 3 to ..\\data\\PAMAP2\\incremental\\fold_3\n",
      "\n",
      "üîÑ Fold 0:\n",
      "  Training activities: [16, 13, 3]\n",
      "  Test activities: [5, 17]\n",
      "  Train samples: 376,113\n",
      "  Val samples: 94,029\n",
      "  Test samples: 336,855\n",
      "\n",
      "üîÑ Fold 1:\n",
      "  Training activities: [16, 13, 3, 5, 17]\n",
      "  Test activities: [1, 2]\n",
      "  Train samples: 645,597\n",
      "  Val samples: 161,400\n",
      "  Test samples: 377,558\n",
      "\n",
      "üîÑ Fold 2:\n",
      "  Training activities: [16, 13, 3, 5, 17, 1, 2]\n",
      "  Test activities: [4, 6]\n",
      "  Train samples: 947,644\n",
      "  Val samples: 236,911\n",
      "  Test samples: 403,322\n",
      "\n",
      "üîÑ Fold 3:\n",
      "  Training activities: [16, 13, 3, 5, 17, 1, 2, 4, 6]\n",
      "  Test activities: [12, 7]\n",
      "  Train samples: 1,270,301\n",
      "  Val samples: 317,576\n",
      "  Test samples: 305,323\n"
     ]
    }
   ],
   "source": [
    "data, labels = load_data(Path(\"../data/PAMAP2_Dataset/Protocol/\"))\n",
    "\n",
    "output_dir = Path(\"../data/PAMAP2/\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data.to_csv(output_dir / \"data.csv\", index=False)\n",
    "labels.to_csv(output_dir / \"labels.csv\", index=False)\n",
    "\n",
    "dataset = IncrementalDataset(\n",
    "\tdata=data,\n",
    "\tlabels=labels,\n",
    "\tinitial_activities=3,\n",
    "\ttest_size=2,\n",
    "\tordering_method=\"pca\",  # or \"frequency\" for simple most-common-first\n",
    ")\n",
    "# dataset.save_all_folds(  # Option 1: Save folds to disk (if you want static splits)\n",
    "# \tPath(\"../data/PAMAP2/incremental/\"), val_split=0.2, normalize=True\n",
    "# )\n",
    "for fold_idx in range(dataset.n_folds):  # Option 2: Use dynamically in training loop\n",
    "\tX_train, X_val, X_test, y_train, y_val, y_test, scaler = dataset.get_fold(\n",
    "\t\tfold_idx, val_split=0.2, normalize=True\n",
    "\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
