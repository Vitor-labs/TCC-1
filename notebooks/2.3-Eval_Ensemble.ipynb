{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from collections.abc import Callable\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from json import dumps\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from typing import Any, Final, Literal, Unpack\n",
    "\n",
    "import numpy as np\n",
    "import structlog\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bayes_opt import BayesianOptimization\n",
    "from numpy import argmax, concatenate, ndarray, vstack, where\n",
    "from pandas import DataFrame, Series, concat, read_csv, set_option, to_datetime\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import loguniform, randint, uniform, wilcoxon\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import (\n",
    "\tauc,\n",
    "\taverage_precision_score,\n",
    "\tclassification_report,\n",
    "\tf1_score,\n",
    "\tprecision_recall_curve,\n",
    "\troc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "set_option(\"display.max_columns\", None)\n",
    "logger: Final[structlog.stdlib.BoundLogger] = structlog.get_logger(__name__)\n",
    "type ParamGrid = dict[str, tuple[float | str, ...]]\n",
    "\n",
    "COLUMNS: Final[list[str]] = [\n",
    "\t\"timestamp\",\n",
    "\t\"activity\",\n",
    "\t\"heart_rate\",\n",
    "\t*[\n",
    "\t\tf\"IMU_{body_part}_{suffix}\"\n",
    "\t\tfor body_part in [\"hand\", \"chest\", \"ankle\"]\n",
    "\t\tfor suffix in [\n",
    "\t\t\t\"temp_C\",\n",
    "\t\t\t*[\n",
    "\t\t\t\tf\"{scalar}_{axis}\"\n",
    "\t\t\t\tfor scalar in [\"acc16g_ms^-2\", \"acc6g_ms^-2\", \"gyro_rad/s\", \"mag_Î¼T\"]\n",
    "\t\t\t\tfor axis in [\"x\", \"y\", \"z\"]\n",
    "\t\t\t],\n",
    "\t\t\t*[f\"orient_{x}\" for x in range(1, 5)],\n",
    "\t\t]\n",
    "\t],\n",
    "]\n",
    "IMU_COLUMNS: Final[list[str]] = [\n",
    "\tcol\n",
    "\tfor col in COLUMNS\n",
    "\tif col.startswith(\"IMU_\") and \"acc6g_ms^-2\" not in col and \"orient\" not in col\n",
    "]\n",
    "NUM_TRIALS: Final[int] = 10\n",
    "\n",
    "# Global variables for scoring\n",
    "testing_data: DataFrame | None = None\n",
    "test_targets: Series | None = None\n",
    "\n",
    "structlog.configure(\n",
    "\tprocessors=[\n",
    "\t\tstructlog.stdlib.filter_by_level,\n",
    "\t\tstructlog.stdlib.add_logger_name,\n",
    "\t\tstructlog.stdlib.add_log_level,\n",
    "\t\tstructlog.stdlib.PositionalArgumentsFormatter(),\n",
    "\t\tstructlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "\t\tstructlog.processors.StackInfoRenderer(),\n",
    "\t\tstructlog.processors.format_exc_info,\n",
    "\t\tstructlog.processors.UnicodeDecoder(),\n",
    "\t\tstructlog.processors.JSONRenderer(),\n",
    "\t],\n",
    "\tcontext_class=dict,\n",
    "\tlogger_factory=structlog.stdlib.LoggerFactory(),\n",
    "\twrapper_class=structlog.stdlib.BoundLogger,\n",
    "\tcache_logger_on_first_use=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_file_logger(filepath: str) -> structlog.BoundLogger:\n",
    "\t\"\"\"Configure a file-specific logger using structlog.\"\"\"\n",
    "\n",
    "\t# Create a specific handler for this file\n",
    "\tfile_handler = logging.FileHandler(filepath, mode=\"a\")\n",
    "\tfile_handler.setLevel(logging.INFO)\n",
    "\n",
    "\t# Configure structlog with file output\n",
    "\tstructlog.configure(\n",
    "\t\tprocessors=[\n",
    "\t\t\tstructlog.stdlib.filter_by_level,\n",
    "\t\t\tstructlog.stdlib.add_logger_name,\n",
    "\t\t\tstructlog.stdlib.add_log_level,\n",
    "\t\t\tstructlog.stdlib.PositionalArgumentsFormatter(),\n",
    "\t\t\tstructlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "\t\t\tstructlog.processors.StackInfoRenderer(),\n",
    "\t\t\tstructlog.processors.format_exc_info,\n",
    "\t\t\tstructlog.processors.UnicodeDecoder(),\n",
    "\t\t\tstructlog.processors.JSONRenderer(),\n",
    "\t\t],\n",
    "\t\tcontext_class=dict,\n",
    "\t\tlogger_factory=structlog.stdlib.LoggerFactory(),\n",
    "\t\twrapper_class=structlog.stdlib.BoundLogger,\n",
    "\t\tcache_logger_on_first_use=False,\n",
    "\t)\n",
    "\t# Get the underlying stdlib logger and add the handler\n",
    "\tstdlib_logger = logging.getLogger(\"hyperparameter_search\")\n",
    "\tstdlib_logger.handlers.clear()  # Clear existing handlers\n",
    "\tstdlib_logger.addHandler(file_handler)\n",
    "\tstdlib_logger.setLevel(logging.INFO)\n",
    "\n",
    "\treturn structlog.get_logger(\"hyperparameter_search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOVELTY DETECTION ENSEMBLE DEFINITION**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "\t\"\"\"LSTM-based Autoencoder for time-series novelty detection.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tn_features: int,\n",
    "\t\tsequence_length: int,\n",
    "\t\thidden_size: int = 64,\n",
    "\t\tn_layers: int = 2,\n",
    "\t\tdropout: float = 0.2,\n",
    "\t):\n",
    "\t\tsuper(LSTMAutoencoder, self).__init__()\n",
    "\t\tself.n_features = n_features\n",
    "\t\tself.sequence_length = sequence_length\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.encoder = nn.LSTM(\n",
    "\t\t\tinput_size=n_features,\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tnum_layers=n_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=dropout if n_layers > 1 else 0,\n",
    "\t\t)\n",
    "\t\tself.decoder = nn.LSTM(\n",
    "\t\t\tinput_size=hidden_size,\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tnum_layers=n_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=dropout if n_layers > 1 else 0,\n",
    "\t\t)\n",
    "\t\tself.output_layer = nn.Linear(hidden_size, n_features)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Encode\n",
    "\t\t_, (hidden, cell) = self.encoder(x)\n",
    "\t\t# Repeat hidden state for decoder\n",
    "\t\tdecoder_input = hidden[-1].unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "\t\t# Decode\n",
    "\t\tdecoder_output, _ = self.decoder(decoder_input, (hidden, cell))\n",
    "\t\t# Output reconstruction\n",
    "\t\treturn self.output_layer(decoder_output)\n",
    "\n",
    "\n",
    "class LSTMAutoencoderWrapper:\n",
    "\t\"\"\"Wrapper for LSTM Autoencoder to match sklearn API.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tn_features: int,\n",
    "\t\tsequence_length: int = 100,\n",
    "\t\thidden_size: int = 64,\n",
    "\t\tn_layers: int = 2,\n",
    "\t\tdropout: float = 0.2,\n",
    "\t\tepochs: int = 50,\n",
    "\t\tbatch_size: int = 64,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\tdevice: str | None = None,\n",
    "\t):\n",
    "\t\tself.n_features = n_features\n",
    "\t\tself.sequence_length = sequence_length\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.n_layers = n_layers\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.epochs = epochs\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\t\tself.model = None\n",
    "\t\tself.threshold = None\n",
    "\n",
    "\tdef _create_sequences(self, data: ndarray) -> ndarray:\n",
    "\t\t\"\"\"Create sliding windows from flat data.\"\"\"\n",
    "\t\tdata = data.astype(np.float32)\n",
    "\t\treturn np.array(\n",
    "\t\t\t[\n",
    "\t\t\t\tdata[i : i + self.sequence_length]\n",
    "\t\t\t\tfor i in range(len(data) - self.sequence_length + 1)\n",
    "\t\t\t],\n",
    "\t\t\tdtype=np.float32,  # Explicitly set dtype\n",
    "\t\t)\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y=None):\n",
    "\t\t\"\"\"Train the LSTM Autoencoder on normal data.\"\"\"\n",
    "\t\t# Convert to numpy (already scaled by normalize_features)\n",
    "\t\tsequences = self._create_sequences(X[IMU_COLUMNS].values.astype(np.float32))\n",
    "\t\tif len(sequences) == 0:\n",
    "\t\t\tlogger.warning(\n",
    "\t\t\t\t\"Not enough data to create sequences. Skipping LSTM-AE training.\"\n",
    "\t\t\t)\n",
    "\t\t\treturn self\n",
    "\n",
    "\t\tself.model = LSTMAutoencoder(\n",
    "\t\t\tn_features=self.n_features,\n",
    "\t\t\tsequence_length=self.sequence_length,\n",
    "\t\t\thidden_size=self.hidden_size,\n",
    "\t\t\tn_layers=self.n_layers,\n",
    "\t\t\tdropout=self.dropout,\n",
    "\t\t).to(self.device)\n",
    "\n",
    "\t\t# Training setup\n",
    "\t\tcriterion = nn.MSELoss()\n",
    "\t\toptimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\t\tdataloader = torch.utils.data.DataLoader(\n",
    "\t\t\ttorch.utils.data.TensorDataset(torch.FloatTensor(sequences)),\n",
    "\t\t\tbatch_size=self.batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t)\n",
    "\t\tself.model.train()\n",
    "\t\tfor epoch in range(self.epochs):  # Training loop\n",
    "\t\t\tepoch_loss = 0\n",
    "\t\t\tfor batch in dataloader:\n",
    "\t\t\t\tbatch_data = batch[0].to(self.device)\n",
    "\t\t\t\t# Forward pass\n",
    "\t\t\t\treconstruction = self.model(batch_data)\n",
    "\t\t\t\tloss = criterion(reconstruction, batch_data)\n",
    "\t\t\t\t# Backward pass\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\tepoch_loss += loss.item()\n",
    "\n",
    "\t\t\tif (epoch + 1) % 10 == 0:\n",
    "\t\t\t\tlogger.info(\n",
    "\t\t\t\t\tf\"LSTM-AE Epoch {epoch + 1}/{self.epochs} \"\n",
    "\t\t\t\t\t+ f\"Loss: {epoch_loss / len(dataloader):.4f}\",\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t# Treshold on training data (95th percentile of reconstruction errors)\n",
    "\t\tself.threshold = np.percentile(self._calc_reconstruction_errors(sequences), 95)\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef _calc_reconstruction_errors(self, sequences: ndarray) -> ndarray:\n",
    "\t\t\"\"\"Compute reconstruction error for sequences.\"\"\"\n",
    "\t\tassert self.model is not None, \"Model is not trained yet.\"\n",
    "\t\tself.model.eval()\n",
    "\t\terrors = []\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i in range(0, len(sequences), self.batch_size):\n",
    "\t\t\t\tbatch_tensor = torch.FloatTensor(sequences[i : i + self.batch_size]).to(\n",
    "\t\t\t\t\tself.device\n",
    "\t\t\t\t)\n",
    "\t\t\t\terrors.extend(  # MSE per sequence\n",
    "\t\t\t\t\ttorch.mean(\n",
    "\t\t\t\t\t\t(batch_tensor - self.model(batch_tensor)) ** 2, dim=(1, 2)\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\t.cpu()\n",
    "\t\t\t\t\t.numpy()\n",
    "\t\t\t\t)\n",
    "\t\treturn np.array(errors)\n",
    "\n",
    "\tdef predict(self, X: DataFrame) -> ndarray:\n",
    "\t\t\"\"\"Predict: 1 for normal, -1 for anomaly.\"\"\"\n",
    "\t\treturn where(self.decision_function(X) > self.threshold, -1, 1)\n",
    "\n",
    "\tdef decision_function(self, X: DataFrame) -> ndarray:\n",
    "\t\t\"\"\"Return anomaly scores (higher = more anomalous).\"\"\"\n",
    "\t\tsequences = self._create_sequences(X[IMU_COLUMNS].values.astype(np.float32))\n",
    "\t\t# Data is already scaled - no transformation needed\n",
    "\t\tif len(sequences) == 0:\n",
    "\t\t\t# If not enough data for a sequence, return high anomaly score\n",
    "\t\t\treturn np.array([self.threshold * 2 if self.threshold else 1.0] * len(X))\n",
    "\n",
    "\t\terrors = self._calc_reconstruction_errors(sequences)\n",
    "\t\t# Pad to match original length (for sequences lost at the end)\n",
    "\t\tif (padding := len(X) - len(errors)) > 0:\n",
    "\t\t\terrors = np.concatenate([errors, errors[-1:].repeat(padding)])\n",
    "\n",
    "\t\treturn errors\n",
    "\n",
    "\tdef score(self, X: DataFrame, y=None) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tScore method for sklearn compatibility.\n",
    "\t\tReturns negative mean reconstruction error (higher is better).\n",
    "\t\t\"\"\"\n",
    "\t\tif self.threshold is None:\n",
    "\t\t\treturn 0.0\n",
    "\n",
    "\t\tsequences = self._create_sequences(X[IMU_COLUMNS].values.astype(np.float32))\n",
    "\t\tif len(sequences) == 0:\n",
    "\t\t\treturn 0.0\n",
    "\t\t# Return negative error (sklearn maximizes score)\n",
    "\t\treturn -float(np.mean(self._calc_reconstruction_errors(sequences)))\n",
    "\n",
    "\tdef get_params(self, deep=True):\n",
    "\t\t\"\"\"\n",
    "\t\tGet parameters for sklearn compatibility.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tdeep (bool): Ignored, present for sklearn compatibility. Default to True.\n",
    "\t\t\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t\"n_features\": self.n_features,\n",
    "\t\t\t\"sequence_length\": self.sequence_length,\n",
    "\t\t\t\"hidden_size\": self.hidden_size,\n",
    "\t\t\t\"n_layers\": self.n_layers,\n",
    "\t\t\t\"dropout\": self.dropout,\n",
    "\t\t\t\"epochs\": self.epochs,\n",
    "\t\t\t\"batch_size\": self.batch_size,\n",
    "\t\t\t\"learning_rate\": self.learning_rate,\n",
    "\t\t}\n",
    "\n",
    "\tdef set_params(self, **params):\n",
    "\t\t\"\"\"Set parameters for sklearn compatibility.\"\"\"\n",
    "\t\tfor key, value in params.items():\n",
    "\t\t\tsetattr(self, key, value)\n",
    "\t\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoveltyDetectionEnsemble(BaseEstimator, ClassifierMixin):\n",
    "\t\"\"\"Ensemble combining LSTM-AE, Isolation Forest, and One-Class SVM.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tn_features: int,\n",
    "\t\t# LSTM-AE params\n",
    "\t\tsequence_length: int = 100,\n",
    "\t\thidden_size: int = 64,\n",
    "\t\tn_layers: int = 2,\n",
    "\t\tdropout: float = 0.2,\n",
    "\t\tepochs: int = 50,\n",
    "\t\tbatch_size: int = 64,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\t# Isolation Forest params\n",
    "\t\tn_estimators: int = 200,\n",
    "\t\tmax_samples: int = 256,\n",
    "\t\tcontamination: float = 0.05,\n",
    "\t\t# OC-SVM params\n",
    "\t\tnu: float = 0.05,\n",
    "\t\tgamma: float | Literal[\"scale\", \"auto\"] = \"scale\",\n",
    "\t\t# Ensemble params\n",
    "\t\tweights: tuple[float, float, float] = (0.4, 0.3, 0.3),\n",
    "\t\t**kwargs: dict[str, Any],  # unnused, just for scikit learn compatibility\n",
    "\t):\n",
    "\t\tself.n_features = n_features\n",
    "\t\tself.weights = np.array(weights)\n",
    "\n",
    "\t\tself.lstm_ae = LSTMAutoencoderWrapper(\n",
    "\t\t\tn_features=n_features,\n",
    "\t\t\tsequence_length=sequence_length,\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tn_layers=n_layers,\n",
    "\t\t\tdropout=dropout,\n",
    "\t\t\tepochs=epochs,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tlearning_rate=learning_rate,\n",
    "\t\t)\n",
    "\t\tself.isolation_forest = IsolationForest(\n",
    "\t\t\tn_estimators=n_estimators,\n",
    "\t\t\tmax_samples=max_samples,\n",
    "\t\t\tcontamination=contamination,\n",
    "\t\t\trandom_state=42,\n",
    "\t\t)\n",
    "\t\tself.ocsvm = OneClassSVM(nu=nu, gamma=gamma, kernel=\"rbf\")\n",
    "\n",
    "\t\tself.models = [self.isolation_forest, self.ocsvm]\n",
    "\t\tself.model_names = [\"IsolationForest\", \"OC-SVM\"]\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y=None):\n",
    "\t\t\"\"\"Train all models in the ensemble.\"\"\"\n",
    "\t\tlogger.info(\"Training ensemble models...\")\n",
    "\t\tfor name, model in zip(self.model_names, self.models, strict=True):\n",
    "\t\t\tlogger.info(f\"Training {name}...\")\n",
    "\t\t\tmodel.fit(X, y)\n",
    "\t\tlogger.info(\"Ensemble training complete.\")\n",
    "\t\treturn self\n",
    "\n",
    "\tdef predict(self, X: DataFrame) -> ndarray:\n",
    "\t\t\"\"\"Predict: 1 for normal, -1 for anomaly.\"\"\"\n",
    "\t\tscores = self.decision_function(X)\n",
    "\t\treturn where(scores > np.median(scores), -1, 1)\n",
    "\n",
    "\tdef decision_function(self, X: DataFrame) -> ndarray:\n",
    "\t\t\"\"\"Return combined anomaly scores (higher = more anomalous).\"\"\"\n",
    "\t\treturn np.average(\n",
    "\t\t\t[\n",
    "\t\t\t\tself._normalize_scores(self.lstm_ae.decision_function(X)),\n",
    "\t\t\t\tself._normalize_scores(\n",
    "\t\t\t\t\t-self.isolation_forest.decision_function(X.values)\n",
    "\t\t\t\t),\n",
    "\t\t\t\tself._normalize_scores(-self.ocsvm.decision_function(X.values)),\n",
    "\t\t\t],\n",
    "\t\t\taxis=0,\n",
    "\t\t\tweights=self.weights,\n",
    "\t\t)\n",
    "\n",
    "\tdef score(self, X: DataFrame, y=None, sample_weight=None) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tScore method for sklearn compatibility.\n",
    "\t\tFor novelty detection, return negative mean anomaly score.\n",
    "\t\t\"\"\"\n",
    "\t\treturn -float(np.mean(self.decision_function(X)))\n",
    "\n",
    "\tdef _normalize_scores(self, scores: ndarray) -> ndarray:\n",
    "\t\t\"\"\"Normalize scores to [0, 1] range using min-max scaling.\"\"\"\n",
    "\t\tif (max_score := np.max(scores)) - (min_score := np.min(scores)) == 0:\n",
    "\t\t\treturn np.zeros_like(scores)\n",
    "\t\treturn (scores - min_score) / (max_score - min_score)\n",
    "\n",
    "\tdef get_params(self, deep=True):\n",
    "\t\t\"\"\"Get all ensemble parameters in flat structure.\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t\"n_features\": self.n_features,\n",
    "\t\t\t\"weights\": tuple(self.weights),\n",
    "\t\t\t**self.lstm_ae.get_params(deep=False),\n",
    "\t\t\t**{\n",
    "\t\t\t\tk: v\n",
    "\t\t\t\tfor k, v in self.isolation_forest.get_params(deep=False).items()\n",
    "\t\t\t\tif k in [\"n_estimators\", \"max_samples\", \"contamination\"]\n",
    "\t\t\t},\n",
    "\t\t\t**{\n",
    "\t\t\t\tk: v\n",
    "\t\t\t\tfor k, v in self.ocsvm.get_params(deep=False).items()\n",
    "\t\t\t\tif k in [\"nu\", \"gamma\"]\n",
    "\t\t\t},\n",
    "\t\t}\n",
    "\n",
    "\tdef set_params(self, **params):\n",
    "\t\t\"\"\"Set parameters and reinitialize models.\"\"\"\n",
    "\t\t# Separate parameters by model\n",
    "\t\tlstm_params = {}\n",
    "\t\tif_params = {}\n",
    "\t\tocsvm_params = {}\n",
    "\t\tensemble_params = {}\n",
    "\n",
    "\t\tfor key, value in params.items():\n",
    "\t\t\tif key in [\n",
    "\t\t\t\t\"sequence_length\",\n",
    "\t\t\t\t\"hidden_size\",\n",
    "\t\t\t\t\"n_layers\",\n",
    "\t\t\t\t\"dropout\",\n",
    "\t\t\t\t\"epochs\",\n",
    "\t\t\t\t\"batch_size\",\n",
    "\t\t\t\t\"learning_rate\",\n",
    "\t\t\t]:\n",
    "\t\t\t\tlstm_params[key] = value\n",
    "\t\t\telif key in [\"n_estimators\", \"max_samples\", \"contamination\"]:\n",
    "\t\t\t\tif_params[key] = value\n",
    "\t\t\telif key in [\"nu\", \"gamma\"]:\n",
    "\t\t\t\tocsvm_params[key] = value\n",
    "\t\t\telif key in [\"weights\", \"n_features\"]:\n",
    "\t\t\t\tensemble_params[key] = value\n",
    "\t\t\telse:\n",
    "\t\t\t\tsetattr(self, key, value)\n",
    "\n",
    "\t\t# Update ensemble-level params\n",
    "\t\tfor key, value in ensemble_params.items():\n",
    "\t\t\tsetattr(self, key, value)\n",
    "\t\t\tif key == \"weights\":\n",
    "\t\t\t\tself.weights = np.array(value)\n",
    "\n",
    "\t\t# Reinitialize models with new parameters\n",
    "\t\tif lstm_params or \"n_features\" in ensemble_params:\n",
    "\t\t\tlstm_params[\"n_features\"] = getattr(self, \"n_features\", self.n_features)\n",
    "\t\t\tfor key in [\n",
    "\t\t\t\t\"sequence_length\",\n",
    "\t\t\t\t\"hidden_size\",\n",
    "\t\t\t\t\"n_layers\",\n",
    "\t\t\t\t\"dropout\",\n",
    "\t\t\t\t\"epochs\",\n",
    "\t\t\t\t\"batch_size\",\n",
    "\t\t\t\t\"learning_rate\",\n",
    "\t\t\t]:\n",
    "\t\t\t\tif key not in lstm_params:\n",
    "\t\t\t\t\tlstm_params[key] = getattr(self.lstm_ae, key)\n",
    "\n",
    "\t\t\tself.lstm_ae = LSTMAutoencoderWrapper(**lstm_params)\n",
    "\n",
    "\t\tif if_params:\n",
    "\t\t\tcurrent_if_params = self.isolation_forest.get_params()\n",
    "\t\t\tcurrent_if_params.update(if_params)\n",
    "\t\t\tself.isolation_forest = IsolationForest(\n",
    "\t\t\t\t**current_if_params, random_state=42\n",
    "\t\t\t)\n",
    "\n",
    "\t\tif ocsvm_params:\n",
    "\t\t\tcurrent_ocsvm_params = self.ocsvm.get_params()\n",
    "\t\t\tcurrent_ocsvm_params.update(ocsvm_params)\n",
    "\t\t\tself.ocsvm = OneClassSVM(**current_ocsvm_params, kernel=\"rbf\")\n",
    "\n",
    "\t\t# Update models list\n",
    "\t\tself.models = [self.lstm_ae, self.isolation_forest, self.ocsvm]\n",
    "\n",
    "\t\treturn self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HYPER-PARAMS OPTIMIZATION METHODS IMPLEMENTAION**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(\n",
    "\tmodel: NoveltyDetectionEnsemble,\n",
    "\tTrain: DataFrame,\n",
    "\ttest: Series,\n",
    "\ttesting_data: DataFrame,\n",
    ") -> dict:\n",
    "\t\"\"\"\n",
    "\tObjective function to maximize, calcs the F1 score on the test set.\n",
    "\tNow works with ensemble model.\n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel (NoveltyDetectionEnsemble): Ensemble model to eval\n",
    "\t\tTrain (DataFrame): train data, only for API compliance\n",
    "\t\ttest (Series): true targets (True for novel, False for normal)\n",
    "\t\ttesting_data (DataFrame): test feature data\n",
    "\n",
    "\tReturns:\n",
    "\t\tdict: Evaluation metrics\n",
    "\t\"\"\"\n",
    "\t# Get predictions and anomaly scores\n",
    "\tf1 = f1_score(test, where(model.predict(testing_data) == -1, True, False))\n",
    "\t# Get decision scores (higher values = more anomalous)\n",
    "\tanomaly_scores = model.decision_function(testing_data)\n",
    "\tprecision, recall, _ = precision_recall_curve(test, anomaly_scores)\n",
    "\tmetrics = {\n",
    "\t\t\"f1_score\": float(f1),\n",
    "\t\t\"avg_precision\": float(average_precision_score(test, anomaly_scores)),\n",
    "\t\t\"auc_pr\": float(auc(recall, precision)),\n",
    "\t\t\"auc_roc\": float(roc_auc_score(test, anomaly_scores)),\n",
    "\t\t\"params\": model.get_params(),\n",
    "\t\t\"datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "\t}\n",
    "\tlogger.info(f\"Evaluation metrics: {metrics}\")\n",
    "\treturn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedAnnealingSearch:\n",
    "\t\"\"\"\n",
    "\tCustom Simulated Annealing implementation for hyperparameter optimization.\n",
    "\n",
    "\tSimulated Annealing Search:\n",
    "\t- Temperature-based acceptance: Accepts worse solutions with decreasing probability\n",
    "\t- Adaptive parameter perturbation: Different strategies for continuous vs discrete parameters\n",
    "\t- Cooling schedule: Exponential cooling with configurable rate\n",
    "\t- Neighbor generation: Smart parameter space exploration\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tparam_space: dict[str, list[int | float | str]],\n",
    "\t\tn_iter: int = 100,\n",
    "\t\tinitial_temp: float = 1.0,\n",
    "\t\tcooling_rate: float = 0.95,\n",
    "\t\tmin_temp: float = 0.01,\n",
    "\t\trandom_state: int = 42,\n",
    "\t):\n",
    "\t\tself.param_space = param_space\n",
    "\t\tself.n_iter = n_iter\n",
    "\t\tself.initial_temp = initial_temp\n",
    "\t\tself.cooling_rate = cooling_rate\n",
    "\t\tself.min_temp = min_temp\n",
    "\t\tself.random_state = random_state\n",
    "\t\tself.best_params_ = None\n",
    "\t\tself.best_score_ = -np.inf\n",
    "\t\tself.cv_results_ = {\"mean_test_score\": []}\n",
    "\n",
    "\tdef _sample_params(self) -> dict:\n",
    "\t\t\"\"\"Sample random parameters from the parameter space.\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\treturn {\n",
    "\t\t\t\tkey: (\n",
    "\t\t\t\t\tvalues.rvs(random_state=self.random_state)\n",
    "\t\t\t\t\tif hasattr(values, \"rvs\")  # scipy distribution\n",
    "\t\t\t\t\telse random.choice(values)\n",
    "\t\t\t\t\tif isinstance(values, (list, tuple))  # Ensure it's a sequence\n",
    "\t\t\t\t\telse values  # Use the value directly if it's not sampleable\n",
    "\t\t\t\t)\n",
    "\t\t\t\tfor key, values in self.param_space.items()\n",
    "\t\t\t}\n",
    "\t\texcept Exception as exc:\n",
    "\t\t\tprint(self.param_space)\n",
    "\t\t\tlogger.error(\"Error sampling parameters.\")\n",
    "\t\t\traise exc\n",
    "\n",
    "\tdef _neighbor_params(self, current_params: dict) -> dict:\n",
    "\t\t\"\"\"Generate neighboring parameters by slightly modifying current ones.\"\"\"\n",
    "\t\tneighbor = deepcopy(current_params)\n",
    "\n",
    "\t\t# Choose a random parameter to modify\n",
    "\t\tparam_to_modify = random.choice(list(self.param_space.keys()))\n",
    "\n",
    "\t\tif hasattr(self.param_space[param_to_modify], \"rvs\"):  # continuous parameter\n",
    "\t\t\tif param_to_modify == \"nu\":\n",
    "\t\t\t\t# For nu, stay within bounds [0.001, 1.0]\n",
    "\t\t\t\tcurrent_val = neighbor[param_to_modify]\n",
    "\t\t\t\tneighbor[param_to_modify] = np.clip(\n",
    "\t\t\t\t\tcurrent_val + np.random.normal(0, 0.05 * current_val), 0.001, 1.0\n",
    "\t\t\t\t)\n",
    "\t\t\telif param_to_modify == \"gamma\":\n",
    "\t\t\t\t# For gamma, use log-space perturbation\n",
    "\t\t\t\tneighbor[param_to_modify] = 10 ** np.clip(\n",
    "\t\t\t\t\tnp.log10(neighbor[param_to_modify]) + np.random.normal(0, 0.1),\n",
    "\t\t\t\t\t-4,\n",
    "\t\t\t\t\t1,\n",
    "\t\t\t\t)  # 1e-4 to 10\n",
    "\t\t\telif param_to_modify == \"tol\":\n",
    "\t\t\t\t# For tolerance, use log-space perturbation\n",
    "\t\t\t\tneighbor[param_to_modify] = 10 ** np.clip(\n",
    "\t\t\t\t\tnp.log10(neighbor[param_to_modify]) + np.random.normal(0, 0.1),\n",
    "\t\t\t\t\t-6,\n",
    "\t\t\t\t\t-1,\n",
    "\t\t\t\t)  # 1e-6 to 1e-1\n",
    "\t\telse:  # discrete parameter\n",
    "\t\t\tneighbor[param_to_modify] = random.choice(self.param_space[param_to_modify])\n",
    "\n",
    "\t\treturn neighbor\n",
    "\n",
    "\tdef _evaluate_params(self, params: ParamGrid, X: DataFrame, y: Series) -> float:\n",
    "\t\t\"\"\"Evaluate parameter configuration using cross-validation.\"\"\"\n",
    "\t\tprint(params)\n",
    "\t\treturn np.mean(\n",
    "\t\t\tcross_val_score(\n",
    "\t\t\t\tNoveltyDetectionEnsemble(**params), X, y, cv=4, scoring=score_function\n",
    "\t\t\t)\n",
    "\t\t).astype(float)\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y: Series):\n",
    "\t\t\"\"\"Fit the simulated annealing search.\"\"\"\n",
    "\t\trandom.seed(self.random_state)\n",
    "\t\tnp.random.seed(self.random_state)\n",
    "\t\t# Initialize with random parameters\n",
    "\t\tcurrent_params = self._sample_params()\n",
    "\t\tcurrent_score = self._evaluate_params(current_params, X, y)\n",
    "\n",
    "\t\tself.best_params_ = deepcopy(current_params)\n",
    "\t\tself.best_score_ = current_score\n",
    "\n",
    "\t\ttemperature = self.initial_temp\n",
    "\n",
    "\t\tfor iteration in range(self.n_iter):\n",
    "\t\t\t# Generate neighbor, store the score for cv_results and Accept or reject neighbor\n",
    "\t\t\tneighbor_params = self._neighbor_params(current_params)\n",
    "\t\t\tneighbor_score = self._evaluate_params(neighbor_params, X, y)\n",
    "\t\t\tself.cv_results_[\"mean_test_score\"].append(neighbor_score)\n",
    "\n",
    "\t\t\tif neighbor_score > current_score:  # Better solution - always accept\n",
    "\t\t\t\tcurrent_params = neighbor_params\n",
    "\t\t\t\tcurrent_score = neighbor_score\n",
    "\t\t\telse:  # Worse solution - accept with probability\n",
    "\t\t\t\tif (\n",
    "\t\t\t\t\trandom.random()\n",
    "\t\t\t\t\t< np.exp(neighbor_score - current_score / temperature)\n",
    "\t\t\t\t\tif temperature > 0\n",
    "\t\t\t\t\telse 0\n",
    "\t\t\t\t):\n",
    "\t\t\t\t\tcurrent_params = neighbor_params\n",
    "\t\t\t\t\tcurrent_score = neighbor_score\n",
    "\n",
    "\t\t\tif current_score > self.best_score_:  # Update best solution\n",
    "\t\t\t\tself.best_params_ = deepcopy(current_params)\n",
    "\t\t\t\tself.best_score_ = current_score\n",
    "\n",
    "\t\t\t# Cool down\n",
    "\t\t\ttemperature = max(temperature * self.cooling_rate, self.min_temp)\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\n",
    "class GeneticSearch:\n",
    "\t\"\"\"\n",
    "\tCustom Genetic Algorithm implementation for hyperparameter optimization.\n",
    "\n",
    "\tGenetic Algorithm Search:\n",
    "\t- Population-based optimization: Maintains diverse parameter sets\n",
    "\t- Tournament selection: Robust parent selection mechanism\n",
    "\t- Uniform crossover: Parameter exchange between parents\n",
    "\t- Adaptive mutation: Random parameter changes with configurable rate\n",
    "\t- Elite preservation: Keeps best solutions across generations\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tparam_space: dict,\n",
    "\t\tpopulation_size: int = 20,\n",
    "\t\tn_generations: int = 10,\n",
    "\t\tmutation_rate: float = 0.1,\n",
    "\t\tcrossover_rate: float = 0.8,\n",
    "\t\telite_size: int = 2,\n",
    "\t\trandom_state: int = 42,\n",
    "\t):\n",
    "\t\tself.param_space = param_space\n",
    "\t\tself.population_size = population_size\n",
    "\t\tself.n_generations = n_generations\n",
    "\t\tself.mutation_rate = mutation_rate\n",
    "\t\tself.crossover_rate = crossover_rate\n",
    "\t\tself.elite_size = elite_size\n",
    "\t\tself.random_state = random_state\n",
    "\t\tself.best_params_ = None\n",
    "\t\tself.best_score_ = -np.inf\n",
    "\t\tself.cv_results_ = {\"mean_test_score\": []}\n",
    "\n",
    "\tdef _create_individual(self) -> dict:\n",
    "\t\t\"\"\"Create a random individual (parameter set).\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\tkey: (\n",
    "\t\t\t\tvalues.rvs(random_state=self.random_state)\n",
    "\t\t\t\tif hasattr(values, \"rvs\")  # scipy distribution\n",
    "\t\t\t\telse random.choice(values)\n",
    "\t\t\t\tif isinstance(values, (list, tuple))  # Ensure it's a sequence\n",
    "\t\t\t\telse values  # Use the value directly if it's not sampleable\n",
    "\t\t\t)\n",
    "\t\t\tfor key, values in self.param_space.items()\n",
    "\t\t}\n",
    "\n",
    "\tdef _crossover(self, parent1: dict, parent2: dict) -> tuple[dict, dict]:\n",
    "\t\t\"\"\"Create two offspring from two parents using uniform crossover.\"\"\"\n",
    "\t\tchild1, child2 = deepcopy(parent1), deepcopy(parent2)\n",
    "\n",
    "\t\tfor key in parent1.keys():\n",
    "\t\t\tif random.random() < 0.5:  # Swap parameter values\n",
    "\t\t\t\tchild1[key], child2[key] = child2[key], child1[key]\n",
    "\n",
    "\t\treturn child1, child2\n",
    "\n",
    "\tdef _mutate(self, individual: dict) -> dict:\n",
    "\t\t\"\"\"Mutate an individual by randomly changing some parameters.\"\"\"\n",
    "\t\tmutated = deepcopy(individual)\n",
    "\n",
    "\t\tfor key in individual.keys():\n",
    "\t\t\tif random.random() < self.mutation_rate:\n",
    "\t\t\t\tmutated[key] = (\n",
    "\t\t\t\t\tself.param_space[key].rvs(random_state=self.random_state)\n",
    "\t\t\t\t\tif hasattr(self.param_space[key], \"rvs\")  # continuous parameter\n",
    "\t\t\t\t\telse random.choice(self.param_space[key])  # discrete parameter\n",
    "\t\t\t\t)\n",
    "\t\treturn mutated\n",
    "\n",
    "\tdef _tournament_selection(\n",
    "\t\tself, population: list, fitness_scores: list, tournament_size: int = 3\n",
    "\t) -> dict:\n",
    "\t\t\"\"\"Select an individual using tournament selection.\"\"\"\n",
    "\t\ttournament_indices = random.sample(\n",
    "\t\t\trange(len(population)), min(tournament_size, len(population))\n",
    "\t\t)\n",
    "\t\treturn population[\n",
    "\t\t\ttournament_indices[\n",
    "\t\t\t\tnp.argmax([fitness_scores[i] for i in tournament_indices])\n",
    "\t\t\t]\n",
    "\t\t]\n",
    "\n",
    "\tdef _evaluate_params(self, params: dict, X: DataFrame, y: Series) -> float:\n",
    "\t\t\"\"\"Evaluate parameter configuration using cross-validation.\"\"\"\n",
    "\t\treturn np.mean(\n",
    "\t\t\tcross_val_score(\n",
    "\t\t\t\tNoveltyDetectionEnsemble(**params, kernel=\"rbf\"),\n",
    "\t\t\t\tX,\n",
    "\t\t\t\ty,\n",
    "\t\t\t\tcv=4,\n",
    "\t\t\t\tscoring=score_function,\n",
    "\t\t\t)\n",
    "\t\t).astype(float)\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y: Series):\n",
    "\t\t\"\"\"Fit the genetic algorithm search.\"\"\"\n",
    "\t\trandom.seed(self.random_state)\n",
    "\t\tnp.random.seed(self.random_state)\n",
    "\t\tpopulation = [self._create_individual() for _ in range(self.population_size)]\n",
    "\n",
    "\t\tfor generation in range(self.n_generations):  # Evaluate fitness\n",
    "\t\t\tprint(f\"Evaluating Generation {generation}\")\n",
    "\t\t\tfitness_scores = []\n",
    "\t\t\tfor individual in population:\n",
    "\t\t\t\tscore = self._evaluate_params(individual, X, y)\n",
    "\t\t\t\tfitness_scores.append(score)\n",
    "\t\t\t\tself.cv_results_[\"mean_test_score\"].append(score)\n",
    "\n",
    "\t\t\t\tif score > self.best_score_:\n",
    "\t\t\t\t\tself.best_params_ = deepcopy(individual)\n",
    "\t\t\t\t\tself.best_score_ = score\n",
    "\n",
    "\t\t\t# Create next generation | Elite selection - keep best individuals\n",
    "\t\t\tnew_population = [\n",
    "\t\t\t\tdeepcopy(population[idx])\n",
    "\t\t\t\tfor idx in np.argsort(fitness_scores)[-self.elite_size :]\n",
    "\t\t\t]\n",
    "\t\t\t# Generate offspring\n",
    "\t\t\twhile len(new_population) < self.population_size:\n",
    "\t\t\t\t# Selection\n",
    "\t\t\t\tparent1 = self._tournament_selection(population, fitness_scores)\n",
    "\t\t\t\tparent2 = self._tournament_selection(population, fitness_scores)\n",
    "\t\t\t\t# Crossover\n",
    "\t\t\t\tif random.random() < self.crossover_rate:\n",
    "\t\t\t\t\tchild1, child2 = self._crossover(parent1, parent2)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tchild1, child2 = deepcopy(parent1), deepcopy(parent2)\n",
    "\t\t\t\t# Mutation\n",
    "\t\t\t\tnew_population.extend([self._mutate(child1), self._mutate(child2)])\n",
    "\t\t\t# Trim to exact population size\n",
    "\t\t\tpopulation = new_population[: self.population_size]\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\n",
    "class BayesianSearch:\n",
    "\t\"\"\"Wrapper for Bayesian Optimization to match sklearn API.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tparam_bounds: dict,\n",
    "\t\tn_iter: int = 100,\n",
    "\t\trandom_state: int = 42,\n",
    "\t\tX_train: DataFrame | None = None,\n",
    "\t\ty_train: Series | None = None,\n",
    "\t):\n",
    "\t\tself.param_bounds = param_bounds\n",
    "\t\tself.n_iter = n_iter\n",
    "\t\tself.random_state = random_state\n",
    "\t\tself.X_train = X_train\n",
    "\t\tself.y_train = y_train\n",
    "\t\tself.best_params_ = None\n",
    "\t\tself.best_score_ = -np.inf\n",
    "\t\tself.cv_results_ = {\"mean_test_score\": []}\n",
    "\n",
    "\tdef _params_to_model_config(self, **params) -> dict:\n",
    "\t\t\"\"\"Convert Bayesian params to model configuration.\"\"\"\n",
    "\t\tdiscrete_params = {  # need rounding\n",
    "\t\t\t\"sequence_length\": int(params[\"sequence_length\"]),\n",
    "\t\t\t\"hidden_size\": int(params[\"hidden_size\"]),\n",
    "\t\t\t\"n_layers\": int(round(params[\"n_layers\"])),\n",
    "\t\t\t\"epochs\": int(params[\"epochs\"]),\n",
    "\t\t\t\"batch_size\": int(params[\"batch_size\"]),\n",
    "\t\t\t\"n_estimators\": int(params[\"n_estimators\"]),\n",
    "\t\t\t\"max_samples\": int(params[\"max_samples\"]),\n",
    "\t\t}\n",
    "\t\tcontinuous_params = {\n",
    "\t\t\t\"dropout\": params[\"dropout\"],\n",
    "\t\t\t\"learning_rate\": params[\"learning_rate\"],\n",
    "\t\t\t\"contamination\": params[\"contamination\"],\n",
    "\t\t\t\"nu\": params[\"nu\"],\n",
    "\t\t\t\"gamma\": 10 ** params[\"gamma_log\"],\n",
    "\t\t}\n",
    "\t\t# Weights derived from weight_lstm\n",
    "\t\tweight_others = (1 - (weight_lstm := params[\"weight_lstm\"])) / 2\n",
    "\t\treturn {\n",
    "\t\t\t**discrete_params,\n",
    "\t\t\t**continuous_params,\n",
    "\t\t\t**{\"weights\": (weight_lstm, weight_others, weight_others)},\n",
    "\t\t\t\"n_features\": len(IMU_COLUMNS),\n",
    "\t\t}\n",
    "\n",
    "\tdef _objective(self, **params) -> float:\n",
    "\t\t\"\"\"Objective function for Bayesian optimization.\"\"\"\n",
    "\t\t# Simple train/test split evaluation (you can modify for CV)\n",
    "\t\tassert self.X_train is not None, \"Training data not set.\"\n",
    "\t\tassert self.y_train is not None, \"Training Labels not set.\"\n",
    "\t\tscore = score_function(\n",
    "\t\t\tNoveltyDetectionEnsemble(**self._params_to_model_config(**params)).fit(\n",
    "\t\t\t\tself.X_train\n",
    "\t\t\t),\n",
    "\t\t\tself.X_train,\n",
    "\t\t\tself.y_train,\n",
    "\t\t\tself.X_train,\n",
    "\t\t)[\"f1_score\"]\n",
    "\t\tself.cv_results_[\"mean_test_score\"].append(score)\n",
    "\t\treturn score\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y: Series):\n",
    "\t\t\"\"\"Fit using Bayesian optimization.\"\"\"\n",
    "\t\tself.X_train = X\n",
    "\t\tself.y_train = y\n",
    "\n",
    "\t\toptimizer = BayesianOptimization(\n",
    "\t\t\tf=self._objective,\n",
    "\t\t\tpbounds=self.param_bounds,\n",
    "\t\t\trandom_state=self.random_state,\n",
    "\t\t\tallow_duplicate_points=True,\n",
    "\t\t)\n",
    "\t\toptimizer.maximize(init_points=10, n_iter=self.n_iter)\n",
    "\n",
    "\t\tassert optimizer.max, \"Optimization did not find any maximum. Somehow...\"\n",
    "\n",
    "\t\tself.best_params_ = self._params_to_model_config(**optimizer.max[\"params\"])\n",
    "\t\tself.best_score_ = optimizer.max[\"target\"]\n",
    "\n",
    "\t\treturn self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **INCREMENTAL DATASET IMPLEMENTATION**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalDataset:\n",
    "\t\"\"\"Manages data splits for incremental/continual learning experiments.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tpath: Path,\n",
    "\t\tinitial_activities: int = 3,\n",
    "\t\ttest_size: int = 2,\n",
    "\t\tactivity_order: list[int] | None = None,\n",
    "\t\tordering_method: Literal[\"pca\", \"statistical\", \"variance\", \"frequency\"] = \"pca\",\n",
    "\t):\n",
    "\t\tself.data, self.labels = self.load_data(path)\n",
    "\t\tself.df = self.data.merge(self.labels, how=\"left\", on=\"id\")\n",
    "\n",
    "\t\tself.initial_activities = initial_activities\n",
    "\t\tself.test_size = test_size\n",
    "\n",
    "\t\tself.activity_order = (\n",
    "\t\t\tself.get_activity_order(self.data, self.labels, method=ordering_method)\n",
    "\t\t\tif activity_order is None\n",
    "\t\t\telse activity_order\n",
    "\t\t)\n",
    "\t\tself.n_activities = len(self.activity_order)\n",
    "\t\tself.n_folds = (self.n_activities - initial_activities) // test_size\n",
    "\n",
    "\t\tprint(\"\\nðŸ“¦ Incremental Dataset Configuration:\")\n",
    "\t\tprint(f\"  Total activities: {self.n_activities}\")\n",
    "\t\tprint(f\"  Initial training activities: {initial_activities}\")\n",
    "\t\tprint(f\"  Test size per fold: {test_size}\")\n",
    "\t\tprint(f\"  Number of folds: {self.n_folds}\")\n",
    "\t\tprint(f\"  Activity order: {self.activity_order}\")\n",
    "\n",
    "\tdef read_w_log(self, path: Path, filename: str) -> tuple[DataFrame, str]:\n",
    "\t\t\"\"\"Read and preprocess PAMAP2 data file.\"\"\"\n",
    "\t\tprint(f\"Reading: {filename}\", end=\"\\r\")\n",
    "\t\tdf = read_csv(os.path.join(path, filename), sep=r\"\\s+\", header=None)\n",
    "\t\tdf.columns = COLUMNS\n",
    "\t\treturn (\n",
    "\t\t\tdf.loc[:, ~df.columns.str.contains(r\"orient|acc6g\", regex=True)],\n",
    "\t\t\tfilename.split(\".\")[0][-2:],\n",
    "\t\t)\n",
    "\n",
    "\tdef handle_nans(self, df: DataFrame) -> DataFrame:\n",
    "\t\t\"\"\"Handles NaN values in the sensor data with a time-series-aware strategy.\"\"\"\n",
    "\t\tdf = df.copy()\n",
    "\t\tfor col in IMU_COLUMNS:\n",
    "\t\t\tdf.loc[:, col] = (\n",
    "\t\t\t\tdf[col]\n",
    "\t\t\t\t.ffill(limit=2)\n",
    "\t\t\t\t.interpolate(\"linear\", limit=5, limit_direction=\"both\")\n",
    "\t\t\t)\n",
    "\t\treturn df.dropna(subset=IMU_COLUMNS)\n",
    "\n",
    "\tdef normalize_features(\n",
    "\t\tself,\n",
    "\t\tX_train: DataFrame,\n",
    "\t\tX_val: DataFrame | None = None,\n",
    "\t\tX_test: DataFrame | None = None,\n",
    "\t\tscaler: RobustScaler | None = None,\n",
    "\t\tforce_refit: bool = False,\n",
    "\t) -> tuple[DataFrame, DataFrame | None, DataFrame | None, RobustScaler]:\n",
    "\t\t\"\"\"Normalizes IMU features using RobustScaler.\"\"\"\n",
    "\t\tif scaler is None or force_refit:\n",
    "\t\t\tscaler = RobustScaler().fit(X_train[IMU_COLUMNS])\n",
    "\n",
    "\t\tX_train = X_train.copy()\n",
    "\t\tX_train.loc[:, IMU_COLUMNS] = scaler.transform(X_train[IMU_COLUMNS])\n",
    "\n",
    "\t\tX_val_norm = None\n",
    "\t\tif X_val is not None:\n",
    "\t\t\tX_val_norm = X_val.copy()\n",
    "\t\t\tX_val_norm.loc[:, IMU_COLUMNS] = scaler.transform(X_val[IMU_COLUMNS])\n",
    "\n",
    "\t\tX_test_norm = None\n",
    "\t\tif X_test is not None:\n",
    "\t\t\tX_test_norm = X_test.copy()\n",
    "\t\t\tX_test_norm.loc[:, IMU_COLUMNS] = scaler.transform(X_test[IMU_COLUMNS])\n",
    "\n",
    "\t\treturn X_train, X_val_norm, X_test_norm, scaler\n",
    "\n",
    "\tdef load_data(self, base_path: Path) -> tuple[DataFrame, DataFrame]:\n",
    "\t\t\"\"\"Load and preprocess all PAMAP2 protocol data.\"\"\"\n",
    "\t\tlogger.info(\"Loading PAMAP2 data...\")\n",
    "\t\tif (data_csv := base_path / \"data.csv\").exists() and (\n",
    "\t\t\tlabels_csv := base_path / \"labels.csv\"\n",
    "\t\t).exists():\n",
    "\t\t\tlogger.info(\"Found preprocessed data files. Loading...\")\n",
    "\t\t\treturn read_csv(data_csv), read_csv(labels_csv)\n",
    "\n",
    "\t\tdata, labels = [], []\n",
    "\t\tfor df, subject in [\n",
    "\t\t\tself.read_w_log(base_path, filename)\n",
    "\t\t\tfor filename in os.listdir(base_path)\n",
    "\t\t\tif filename.endswith(\".dat\")\n",
    "\t\t]:\n",
    "\t\t\tdf = self.handle_nans(df[~df[\"activity\"].isin([0, 24])])\n",
    "\t\t\tdf[\"subject\"] = str(subject)\n",
    "\t\t\tdf[\"timestamp\"] = to_datetime(df[\"timestamp\"], unit=\"s\").dt.time\n",
    "\t\t\tdf[\"id\"] = df[\"subject\"] + \"_\" + df[\"timestamp\"].astype(str)\n",
    "\n",
    "\t\t\tdata.append(df.drop(columns=[\"timestamp\", \"activity\", \"heart_rate\"]))\n",
    "\t\t\tlabels.append(df[[\"id\", \"timestamp\", \"activity\"]])\n",
    "\n",
    "\t\tdata, labels = concat(data), concat(labels)\n",
    "\t\tdata[\"subject\"] = data[\"subject\"].astype(\"category\")\n",
    "\t\tlabels[\"activity\"] = labels[\"activity\"].astype(\"category\")\n",
    "\n",
    "\t\tdata.to_csv(\"../data/PAMAP2/data.csv\", index=False)\n",
    "\t\tlabels.to_csv(\"../data/PAMAP2/labels.csv\", index=False)\n",
    "\n",
    "\t\treturn data, labels\n",
    "\n",
    "\tdef get_activity_order(\n",
    "\t\tself,\n",
    "\t\tdata: DataFrame,\n",
    "\t\tlabels: DataFrame,\n",
    "\t\tmethod: Literal[\"pca\", \"statistical\", \"variance\", \"frequency\"] = \"pca\",\n",
    "\t) -> list[int]:\n",
    "\t\t\"\"\"Order activities by distinctiveness for incremental learning.\"\"\"\n",
    "\t\tprint(f\"\\nðŸ” Ordering activities for incremental learning using '{method}'...\")\n",
    "\t\tdf = data.merge(labels, how=\"left\", on=\"id\")\n",
    "\n",
    "\t\tif method == \"frequency\":\n",
    "\t\t\tactivity_counts = df[\"activity\"].value_counts()\n",
    "\t\t\tprint(\"\\nðŸ“Š Activity ordering (most â†’ least frequent):\")\n",
    "\t\t\tfor i, act in enumerate(ordered := activity_counts.index.tolist(), 1):\n",
    "\t\t\t\tprint(f\"  {i}. Activity {act}: {activity_counts[act]:,} samples\")\n",
    "\t\t\treturn ordered\n",
    "\n",
    "\t\tactivity_stats = {}\n",
    "\t\tactivity_counts = {}\n",
    "\t\tfor activity in df[\"activity\"].unique():\n",
    "\t\t\tactivity_data = df[df[\"activity\"] == activity][IMU_COLUMNS]\n",
    "\t\t\tactivity_counts[activity] = len(activity_data)\n",
    "\n",
    "\t\t\tif method == \"pca\":\n",
    "\t\t\t\tactivity_stats[activity] = (\n",
    "\t\t\t\t\tPCA(n_components=min(10, len(IMU_COLUMNS), len(activity_data)))\n",
    "\t\t\t\t\t.fit_transform(activity_data)\n",
    "\t\t\t\t\t.mean(axis=0)\n",
    "\t\t\t\t)\n",
    "\t\t\telif method == \"statistical\":\n",
    "\t\t\t\tactivity_stats[activity] = concatenate(\n",
    "\t\t\t\t\t[\n",
    "\t\t\t\t\t\tactivity_data.mean().values,\n",
    "\t\t\t\t\t\tactivity_data.std().values,\n",
    "\t\t\t\t\t\tactivity_data.quantile(0.25).values,\n",
    "\t\t\t\t\t\tactivity_data.quantile(0.75).values,\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t)\n",
    "\t\t\telif method == \"variance\":\n",
    "\t\t\t\tactivity_stats[activity] = concatenate(\n",
    "\t\t\t\t\t[\n",
    "\t\t\t\t\t\tactivity_data.var().values,\n",
    "\t\t\t\t\t\tactivity_data.abs().mean().values,\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tactivities = list(activity_stats.keys())\n",
    "\t\tdistances = squareform(\n",
    "\t\t\tpdist(\n",
    "\t\t\t\tvstack([activity_stats[act] for act in activities]), metric=\"euclidean\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\tordered = []\n",
    "\t\tremaining = set(range(len(activities)))\n",
    "\t\tavg_distances = distances.mean(axis=1)\n",
    "\t\tfirst_idx = argmax(\n",
    "\t\t\t[\n",
    "\t\t\t\t-avg_distances[i] if i in remaining else float(\"-inf\")\n",
    "\t\t\t\tfor i in range(len(activities))\n",
    "\t\t\t]\n",
    "\t\t).astype(int)\n",
    "\t\tordered.append(first_idx)\n",
    "\t\tremaining.remove(first_idx)\n",
    "\n",
    "\t\twhile remaining:\n",
    "\t\t\tmin_dists = [\n",
    "\t\t\t\tmin([distances[idx, sel_idx] for sel_idx in ordered])\n",
    "\t\t\t\tif idx in remaining\n",
    "\t\t\t\telse float(\"inf\")\n",
    "\t\t\t\tfor idx in range(len(activities))\n",
    "\t\t\t]\n",
    "\t\t\tnext_idx = argmax(\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t-min_dists[i] if i in remaining else float(\"-inf\")\n",
    "\t\t\t\t\tfor i in range(len(activities))\n",
    "\t\t\t\t]\n",
    "\t\t\t).astype(int)\n",
    "\t\t\tordered.append(next_idx)\n",
    "\t\t\tremaining.remove(next_idx)\n",
    "\n",
    "\t\tordered_activities = [activities[i] for i in ordered]\n",
    "\t\tprint(\"\\nðŸ“Š Incremental learning order:\")\n",
    "\t\tfor i, act in enumerate(ordered_activities, 1):\n",
    "\t\t\tprint(f\"  {act}: {activity_counts[act]:,} samples\")\n",
    "\n",
    "\t\treturn ordered_activities\n",
    "\n",
    "\tdef get_fold(\n",
    "\t\tself,\n",
    "\t\tfold_idx: int,\n",
    "\t\tval_split: float = 0.2,\n",
    "\t\tnormalize: bool = True,\n",
    "\t\tscaler: RobustScaler | None = None,\n",
    "\t) -> tuple[\n",
    "\t\tDataFrame,\n",
    "\t\tDataFrame | None,\n",
    "\t\tDataFrame | None,\n",
    "\t\tDataFrame,\n",
    "\t\tDataFrame | None,\n",
    "\t\tDataFrame,\n",
    "\t\tRobustScaler | None,\n",
    "\t]:\n",
    "\t\t\"\"\"Get train/val/test split for a specific fold.\"\"\"\n",
    "\t\tif fold_idx >= self.n_folds:\n",
    "\t\t\traise ValueError(f\"fold_idx {fold_idx} >= n_folds {self.n_folds}\")\n",
    "\n",
    "\t\tn_train_activities = self.initial_activities + fold_idx * self.test_size\n",
    "\t\ttrain_activities = self.activity_order[:n_train_activities]\n",
    "\t\ttest_activities = self.activity_order[\n",
    "\t\t\tn_train_activities : n_train_activities + self.test_size\n",
    "\t\t]\n",
    "\n",
    "\t\tprint(f\"\\nðŸ”„ Fold {fold_idx}:\")\n",
    "\t\tprint(f\"  Training activities (normal): {train_activities}\")\n",
    "\t\tprint(f\"  Test activities (novel): {test_activities}\")\n",
    "\n",
    "\t\ttrain_mask = self.df[\"activity\"].isin(train_activities)\n",
    "\t\ttest_mask = self.df[\"activity\"].isin(test_activities)\n",
    "\n",
    "\t\ttrain_val_df = self.df[train_mask].copy()\n",
    "\t\ttest_df = self.df[test_mask].copy()\n",
    "\n",
    "\t\tsplit_idx = int(len(train_val_df) * (1 - val_split))\n",
    "\t\ttrain_df = train_val_df.iloc[:split_idx].copy()\n",
    "\t\tval_df = train_val_df.iloc[split_idx:].copy() if val_split > 0 else None\n",
    "\n",
    "\t\tfeature_cols = [col for col in self.data.columns if col != \"id\"]\n",
    "\n",
    "\t\tX_train = train_df[feature_cols].reset_index(drop=True)\n",
    "\t\tX_val = (\n",
    "\t\t\tval_df[feature_cols].reset_index(drop=True) if val_df is not None else None\n",
    "\t\t)\n",
    "\t\tX_test = test_df[feature_cols].reset_index(drop=True)\n",
    "\n",
    "\t\ty_train = train_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\t\ty_val = (\n",
    "\t\t\tval_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\t\t\tif val_df is not None\n",
    "\t\t\telse None\n",
    "\t\t)\n",
    "\t\ty_test = test_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\n",
    "\t\tprint(f\"  Train samples: {len(X_train):,}\")\n",
    "\t\tif X_val is not None:\n",
    "\t\t\tprint(f\"  Val samples: {len(X_val):,}\")\n",
    "\t\tprint(f\"  Test samples: {len(X_test):,}\")\n",
    "\n",
    "\t\tif normalize:\n",
    "\t\t\tX_train, X_val, X_test, scaler = self.normalize_features(\n",
    "\t\t\t\tX_train, X_val, X_test, scaler=scaler\n",
    "\t\t\t)\n",
    "\t\treturn X_train, X_val, X_test, y_train, y_val, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRIALS AND EVALUATION PHASE**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_space(\n",
    "\tsearch_method: Literal[\"Random\", \"Annealing\", \"Genetic\", \"Bayesian\"],\n",
    "\tuse_log_dist: bool = True,\n",
    ") -> dict:\n",
    "\t\"\"\"\n",
    "\tGet parameter space for ensemble hyperparameter optimization.\n",
    "\n",
    "\tArgs:\n",
    "\t\tsearch_method: The optimization method to use\n",
    "\t\tuse_log_dist: Whether to use log-uniform distributions for continuous params.\n",
    "\t\tused only for RandomSearchCV params.\n",
    "\n",
    "\tReturns:\n",
    "\t\tParameter space dictionary appropriate for the search method\n",
    "\t\"\"\"\n",
    "\tif search_method == \"Random\":\n",
    "\t\t# Log-uniform distributions for better coverage\n",
    "\t\tif use_log_dist:\n",
    "\t\t\treturn {  # Discrete uniform distributions\n",
    "\t\t\t\t\"sequence_length\": [50, 75, 100, 125, 150, 175, 200, 225],\n",
    "\t\t\t\t\"hidden_size\": [32, 48, 64, 80, 128, 256, 512],\n",
    "\t\t\t\t\"n_layers\": [1, 2, 3, 4],\n",
    "\t\t\t\t\"dropout\": [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5],\n",
    "\t\t\t\t\"epochs\": [20, 25, 30, 45, 50],\n",
    "\t\t\t\t\"batch_size\": [64, 128, 256],\n",
    "\t\t\t\t\"learning_rate\": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2],\n",
    "\t\t\t\t\"n_estimators\": [100, 200, 300],\n",
    "\t\t\t\t\"max_samples\": [128, 256, 512],\n",
    "\t\t\t\t\"contamination\": [0.01, 0.05, 0.1, 0.15],\n",
    "\t\t\t\t\"nu\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "\t\t\t\t\"gamma\": [1e-4, 1e-3, 1e-2, 0.1, 0.5, 1.0, \"scale\"],\n",
    "\t\t\t\t\"weights\": [\n",
    "\t\t\t\t\t(0.5, 0.25, 0.25),\n",
    "\t\t\t\t\t(0.4, 0.3, 0.3),\n",
    "\t\t\t\t\t(0.33, 0.33, 0.34),\n",
    "\t\t\t\t\t(0.6, 0.2, 0.2),\n",
    "\t\t\t\t],\n",
    "\t\t\t}\n",
    "\t\telse:\n",
    "\t\t\treturn {  # Discrete uniform distributions\n",
    "\t\t\t\t# LSTM-AE params\n",
    "\t\t\t\t\"sequence_length\": [50, 75, 100, 150, 200],\n",
    "\t\t\t\t\"hidden_size\": [32, 64, 128, 256],\n",
    "\t\t\t\t\"n_layers\": [1, 2, 3],\n",
    "\t\t\t\t\"dropout\": loguniform(0.1, 0.5),\n",
    "\t\t\t\t\"epochs\": [20, 30, 50],\n",
    "\t\t\t\t\"batch_size\": [64, 128, 256],\n",
    "\t\t\t\t\"learning_rate\": loguniform(1e-4, 1e-2),\n",
    "\t\t\t\t# Isolation Forest params\n",
    "\t\t\t\t\"n_estimators\": [100, 200, 300],\n",
    "\t\t\t\t\"max_samples\": [128, 256, 512],\n",
    "\t\t\t\t\"contamination\": loguniform(0.01, 0.15),\n",
    "\t\t\t\t# OC-SVM params\n",
    "\t\t\t\t\"nu\": loguniform(0.01, 0.3),\n",
    "\t\t\t\t\"gamma\": loguniform(1e-4, 1.0),\n",
    "\t\t\t\t# Ensemble weights\n",
    "\t\t\t\t\"weights\": [\n",
    "\t\t\t\t\t(0.5, 0.25, 0.25),\n",
    "\t\t\t\t\t(0.4, 0.3, 0.3),\n",
    "\t\t\t\t\t(0.33, 0.33, 0.34),\n",
    "\t\t\t\t\t(0.6, 0.2, 0.2),\n",
    "\t\t\t\t],\n",
    "\t\t\t}\n",
    "\tif search_method in [\"Annealing\", \"Genetic\"]:\n",
    "\t\treturn {  # Using log-uniform for these methods as well\n",
    "\t\t\t\"sequence_length\": [50, 75, 100, 150, 200],\n",
    "\t\t\t\"hidden_size\": [32, 64, 128, 256],\n",
    "\t\t\t\"n_layers\": [1, 2, 3],\n",
    "\t\t\t\"dropout\": loguniform(0.1, 0.5),\n",
    "\t\t\t\"epochs\": [20, 30, 50],\n",
    "\t\t\t\"batch_size\": [64, 128, 256],\n",
    "\t\t\t\"learning_rate\": loguniform(1e-4, 1e-2),\n",
    "\t\t\t\"n_estimators\": [100, 200, 300],\n",
    "\t\t\t\"max_samples\": [128, 256, 512],\n",
    "\t\t\t\"contamination\": loguniform(0.01, 0.15),\n",
    "\t\t\t\"nu\": loguniform(0.01, 0.3),\n",
    "\t\t\t\"gamma\": loguniform(1e-4, 1.0),\n",
    "\t\t\t\"weights\": [\n",
    "\t\t\t\t(0.5, 0.25, 0.25),\n",
    "\t\t\t\t(0.4, 0.3, 0.3),\n",
    "\t\t\t\t(0.33, 0.33, 0.34),\n",
    "\t\t\t\t(0.6, 0.2, 0.2),\n",
    "\t\t\t],\n",
    "\t\t}\n",
    "\telif search_method == \"Bayesian\":\n",
    "\t\treturn {  # Bounds for Bayesian optimization (continuous parameters only)\n",
    "\t\t\t\"sequence_length\": (50, 200),\n",
    "\t\t\t\"hidden_size\": (32, 256),\n",
    "\t\t\t\"n_layers\": (1, 3),\n",
    "\t\t\t\"dropout\": (0.1, 0.5),\n",
    "\t\t\t\"epochs\": (20, 50),\n",
    "\t\t\t\"batch_size\": (64, 256),\n",
    "\t\t\t\"learning_rate\": (1e-4, 1e-2),\n",
    "\t\t\t\"n_estimators\": (100, 300),\n",
    "\t\t\t\"max_samples\": (128, 512),\n",
    "\t\t\t\"contamination\": (0.01, 0.15),\n",
    "\t\t\t\"nu\": (0.01, 0.3),\n",
    "\t\t\t\"gamma_log\": (-4, 0),  # log10(gamma), so 1e-4 to 1\n",
    "\t\t\t\"weight_lstm\": (0.2, 0.6),  # i'll derive other weights from this\n",
    "\t\t}\n",
    "\traise ValueError(f\"Unknown search method: {search_method}\")\n",
    "\n",
    "\n",
    "def train_search_method(\n",
    "\ttraining_data: DataFrame,\n",
    "\ttrain_targets: Series,\n",
    "\tsearch_type: Literal[\"Random\", \"Annealing\", \"Genetic\", \"Bayesian\"],\n",
    "\tparams_type: Literal[\"Continuous\", \"Discrete\"] = \"Continuous\",\n",
    "\tn_iter: int = 100,\n",
    "\tcv: int = 4,\n",
    "\tverbose: int = 1,\n",
    "\trandom_state: int = 42,\n",
    ") -> RandomizedSearchCV | SimulatedAnnealingSearch | GeneticSearch | BayesianSearch:\n",
    "\t\"\"\"\n",
    "\tFactory function to create and train hyperparameter search method.\n",
    "\n",
    "\tArgs:\n",
    "\t\ttraining_data: Training feature data\n",
    "\t\ttrain_targets: Training labels\n",
    "\t\tsearch_type: Type of search method to use\n",
    "\t\tn_iter: Number of iterations for the search\n",
    "\t\tcv: Number of cross-validation folds\n",
    "\t\tverbose: Verbosity level\n",
    "\t\trandom_state: Random seed\n",
    "\n",
    "\tReturns:\n",
    "\t\tFitted search object\n",
    "\t\"\"\"\n",
    "\t# Determine if using log distributions for Random search\n",
    "\tuse_log_dist = search_type == \"Random\" and params_type == \"Discrete\"\n",
    "\tparams = get_param_space(\n",
    "\t\t\"Random\" if use_log_dist else search_type, use_log_dist=use_log_dist\n",
    "\t)\n",
    "\tparams[\"n_features\"] = len(IMU_COLUMNS)\n",
    "\tif search_type == \"Annealing\":\n",
    "\t\treturn SimulatedAnnealingSearch(\n",
    "\t\t\tparam_space=params,\n",
    "\t\t\tn_iter=n_iter,\n",
    "\t\t\tinitial_temp=1.0,\n",
    "\t\t\tcooling_rate=0.95,\n",
    "\t\t\trandom_state=random_state,\n",
    "\t\t).fit(training_data, train_targets)\n",
    "\n",
    "\tif search_type == \"Genetic\":\n",
    "\t\tpopulation_size = min(20, n_iter // 5)\n",
    "\t\treturn GeneticSearch(\n",
    "\t\t\tparam_space=params,\n",
    "\t\t\tpopulation_size=population_size,\n",
    "\t\t\tn_generations=n_iter // population_size,\n",
    "\t\t\tmutation_rate=0.1,\n",
    "\t\t\tcrossover_rate=0.8,\n",
    "\t\t\trandom_state=random_state,\n",
    "\t\t).fit(training_data, train_targets)\n",
    "\n",
    "\tif search_type == \"Bayesian\":\n",
    "\t\treturn BayesianSearch(\n",
    "\t\t\tparam_bounds=params,\n",
    "\t\t\tn_iter=n_iter,\n",
    "\t\t\trandom_state=random_state,\n",
    "\t\t).fit(training_data, train_targets)\n",
    "\n",
    "\tif search_type == \"Random\":\n",
    "\t\treturn RandomizedSearchCV(\n",
    "\t\t\testimator=NoveltyDetectionEnsemble(n_features=len(IMU_COLUMNS)),\n",
    "\t\t\tparam_distributions=params,\n",
    "\t\t\tn_iter=n_iter,\n",
    "\t\t\tcv=cv,\n",
    "\t\t\tscoring=lambda est, X, y: score_function(est, X, y, X)[\"f1_score\"],\n",
    "\t\t\tverbose=verbose,\n",
    "\t\t\trandom_state=random_state,\n",
    "\t\t\terror_score=\"raise\",\n",
    "\t\t).fit(training_data, train_targets)\n",
    "\n",
    "\traise ValueError(f\"Unknown search type: {search_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_search_method(\n",
    "\tdataset: IncrementalDataset,\n",
    "\tsearch_type: Literal[\"Random\", \"Annealing\", \"Genetic\", \"Bayesian\"],\n",
    "\tlog_distributions: bool = False,\n",
    "\tn_iter: int = 50,\n",
    "\tlog_file: Path | str | None = None,\n",
    ") -> list[dict]:\n",
    "\t\"\"\"\n",
    "\tEvaluate a hyperparameter search method across all dataset folds.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdataset: IncrementalDataset instance with prepared folds\n",
    "\t\tsearch_type: Type of search method to use\n",
    "\t\tn_iter: Number of iterations for the search\n",
    "\t\tlog_file: Optional path to log file. If None, uses default naming\n",
    "\n",
    "\tReturns:\n",
    "\t\tList of evaluation metrics for each fold\n",
    "\t\"\"\"\n",
    "\t# Setup logging\n",
    "\tif log_file is None:\n",
    "\t\tlog_file = Path(f\"../reports/logs_{search_type.lower()}.log\")\n",
    "\n",
    "\tlog_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\tfile_logger = configure_file_logger(log_file)\n",
    "\n",
    "\tall_fold_results = []\n",
    "\n",
    "\tfor fold_idx in range(dataset.n_folds):\n",
    "\t\tX_train, X_val, X_test, y_train, y_val, y_test, scaler = dataset.get_fold(\n",
    "\t\t\tfold_idx, val_split=0.2, normalize=True\n",
    "\t\t)\n",
    "\t\tsearch = train_search_method(\n",
    "\t\t\ttraining_data=X_train,\n",
    "\t\t\ttrain_targets=Series([1] * len(X_train)),  # All training data is \"normal\"\n",
    "\t\t\tparams_type=\"Discrete\" if log_distributions else \"Continuous\",\n",
    "\t\t\tsearch_type=search_type,\n",
    "\t\t\tn_iter=n_iter,\n",
    "\t\t\trandom_state=42,\n",
    "\t\t)\n",
    "\t\tfile_logger.info(\n",
    "\t\t\tjson.dumps(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\tk: (v if not isinstance(v, (np.integer, np.floating)) else float(v))\n",
    "\t\t\t\t\tfor k, v in search.best_params_.items()\n",
    "\t\t\t\t},\n",
    "\t\t\t\tindent=2,\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\tall_fold_results.append(\n",
    "\t\t\t{  # Evaluate on test set (all test samples are \"novel\")\n",
    "\t\t\t\t\"fold\": fold_idx,\n",
    "\t\t\t\t\"search_type\": search_type,\n",
    "\t\t\t\t\"best_params\": search.best_params_,\n",
    "\t\t\t\t**score_function(\n",
    "\t\t\t\t\tNoveltyDetectionEnsemble(  # Final model with best params\n",
    "\t\t\t\t\t\tn_features=len(IMU_COLUMNS),\n",
    "\t\t\t\t\t\t**{\n",
    "\t\t\t\t\t\t\tk: v\n",
    "\t\t\t\t\t\t\tfor k, v in search.best_params_.items()\n",
    "\t\t\t\t\t\t\tif k != \"n_features\"\n",
    "\t\t\t\t\t\t},\n",
    "\t\t\t\t\t).fit(X_train),\n",
    "\t\t\t\t\tX_train,\n",
    "\t\t\t\t\tSeries([True] * len(X_test)),\n",
    "\t\t\t\t\tX_test,\n",
    "\t\t\t\t),\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\treturn all_fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Running Simulated Annealing...\n",
      "\n",
      "ðŸ”„ Fold 0:\n",
      "  Training activities (normal): [16, 13, 3]\n",
      "  Test activities (novel): [5, 17]\n",
      "  Train samples: 376,113\n",
      "  Val samples: 94,029\n",
      "  Test samples: 336,855\n",
      "{'sequence_length': 50, 'hidden_size': 32, 'n_layers': 3, 'dropout': np.float64(0.18272261776066237), 'epochs': 30, 'batch_size': 64, 'learning_rate': np.float64(0.0005611516415334506), 'n_estimators': 100, 'max_samples': 128, 'contamination': np.float64(0.02757359293934948), 'nu': np.float64(0.03574712922600244), 'gamma': np.float64(0.003148911647956862), 'weights': (0.5, 0.25, 0.25), 'n_features': 30}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object NoveltyDetectionEnsemble(contamination=np.float64(0.02757359293934948),\n                         dropout=np.float64(0.18272261776066237), epochs=30,\n                         gamma=np.float64(0.003148911647956862), hidden_size=32,\n                         learning_rate=np.float64(0.0005611516415334506),\n                         max_samples=128, n_estimators=100, n_features=30,\n                         n_layers=3, nu=np.float64(0.03574712922600244),\n                         sequence_length=50,\n                         weights=(np.float64(0.5), np.float64(0.25),\n                                  np.float64(0.25))), as the constructor either does not set or modifies parameter weights",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[141]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# dataset = IncrementalDataset(\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# \tPath(\"../data/PAMAP2_Dataset/Protocol/\"),\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# \tinitial_activities=3,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# \tlog_file=Path(\"../reports/logs_random_log.log\"),\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m3. Running Simulated Annealing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m annealing_results = \u001b[43meval_search_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\t\u001b[49m\u001b[43msearch_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnnealing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../reports/logs_annealing.log\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m4. Running Genetic Algorithm...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m genetic_results = eval_search_method(\n\u001b[32m     31\u001b[39m \tdataset=dataset,\n\u001b[32m     32\u001b[39m \tsearch_type=\u001b[33m\"\u001b[39m\u001b[33mGenetic\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m \tn_iter=\u001b[32m100\u001b[39m,\n\u001b[32m     34\u001b[39m \tlog_file=Path(\u001b[33m\"\u001b[39m\u001b[33m../reports/logs_genetic.log\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     35\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[140]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36meval_search_method\u001b[39m\u001b[34m(dataset, search_type, log_distributions, n_iter, log_file)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dataset.n_folds):\n\u001b[32m     30\u001b[39m \tX_train, X_val, X_test, y_train, y_val, y_test, scaler = dataset.get_fold(\n\u001b[32m     31\u001b[39m \t\tfold_idx, val_split=\u001b[32m0.2\u001b[39m, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     32\u001b[39m \t)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \tsearch = \u001b[43mtrain_search_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# All training data is \"normal\"\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mparams_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDiscrete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlog_distributions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContinuous\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43msearch_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \tfile_logger.info(\n\u001b[32m     42\u001b[39m \t\tjson.dumps(\n\u001b[32m     43\u001b[39m \t\t\t{\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \t\t)\n\u001b[32m     49\u001b[39m \t)\n\u001b[32m     50\u001b[39m \tall_fold_results.append(\n\u001b[32m     51\u001b[39m \t\t{  \u001b[38;5;66;03m# Evaluate on test set (all test samples are \"novel\")\u001b[39;00m\n\u001b[32m     52\u001b[39m \t\t\t\u001b[33m\"\u001b[39m\u001b[33mfold\u001b[39m\u001b[33m\"\u001b[39m: fold_idx,\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \t\t}\n\u001b[32m     69\u001b[39m \t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mtrain_search_method\u001b[39m\u001b[34m(training_data, train_targets, search_type, params_type, n_iter, cv, verbose, random_state)\u001b[39m\n\u001b[32m    134\u001b[39m params[\u001b[33m\"\u001b[39m\u001b[33mn_features\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlen\u001b[39m(IMU_COLUMNS)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m search_type == \u001b[33m\"\u001b[39m\u001b[33mAnnealing\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    136\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSimulatedAnnealingSearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43minitial_temp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mcooling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m search_type == \u001b[33m\"\u001b[39m\u001b[33mGenetic\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    145\u001b[39m \tpopulation_size = \u001b[38;5;28mmin\u001b[39m(\u001b[32m20\u001b[39m, n_iter // \u001b[32m5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mSimulatedAnnealingSearch.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Initialize with random parameters\u001b[39;00m\n\u001b[32m     96\u001b[39m current_params = \u001b[38;5;28mself\u001b[39m._sample_params()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m current_score = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.best_params_ = deepcopy(current_params)\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.best_score_ = current_score\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mSimulatedAnnealingSearch._evaluate_params\u001b[39m\u001b[34m(self, params, X, y)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluate parameter configuration using cross-validation.\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(params)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \t\u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mNoveltyDetectionEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscore_function\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m ).astype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:684\u001b[39m, in \u001b[36mcross_val_score\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[32m    682\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[33m\"\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:411\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1844\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1843\u001b[39m \u001b[38;5;66;03m# Sequentially call the tasks and yield the results.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_dispatched_batches\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1846\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_dispatched_tasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:75\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = \u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdelayed_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(iterable_with_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:413\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    411\u001b[39m results = parallel(\n\u001b[32m    412\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m         \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    414\u001b[39m         X,\n\u001b[32m    415\u001b[39m         y,\n\u001b[32m    416\u001b[39m         scorer=scorers,\n\u001b[32m    417\u001b[39m         train=train,\n\u001b[32m    418\u001b[39m         test=test,\n\u001b[32m    419\u001b[39m         verbose=verbose,\n\u001b[32m    420\u001b[39m         parameters=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    421\u001b[39m         fit_params=routed_params.estimator.fit,\n\u001b[32m    422\u001b[39m         score_params=routed_params.scorer.score,\n\u001b[32m    423\u001b[39m         return_train_score=return_train_score,\n\u001b[32m    424\u001b[39m         return_times=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    425\u001b[39m         return_estimator=return_estimator,\n\u001b[32m    426\u001b[39m         error_score=error_score,\n\u001b[32m    427\u001b[39m     )\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    429\u001b[39m )\n\u001b[32m    431\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\base.py:94\u001b[39m, in \u001b[36mclone\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m \u001b[33;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m \u001b[33;03mFalse\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m__sklearn_clone__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect.isclass(estimator):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__sklearn_clone__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe=safe)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\base.py:300\u001b[39m, in \u001b[36mBaseEstimator.__sklearn_clone__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\VDUART10\\Desktop\\TCC-1\\.venv\\Lib\\site-packages\\sklearn\\base.py:142\u001b[39m, in \u001b[36m_clone_parametrized\u001b[39m\u001b[34m(estimator, safe)\u001b[39m\n\u001b[32m    140\u001b[39m     param2 = params_set[name]\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m param1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param2:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    143\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot clone object \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, as the constructor \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    144\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33meither does not set or modifies parameter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (estimator, name)\n\u001b[32m    145\u001b[39m         )\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# _sklearn_output_config is used by `set_output` to configure the output\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# container of an estimator.\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m_sklearn_output_config\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: Cannot clone object NoveltyDetectionEnsemble(contamination=np.float64(0.02757359293934948),\n                         dropout=np.float64(0.18272261776066237), epochs=30,\n                         gamma=np.float64(0.003148911647956862), hidden_size=32,\n                         learning_rate=np.float64(0.0005611516415334506),\n                         max_samples=128, n_estimators=100, n_features=30,\n                         n_layers=3, nu=np.float64(0.03574712922600244),\n                         sequence_length=50,\n                         weights=(np.float64(0.5), np.float64(0.25),\n                                  np.float64(0.25))), as the constructor either does not set or modifies parameter weights"
     ]
    }
   ],
   "source": [
    "# dataset = IncrementalDataset(\n",
    "# \tPath(\"../data/PAMAP2_Dataset/Protocol/\"),\n",
    "# \tinitial_activities=3,\n",
    "# \ttest_size=2,\n",
    "# \tordering_method=\"pca\",\n",
    "# )\n",
    "# print(\"\\n1. Running Random Search (discrete)...\")\n",
    "# random_results = eval_search_method(\n",
    "# \tdataset=dataset,\n",
    "# \tsearch_type=\"Random\",\n",
    "# \tn_iter=50,\n",
    "# \tlog_file=Path(\"../reports/logs_random_discrete.log\"),\n",
    "# )\n",
    "# print(\"\\n2. Running Random Search (log distributions)...\")\n",
    "# random_log_results = eval_search_method(\n",
    "# \tdataset=dataset,\n",
    "# \tsearch_type=\"Random\",\n",
    "# log_distributions=True,\n",
    "# \tn_iter=50,\n",
    "# \tlog_file=Path(\"../reports/logs_random_log.log\"),\n",
    "# )\n",
    "print(\"\\n3. Running Simulated Annealing...\")\n",
    "annealing_results = eval_search_method(\n",
    "\tdataset=dataset,\n",
    "\tsearch_type=\"Annealing\",\n",
    "\tn_iter=100,\n",
    "\tlog_file=Path(\"../reports/logs_annealing.log\"),\n",
    ")\n",
    "print(\"\\n4. Running Genetic Algorithm...\")\n",
    "genetic_results = eval_search_method(\n",
    "\tdataset=dataset,\n",
    "\tsearch_type=\"Genetic\",\n",
    "\tn_iter=100,\n",
    "\tlog_file=Path(\"../reports/logs_genetic.log\"),\n",
    ")\n",
    "print(\"\\n5. Running Bayesian Optimization...\")\n",
    "bayesian_results = eval_search_method(\n",
    "\tdataset=dataset,\n",
    "\tsearch_type=\"Bayesian\",\n",
    "\tn_iter=50,\n",
    "\tlog_file=Path(\"../reports/logs_bayesian.log\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
