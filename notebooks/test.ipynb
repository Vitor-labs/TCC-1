{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import (\n",
    "\tauc,\n",
    "\tclassification_report,\n",
    "\tconfusion_matrix,\n",
    "\tf1_score,\n",
    "\tprecision_recall_curve,\n",
    "\tprecision_score,\n",
    "\trecall_score,\n",
    "\troc_auc_score,\n",
    ")\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Activity mapping\n",
    "activity_map = {\n",
    "\t1: \"lying\",\n",
    "\t2: \"sitting\",\n",
    "\t3: \"standing\",\n",
    "\t4: \"walking\",\n",
    "\t5: \"running\",\n",
    "\t6: \"cycling\",\n",
    "\t7: \"Nordic walking\",\n",
    "\t12: \"ascending stairs\",\n",
    "\t13: \"descending stairs\",\n",
    "\t16: \"vacuum cleaning\",\n",
    "\t17: \"ironing\",\n",
    "\t24: \"rope jumping\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset loaded successfully!\n",
      "  Total samples: 1,893,200\n",
      "  Total features: 30\n",
      "  Activities: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(12), np.int64(13), np.int64(16), np.int64(17)]\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"../data/PAMAP2/x_train_data.csv\")\n",
    "y_train = pd.read_csv(\"../data/PAMAP2/y_train_data.csv\")\n",
    "X_test = pd.read_csv(\"../data/PAMAP2/x_test_data.csv\")\n",
    "y_test = pd.read_csv(\"../data/PAMAP2/y_test_data.csv\")\n",
    "\n",
    "X_train[\"activityID\"] = y_train.values\n",
    "X_test[\"activityID\"] = y_test.values\n",
    "df_total = pd.concat([X_train, X_test], ignore_index=True)\n",
    "\n",
    "SENSOR_COLS = [\n",
    "\tcol for col in df_total.columns if col not in [\"timestamp\", \"subject\", \"activityID\"]\n",
    "]\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"  Total samples: {len(df_total):,}\")\n",
    "print(f\"  Total features: {len(SENSOR_COLS)}\")\n",
    "print(f\"  Activities: {sorted(df_total['activityID'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Window Configuration:\n",
      "  Window size: 100 samples (1.0s)\n",
      "  Overlap: 50 samples (50%)\n",
      "  Step size: 50 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VDUART10\\AppData\\Local\\Temp\\ipykernel_15128\\942815994.py:39: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  window_features[f\"{col}_skew\"] = stats.skew(values)\n",
      "C:\\Users\\VDUART10\\AppData\\Local\\Temp\\ipykernel_15128\\942815994.py:40: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  window_features[f\"{col}_kurtosis\"] = stats.kurtosis(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from windows...\n",
      "  Processing window 1...\n",
      "  Processing window 1001...\n",
      "  Processing window 2001...\n",
      "  Processing window 3001...\n",
      "  Processing window 4001...\n",
      "  Processing window 5001...\n",
      "  Processing window 6001...\n",
      "  Processing window 7001...\n",
      "  Processing window 8001...\n",
      "  Processing window 9001...\n",
      "  Processing window 10001...\n",
      "  Processing window 11001...\n",
      "  Processing window 12001...\n",
      "  Processing window 13001...\n",
      "  Processing window 14001...\n",
      "  Processing window 15001...\n",
      "  Processing window 16001...\n",
      "  Processing window 17001...\n",
      "  Processing window 18001...\n",
      "  Processing window 19001...\n",
      "  Processing window 20001...\n",
      "  Processing window 21001...\n",
      "  Processing window 22001...\n",
      "  Processing window 23001...\n",
      "  Processing window 24001...\n",
      "  Processing window 25001...\n",
      "  Processing window 26001...\n",
      "  Processing window 27001...\n",
      "  Processing window 28001...\n",
      "  Processing window 29001...\n",
      "  Processing window 30001...\n",
      "  Processing window 31001...\n",
      "  Processing window 32001...\n",
      "  Processing window 33001...\n",
      "  Processing window 34001...\n",
      "  Processing window 35001...\n",
      "  Processing window 36001...\n",
      "  Processing window 37001...\n",
      "\n",
      "Windowing complete!\n",
      "  Total windows: 37,663\n",
      "  Features per window: 270\n",
      "\n",
      "Window distribution by activity:\n",
      "   1 - lying               :  3,834 windows\n",
      "   2 - sitting             :  3,686 windows\n",
      "   3 - standing            :  3,781 windows\n",
      "   4 - walking             :  4,759 windows\n",
      "   5 - running             :  1,951 windows\n",
      "   6 - cycling             :  3,278 windows\n",
      "   7 - Nordic walking      :  3,750 windows\n",
      "  12 - ascending stairs    :  2,314 windows\n",
      "  13 - descending stairs   :  2,062 windows\n",
      "  16 - vacuum cleaning     :  3,491 windows\n",
      "  17 - ironing             :  4,757 windows\n"
     ]
    }
   ],
   "source": [
    "window_size = 100  # 1 second at 100Hz\n",
    "overlap = 50  # 50% overlap\n",
    "step = window_size - overlap\n",
    "\n",
    "print(f\"\\nWindow Configuration:\")\n",
    "print(f\"  Window size: {window_size} samples (1.0s)\")\n",
    "print(f\"  Overlap: {overlap} samples ({overlap / window_size * 100:.0f}%)\")\n",
    "print(f\"  Step size: {step} samples\")\n",
    "\n",
    "# Sort by subject and timestamp\n",
    "df_sorted = df_total.sort_values([\"subject\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "# Create windows with features\n",
    "windows_data = []\n",
    "window_labels = []\n",
    "window_subjects = []\n",
    "\n",
    "print(\"\\nExtracting features from windows...\")\n",
    "\n",
    "for start_idx in range(0, len(df_sorted) - window_size + 1, step):\n",
    "\tif start_idx % 50000 == 0:\n",
    "\t\tprint(f\"  Processing window {start_idx // step + 1}...\")\n",
    "\n",
    "\twindow = df_sorted.iloc[start_idx : start_idx + window_size]\n",
    "\n",
    "\t# Ensure single activity and subject in window\n",
    "\tif window[\"activityID\"].nunique() == 1 and window[\"subject\"].nunique() == 1:\n",
    "\t\twindow_features = {}\n",
    "\n",
    "\t\tfor col in SENSOR_COLS:\n",
    "\t\t\tvalues = window[col].values\n",
    "\n",
    "\t\t\t# Time-domain features\n",
    "\t\t\twindow_features[f\"{col}_mean\"] = np.mean(values)\n",
    "\t\t\twindow_features[f\"{col}_std\"] = np.std(values)\n",
    "\t\t\twindow_features[f\"{col}_min\"] = np.min(values)\n",
    "\t\t\twindow_features[f\"{col}_max\"] = np.max(values)\n",
    "\t\t\twindow_features[f\"{col}_range\"] = np.max(values) - np.min(values)\n",
    "\t\t\twindow_features[f\"{col}_skew\"] = stats.skew(values)\n",
    "\t\t\twindow_features[f\"{col}_kurtosis\"] = stats.kurtosis(values)\n",
    "\t\t\twindow_features[f\"{col}_energy\"] = np.sum(values**2)\n",
    "\t\t\twindow_features[f\"{col}_rms\"] = np.sqrt(np.mean(values**2))\n",
    "\n",
    "\t\twindows_data.append(window_features)\n",
    "\t\twindow_labels.append(window[\"activityID\"].iloc[0])\n",
    "\t\twindow_subjects.append(window[\"subject\"].iloc[0])\n",
    "\n",
    "windows_df = pd.DataFrame(windows_data)\n",
    "windows_df[\"activityID\"] = window_labels\n",
    "windows_df[\"subject\"] = window_subjects\n",
    "\n",
    "print(f\"\\nWindowing complete!\")\n",
    "print(f\"  Total windows: {len(windows_df):,}\")\n",
    "print(f\"  Features per window: {len(windows_df.columns) - 2}\")\n",
    "print(f\"\\nWindow distribution by activity:\")\n",
    "for activity_id in sorted(windows_df[\"activityID\"].unique()):\n",
    "\tcount = (windows_df[\"activityID\"] == activity_id).sum()\n",
    "\tprint(\n",
    "\t\tf\"  {activity_id:2d} - {activity_map.get(activity_id, 'Unknown'):20s}: {count:6,} windows\"\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNOWN Activities (Normal class):\n",
      "   1 - lying               :  3,834 windows\n",
      "   2 - sitting             :  3,686 windows\n",
      "   3 - standing            :  3,781 windows\n",
      "   4 - walking             :  4,759 windows\n",
      "   6 - cycling             :  3,278 windows\n",
      "   7 - Nordic walking      :  3,750 windows\n",
      "  17 - ironing             :  4,757 windows\n",
      "\n",
      "UNSEEN Activities (Novelty class):\n",
      "   5 - running             :  1,951 windows\n",
      "  12 - ascending stairs    :  2,314 windows\n",
      "  13 - descending stairs   :  2,062 windows\n",
      "  16 - vacuum cleaning     :  3,491 windows\n",
      "  24 - rope jumping        :      0 windows\n",
      "\n",
      "Train subjects: [1, 2, 3, 4, 5, 6]\n",
      "Test subjects: [7, 8]\n"
     ]
    }
   ],
   "source": [
    "# Known activities (train on these)\n",
    "known_activities = [1, 2, 3, 4, 6, 7, 17]\n",
    "# Unseen activities (test novelty detection on these)\n",
    "unseen_activities = [5, 12, 13, 16, 24]\n",
    "\n",
    "print(\"\\nKNOWN Activities (Normal class):\")\n",
    "for activity_id in known_activities:\n",
    "\tcount = (windows_df[\"activityID\"] == activity_id).sum()\n",
    "\tprint(f\"  {activity_id:2d} - {activity_map[activity_id]:20s}: {count:6,} windows\")\n",
    "\n",
    "print(\"\\nUNSEEN Activities (Novelty class):\")\n",
    "for activity_id in unseen_activities:\n",
    "\tcount = (windows_df[\"activityID\"] == activity_id).sum()\n",
    "\tprint(f\"  {activity_id:2d} - {activity_map[activity_id]:20s}: {count:6,} windows\")\n",
    "\n",
    "# Use subjects 1-6 for training, 7-8 for testing\n",
    "train_subjects = [1, 2, 3, 4, 5, 6]\n",
    "test_subjects = [7, 8]\n",
    "\n",
    "print(f\"\\nTrain subjects: {train_subjects}\")\n",
    "print(f\"Test subjects: {test_subjects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sizes:\n",
      "  Training (known activities): 20,484 windows\n",
      "  Test (known activities): 7,361 windows\n",
      "  Test (unseen activities): 2,308 windows\n",
      "  Total test set: 9,669 windows\n",
      "    - Normal (0): 7,361\n",
      "    - Novelty (1): 2,308\n"
     ]
    }
   ],
   "source": [
    "# Training set: known activities from train subjects\n",
    "train_mask = windows_df[\"activityID\"].isin(known_activities) & windows_df[\n",
    "\t\"subject\"\n",
    "].isin(train_subjects)\n",
    "X_train_known = windows_df[train_mask].drop([\"activityID\", \"subject\"], axis=1)\n",
    "y_train_known = windows_df[train_mask][\"activityID\"]\n",
    "\n",
    "# Test set: known activities from test subjects (should classify as normal)\n",
    "test_known_mask = windows_df[\"activityID\"].isin(known_activities) & windows_df[\n",
    "\t\"subject\"\n",
    "].isin(test_subjects)\n",
    "X_test_known = windows_df[test_known_mask].drop([\"activityID\", \"subject\"], axis=1)\n",
    "y_test_known = windows_df[test_known_mask][\"activityID\"]\n",
    "\n",
    "# Test set: unseen activities from test subjects (should classify as novelty)\n",
    "test_unseen_mask = windows_df[\"activityID\"].isin(unseen_activities) & windows_df[\n",
    "\t\"subject\"\n",
    "].isin(test_subjects)\n",
    "X_test_unseen = windows_df[test_unseen_mask].drop([\"activityID\", \"subject\"], axis=1)\n",
    "y_test_unseen = windows_df[test_unseen_mask][\"activityID\"]\n",
    "\n",
    "# Combine test sets\n",
    "X_test_combined = pd.concat([X_test_known, X_test_unseen], ignore_index=True)\n",
    "y_test_combined = pd.concat([y_test_known, y_test_unseen], ignore_index=True)\n",
    "y_test_binary = np.array(\n",
    "\t[0 if act in known_activities else 1 for act in y_test_combined]\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training (known activities): {len(X_train_known):,} windows\")\n",
    "print(f\"  Test (known activities): {len(X_test_known):,} windows\")\n",
    "print(f\"  Test (unseen activities): {len(X_test_unseen):,} windows\")\n",
    "print(f\"  Total test set: {len(X_test_combined):,} windows\")\n",
    "print(f\"    - Normal (0): {(y_test_binary == 0).sum():,}\")\n",
    "print(f\"    - Novelty (1): {(y_test_binary == 1).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] Training One-Class SVM...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/3] Training One-Class SVM...\")\n",
    "ocsvm = OneClassSVM(\n",
    "\tkernel=\"rbf\",\n",
    "\tgamma=\"auto\",\n",
    "\tnu=0.1,  # Expected fraction of outliers\n",
    "\tverbose=False,\n",
    ")\n",
    "ocsvm.fit(X_train)\n",
    "print(\"  ✓ One-Class SVM trained successfully\")\n",
    "\n",
    "# Model 2: Isolation Forest\n",
    "print(\"\\n[2/3] Training Isolation Forest...\")\n",
    "iforest = IsolationForest(\n",
    "\tn_estimators=100,\n",
    "\tcontamination=0.1,  # Expected proportion of outliers\n",
    "\tmax_samples=\"auto\",\n",
    "\trandom_state=42,\n",
    "\tverbose=0,\n",
    ")\n",
    "iforest.fit(X_train)\n",
    "print(\"  ✓ Isolation Forest trained successfully\")\n",
    "\n",
    "# Model 3: Local Outlier Factor\n",
    "print(\"\\n[3/3] Training Local Outlier Factor...\")\n",
    "lof = LocalOutlierFactor(\n",
    "\tn_neighbors=20,\n",
    "\tcontamination=0.1,\n",
    "\tnovelty=True,  # Important: enables predict for new data\n",
    ")\n",
    "lof.fit(X_train)\n",
    "print(\"  ✓ Local Outlier Factor trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get decision scores (higher = more normal, lower = more anomalous)\n",
    "> Note: sklearn outputs are inconsistent, we need to normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Class SVM decision scores\n",
    "ocsvm_scores = ocsvm.decision_function(X_test)\n",
    "ocsvm_pred = ocsvm.predict(X_test)\n",
    "ocsvm_pred_binary = np.where(ocsvm_pred == 1, 0, 1)  # 1=normal->0, -1=anomaly->1\n",
    "\n",
    "# Isolation Forest scores\n",
    "iforest_scores = iforest.decision_function(X_test)\n",
    "iforest_pred = iforest.predict(X_test)\n",
    "iforest_pred_binary = np.where(iforest_pred == 1, 0, 1)  # 1=normal->0, -1=anomaly->1\n",
    "\n",
    "# LOF scores\n",
    "lof_scores = lof.decision_function(X_test)\n",
    "lof_pred = lof.predict(X_test)\n",
    "lof_pred_binary = np.where(lof_pred == 1, 0, 1)  # 1=normal->0, -1=anomaly->1\n",
    "\n",
    "print(\"\\nDecision scores computed:\")\n",
    "print(f\"  One-Class SVM: range [{ocsvm_scores.min():.3f}, {ocsvm_scores.max():.3f}]\")\n",
    "print(\n",
    "\tf\"  Isolation Forest: range [{iforest_scores.min():.3f}, {iforest_scores.max():.3f}]\"\n",
    ")\n",
    "print(f\"  LOF: range [{lof_scores.min():.3f}, {lof_scores.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Majority Voting\n",
    "ensemble_voting = (ocsvm_pred_binary + iforest_pred_binary + lof_pred_binary) >= 2\n",
    "ensemble_voting = ensemble_voting.astype(int)\n",
    "\n",
    "# Strategy 2: Average of normalized scores\n",
    "# Normalize scores to [0, 1] where higher = more anomalous\n",
    "ocsvm_scores_norm = (ocsvm_scores - ocsvm_scores.min()) / (\n",
    "\tocsvm_scores.max() - ocsvm_scores.min()\n",
    ")\n",
    "ocsvm_scores_norm = 1 - ocsvm_scores_norm  # Invert: higher = more anomalous\n",
    "\n",
    "iforest_scores_norm = (iforest_scores - iforest_scores.min()) / (\n",
    "\tiforest_scores.max() - iforest_scores.min()\n",
    ")\n",
    "iforest_scores_norm = 1 - iforest_scores_norm  # Invert\n",
    "\n",
    "lof_scores_norm = (lof_scores - lof_scores.min()) / (\n",
    "\tlof_scores.max() - lof_scores.min()\n",
    ")\n",
    "lof_scores_norm = 1 - lof_scores_norm  # Invert\n",
    "\n",
    "ensemble_avg_scores = (ocsvm_scores_norm + iforest_scores_norm + lof_scores_norm) / 3\n",
    "\n",
    "# Strategy 3: Weighted Average (can tune weights)\n",
    "weights = {\"ocsvm\": 0.3, \"iforest\": 0.4, \"lof\": 0.3}\n",
    "ensemble_weighted_scores = (\n",
    "\tweights[\"ocsvm\"] * ocsvm_scores_norm\n",
    "\t+ weights[\"iforest\"] * iforest_scores_norm\n",
    "\t+ weights[\"lof\"] * lof_scores_norm\n",
    ")\n",
    "print(\"\\nEnsemble strategies created:\")\n",
    "print(\"  [1] Majority Voting\")\n",
    "print(\"  [2] Average Score\")\n",
    "print(\"  [3] Weighted Average Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, scores, model_name):\n",
    "\tprint(f\"\\n{model_name}\")\n",
    "\tprint(\"-\" * 60)\n",
    "\n",
    "\t# Classification metrics\n",
    "\tprecision = precision_score(y_true, y_pred)\n",
    "\trecall = recall_score(y_true, y_pred)\n",
    "\tf1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\tprint(f\"  Precision: {precision:.4f}\")\n",
    "\tprint(f\"  Recall: {recall:.4f}\")\n",
    "\tprint(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "\t# ROC-AUC\n",
    "\troc_auc = roc_auc_score(y_true, scores)\n",
    "\tprint(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "\t# Precision-Recall AUC\n",
    "\tprec, rec, _ = precision_recall_curve(y_true, scores)\n",
    "\tpr_auc = auc(rec, prec)\n",
    "\tprint(f\"  PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "\t# Confusion matrix\n",
    "\tcm = confusion_matrix(y_true, y_pred)\n",
    "\tprint(f\"\\n  Confusion Matrix:\")\n",
    "\tprint(f\"    TN: {cm[0, 0]:6,}  |  FP: {cm[0, 1]:6,}\")\n",
    "\tprint(f\"    FN: {cm[1, 0]:6,}  |  TP: {cm[1, 1]:6,}\")\n",
    "\n",
    "\treturn {\n",
    "\t\t\"precision\": precision,\n",
    "\t\t\"recall\": recall,\n",
    "\t\t\"f1\": f1,\n",
    "\t\t\"roc_auc\": roc_auc,\n",
    "\t\t\"pr_auc\": pr_auc,\n",
    "\t\t\"cm\": cm,\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "\t\"ocsvm\": evaluate_model(\n",
    "\t\ty_test_binary, ocsvm_pred_binary, ocsvm_scores_norm, \"ONE-CLASS SVM\"\n",
    "\t),\n",
    "\t\"iforest\": evaluate_model(\n",
    "\t\ty_test_binary, iforest_pred_binary, iforest_scores_norm, \"ISOLATION FOREST\"\n",
    "\t),\n",
    "\t\"lof\": evaluate_model(\n",
    "\t\ty_test_binary, lof_pred_binary, lof_scores_norm, \"LOCAL OUTLIER FACTOR\"\n",
    "\t),\n",
    "\t\"voting\": evaluate_model(\n",
    "\t\ty_test_binary, ensemble_voting, ensemble_avg_scores, \"ENSEMBLE: MAJORITY VOTING\"\n",
    "\t),\n",
    "\t\"avg\": evaluate_model(  # For average scores, need to threshold\n",
    "\t\ty_test_binary,\n",
    "\t\t(ensemble_avg_scores > 0.5).astype(int),\n",
    "\t\tensemble_avg_scores,\n",
    "\t\t\"ENSEMBLE: AVERAGE SCORE\",\n",
    "\t),\n",
    "\t\"weighted\": evaluate_model(  # For weighted scores\n",
    "\t\ty_test_binary,\n",
    "\t\t(ensemble_weighted_scores > 0.5).astype(int),\n",
    "\t\tensemble_weighted_scores,\n",
    "\t\t\"ENSEMBLE: WEIGHTED AVERAGE\",\n",
    "\t),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "models = [\"One-Class SVM\", \"Isolation Forest\", \"LOF\", \"Voting\", \"Avg Score\", \"Weighted\"]\n",
    "model_keys = [\"ocsvm\", \"iforest\", \"lof\", \"voting\", \"avg\", \"weighted\"]\n",
    "colors_models = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\", \"#9b59b6\", \"#1abc9c\"]\n",
    "\n",
    "# Precision\n",
    "ax = axes[0, 0]\n",
    "precisions = [results[key][\"precision\"] for key in model_keys]\n",
    "bars = ax.bar(\n",
    "\trange(len(models)), precisions, color=colors_models, edgecolor=\"black\", alpha=0.8\n",
    ")\n",
    "ax.set_xticks(range(len(models)))\n",
    "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Precision\", fontsize=12)\n",
    "ax.set_title(\"Precision Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "for i, bar in enumerate(bars):\n",
    "\theight = bar.get_height()\n",
    "\tax.text(\n",
    "\t\tbar.get_x() + bar.get_width() / 2.0,\n",
    "\t\theight,\n",
    "\t\tf\"{precisions[i]:.3f}\",\n",
    "\t\tha=\"center\",\n",
    "\t\tva=\"bottom\",\n",
    "\t\tfontsize=10,\n",
    "\t)\n",
    "\n",
    "# Recall\n",
    "ax = axes[0, 1]\n",
    "recalls = [results[key][\"recall\"] for key in model_keys]\n",
    "bars = ax.bar(\n",
    "\trange(len(models)), recalls, color=colors_models, edgecolor=\"black\", alpha=0.8\n",
    ")\n",
    "ax.set_xticks(range(len(models)))\n",
    "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Recall\", fontsize=12)\n",
    "ax.set_title(\"Recall Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "for i, bar in enumerate(bars):\n",
    "\theight = bar.get_height()\n",
    "\tax.text(\n",
    "\t\tbar.get_x() + bar.get_width() / 2.0,\n",
    "\t\theight,\n",
    "\t\tf\"{recalls[i]:.3f}\",\n",
    "\t\tha=\"center\",\n",
    "\t\tva=\"bottom\",\n",
    "\t\tfontsize=10,\n",
    "\t)\n",
    "\n",
    "# F1-Score\n",
    "ax = axes[1, 0]\n",
    "f1_scores = [results[key][\"f1\"] for key in model_keys]\n",
    "bars = ax.bar(\n",
    "\trange(len(models)), f1_scores, color=colors_models, edgecolor=\"black\", alpha=0.8\n",
    ")\n",
    "ax.set_xticks(range(len(models)))\n",
    "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"F1-Score\", fontsize=12)\n",
    "ax.set_title(\"F1-Score Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "for i, bar in enumerate(bars):\n",
    "\theight = bar.get_height()\n",
    "\tax.text(\n",
    "\t\tbar.get_x() + bar.get_width() / 2.0,\n",
    "\t\theight,\n",
    "\t\tf\"{f1_scores[i]:.3f}\",\n",
    "\t\tha=\"center\",\n",
    "\t\tva=\"bottom\",\n",
    "\t\tfontsize=10,\n",
    "\t)\n",
    "\n",
    "# ROC-AUC\n",
    "ax = axes[1, 1]\n",
    "roc_aucs = [results[key][\"roc_auc\"] for key in model_keys]\n",
    "bars = ax.bar(\n",
    "\trange(len(models)), roc_aucs, color=colors_models, edgecolor=\"black\", alpha=0.8\n",
    ")\n",
    "ax.set_xticks(range(len(models)))\n",
    "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"ROC-AUC\", fontsize=12)\n",
    "ax.set_title(\"ROC-AUC Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "for i, bar in enumerate(bars):\n",
    "\theight = bar.get_height()\n",
    "\tax.text(\n",
    "\t\tbar.get_x() + bar.get_width() / 2.0,\n",
    "\t\theight,\n",
    "\t\tf\"{roc_aucs[i]:.3f}\",\n",
    "\t\tha=\"center\",\n",
    "\t\tva=\"bottom\",\n",
    "\t\tfontsize=10,\n",
    "\t)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 11: ROC & PR CURVES\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curves\n",
    "ax = axes[0]\n",
    "all_scores = {\n",
    "\t\"One-Class SVM\": ocsvm_scores_norm,\n",
    "\t\"Isolation Forest\": iforest_scores_norm,\n",
    "\t\"LOF\": lof_scores_norm,\n",
    "\t\"Ensemble (Avg)\": ensemble_avg_scores,\n",
    "\t\"Ensemble (Weighted)\": ensemble_weighted_scores,\n",
    "}\n",
    "\n",
    "for idx, (name, scores) in enumerate(all_scores.items()):\n",
    "\tfpr, tpr, _ = roc_curve(y_test_binary, scores)\n",
    "\troc_auc = auc(fpr, tpr)\n",
    "\tax.plot(fpr, tpr, linewidth=2, label=f\"{name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", linewidth=1.5, label=\"Random Classifier\")\n",
    "ax.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
    "ax.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
    "ax.set_title(\"ROC Curves\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\", fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "ax = axes[1]\n",
    "for idx, (name, scores) in enumerate(all_scores.items()):\n",
    "\tprec, rec, _ = precision_recall_curve(y_test_binary, scores)\n",
    "\tpr_auc = auc(rec, prec)\n",
    "\tax.plot(rec, prec, linewidth=2, label=f\"{name} (AUC={pr_auc:.3f})\")\n",
    "\n",
    "baseline = y_test_binary.sum() / len(y_test_binary)\n",
    "ax.axhline(\n",
    "\ty=baseline,\n",
    "\tcolor=\"k\",\n",
    "\tlinestyle=\"--\",\n",
    "\tlinewidth=1.5,\n",
    "\tlabel=f\"Baseline ({baseline:.3f})\",\n",
    ")\n",
    "ax.set_xlabel(\"Recall\", fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=12)\n",
    "ax.set_title(\"Precision-Recall Curves\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower left\", fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cms = [\n",
    "\t(results[\"ocsvm\"][\"cm\"], \"One-Class SVM\"),\n",
    "\t(results[\"iforest\"][\"cm\"], \"Isolation Forest\"),\n",
    "\t(results[\"lof\"][\"cm\"], \"Local Outlier Factor\"),\n",
    "\t(results[\"voting\"][\"cm\"], \"Ensemble: Voting\"),\n",
    "\t(results[\"avg\"][\"cm\"], \"Ensemble: Average\"),\n",
    "\t(results[\"weighted\"][\"cm\"], \"Ensemble: Weighted\"),\n",
    "]\n",
    "\n",
    "for idx, (cm, title) in enumerate(cms):\n",
    "\tax = axes[idx]\n",
    "\n",
    "\tsns.heatmap(\n",
    "\t\tcm,\n",
    "\t\tannot=True,\n",
    "\t\tfmt=\"d\",\n",
    "\t\tcmap=\"Blues\",\n",
    "\t\tax=ax,\n",
    "\t\tcbar_kws={\"label\": \"Count\"},\n",
    "\t\txticklabels=[\"Normal\", \"Novelty\"],\n",
    "\t\tyticklabels=[\"Normal\", \"Novelty\"],\n",
    "\t)\n",
    "\tax.set_xlabel(\"Predicted\", fontsize=11)\n",
    "\tax.set_ylabel(\"Actual\", fontsize=11)\n",
    "\tax.set_title(title, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_test_activities = sorted(y_test_combined.unique())\n",
    "\n",
    "activity_scores = {}\n",
    "for activity_id in unique_test_activities:\n",
    "\tmask = y_test_combined == activity_id\n",
    "\tactivity_scores[activity_id] = ensemble_weighted_scores[mask]\n",
    "\n",
    "# Visualize score distributions per activity\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "positions = []\n",
    "labels = []\n",
    "data = []\n",
    "\n",
    "for idx, activity_id in enumerate(unique_test_activities):\n",
    "\tscores = activity_scores[activity_id]\n",
    "\tdata.append(scores)\n",
    "\tpositions.append(idx)\n",
    "\n",
    "\tactivity_name = activity_map.get(activity_id, f\"ID{activity_id}\")\n",
    "\tlabel = f\"{activity_id}: {activity_name}\"\n",
    "\tif activity_id in known_activities:\n",
    "\t\tlabel += \" (KNOWN)\"\n",
    "\telse:\n",
    "\t\tlabel += \" (UNSEEN)\"\n",
    "\tlabels.append(label)\n",
    "\n",
    "bp = ax.boxplot(\n",
    "\tdata,\n",
    "\tpositions=positions,\n",
    "\tlabels=labels,\n",
    "\tpatch_artist=True,\n",
    "\tshowfliers=True,\n",
    "\twidths=0.6,\n",
    ")\n",
    "\n",
    "# Color boxes: blue for known, red for unseen\n",
    "for idx, (patch, activity_id) in enumerate(zip(bp[\"boxes\"], unique_test_activities)):\n",
    "\tif activity_id in known_activities:\n",
    "\t\tpatch.set_facecolor(\"#3498db\")\n",
    "\t\tpatch.set_alpha(0.7)\n",
    "\telse:\n",
    "\t\tpatch.set_facecolor(\"#e74c3c\")\n",
    "\t\tpatch.set_alpha(0.7)\n",
    "\n",
    "ax.axhline(\n",
    "\ty=0.5,\n",
    "\tcolor=\"black\",\n",
    "\tlinestyle=\"--\",\n",
    "\tlinewidth=2,\n",
    "\tlabel=\"Decision Threshold (0.5)\",\n",
    "\talpha=0.7,\n",
    ")\n",
    "ax.set_xlabel(\"Activity\", fontsize=12)\n",
    "ax.set_ylabel(\"Novelty Score (Ensemble)\", fontsize=12)\n",
    "ax.set_title(\"Novelty Score Distribution by Activity\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(loc=\"upper right\", fontsize=11)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPer-Activity Novelty Scores (Ensemble Weighted):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Activity':<30} {'Type':<10} {'Mean':>8} {'Std':>8} {'Min':>8} {'Max':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for activity_id in unique_test_activities:\n",
    "\tscores = activity_scores[activity_id]\n",
    "\tactivity_name = activity_map.get(activity_id, f\"ID{activity_id}\")\n",
    "\tactivity_type = \"KNOWN\" if activity_id in known_activities else \"UNSEEN\"\n",
    "\n",
    "\tprint(\n",
    "\t\tf\"{activity_id:2d} - {activity_name:<25} {activity_type:<10} \"\n",
    "\t\tf\"{np.mean(scores):8.4f} {np.std(scores):8.4f} \"\n",
    "\t\tf\"{np.min(scores):8.4f} {np.max(scores):8.4f}\"\n",
    "\t)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "# Distribution for known vs unseen\n",
    "ax = axes[0]\n",
    "\n",
    "known_mask = y_test_binary == 0\n",
    "unseen_mask = y_test_binary == 1\n",
    "\n",
    "ax.hist(\n",
    "\tensemble_weighted_scores[known_mask],\n",
    "\tbins=50,\n",
    "\talpha=0.6,\n",
    "\tlabel=\"Known Activities\",\n",
    "\tcolor=\"#3498db\",\n",
    "\tedgecolor=\"black\",\n",
    ")\n",
    "ax.hist(\n",
    "\tensemble_weighted_scores[unseen_mask],\n",
    "\tbins=50,\n",
    "\talpha=0.6,\n",
    "\tlabel=\"Unseen Activities\",\n",
    "\tcolor=\"#e74c3c\",\n",
    "\tedgecolor=\"black\",\n",
    ")\n",
    "ax.axvline(x=0.5, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Threshold (0.5)\")\n",
    "ax.set_xlabel(\"Novelty Score (Ensemble)\", fontsize=12)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "ax.set_title(\"Score Distribution: Known vs Unseen\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Individual model comparison\n",
    "ax = axes[1]\n",
    "ax.hist(\n",
    "\tocsvm_scores_norm,\n",
    "\tbins=50,\n",
    "\talpha=0.4,\n",
    "\tlabel=\"One-Class SVM\",\n",
    "\tcolor=\"#3498db\",\n",
    "\tedgecolor=\"black\",\n",
    ")\n",
    "ax.hist(\n",
    "\tiforest_scores_norm,\n",
    "\tbins=50,\n",
    "\talpha=0.4,\n",
    "\tlabel=\"Isolation Forest\",\n",
    "\tcolor=\"#e74c3c\",\n",
    "\tedgecolor=\"black\",\n",
    ")\n",
    "ax.hist(\n",
    "\tlof_scores_norm, bins=50, alpha=0.4, label=\"LOF\", color=\"#2ecc71\", edgecolor=\"black\"\n",
    ")\n",
    "ax.axvline(x=0.5, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Threshold\")\n",
    "ax.set_xlabel(\"Normalized Novelty Score\", fontsize=12)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "ax.set_title(\"Score Distribution by Model\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n1. BEST PERFORMING MODEL:\")\n",
    "best_model_key = max(model_keys, key=lambda k: results[k][\"f1\"])\n",
    "best_model_names = {\n",
    "\t\"ocsvm\": \"One-Class SVM\",\n",
    "\t\"iforest\": \"Isolation Forest\",\n",
    "\t\"lof\": \"Local Outlier Factor\",\n",
    "\t\"voting\": \"Ensemble: Majority Voting\",\n",
    "\t\"avg\": \"Ensemble: Average Score\",\n",
    "\t\"weighted\": \"Ensemble: Weighted Average\",\n",
    "}\n",
    "best_model = best_model_names[best_model_key]\n",
    "print(f\"   {best_model}\")\n",
    "print(f\"   F1-Score: {results[best_model_key]['f1']:.4f}\")\n",
    "print(f\"   ROC-AUC: {results[best_model_key]['roc_auc']:.4f}\")\n",
    "print(f\"   Precision: {results[best_model_key]['precision']:.4f}\")\n",
    "print(f\"   Recall: {results[best_model_key]['recall']:.4f}\")\n",
    "\n",
    "print(\"\\n2. MODEL COMPARISON:\")\n",
    "print(f\"   {'Model':<30} {'F1':>8} {'ROC-AUC':>8} {'Precision':>10} {'Recall':>8}\")\n",
    "print(f\"   {'-' * 70}\")\n",
    "for key, name in zip(model_keys, models):\n",
    "\tprint(\n",
    "\t\tf\"   {name:<30} {results[key]['f1']:8.4f} {results[key]['roc_auc']:8.4f} \"\n",
    "\t\tf\"{results[key]['precision']:10.4f} {results[key]['recall']:8.4f}\"\n",
    "\t)\n",
    "\n",
    "print(\"\\n3. ENSEMBLE BENEFITS:\")\n",
    "ensemble_improvement = results[\"weighted\"][\"f1\"] - np.mean(\n",
    "\t[results[\"ocsvm\"][\"f1\"], results[\"iforest\"][\"f1\"], results[\"lof\"][\"f1\"]]\n",
    ")\n",
    "print(f\"   F1-Score improvement over individual models: {ensemble_improvement:+.4f}\")\n",
    "\n",
    "print(\"\\n4. ACTIVITIES SUCCESSFULLY DETECTED AS NOVEL:\")\n",
    "correctly_detected = []\n",
    "for activity_id in unseen_activities:\n",
    "\tscores = activity_scores.get(activity_id)\n",
    "\tif scores is not None:\n",
    "\t\tmean_score = np.mean(scores)\n",
    "\t\tif mean_score > 0.5:\n",
    "\t\t\tcorrectly_detected.append(activity_id)\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf\"   ✓ {activity_id} - {activity_map[activity_id]}: avg score = {mean_score:.4f}\"\n",
    "\t\t\t)\n",
    "\n",
    "print(\"\\n5. CHALLENGING ACTIVITIES (potential misclassification):\")\n",
    "for activity_id in unseen_activities:\n",
    "\tscores = activity_scores.get(activity_id)\n",
    "\tif scores is not None:\n",
    "\t\tmean_score = np.mean(scores)\n",
    "\t\tif mean_score <= 0.5:\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf\"   ✗ {activity_id} - {activity_map[activity_id]}: avg score = {mean_score:.4f}\"\n",
    "\t\t\t)\n",
    "\t\t\tprint(f\"      → May be similar to known activities, consider refinement\")\n",
    "\n",
    "print(\"\\n6. RECOMMENDATIONS:\")\n",
    "print(\"   • Current ensemble uses equal/near-equal weights\")\n",
    "print(\"   • Consider grid search to optimize individual model hyperparameters\")\n",
    "print(\"   • Try different weight combinations for ensemble\")\n",
    "print(\"   • Experiment with different window sizes (0.5s, 2s, 3s)\")\n",
    "print(\"   • Add frequency-domain features for better separation\")\n",
    "print(\"   • Consider deep learning approaches (Autoencoders, VAE) for complex patterns\")\n",
    "print(\"   • Implement online/streaming novelty detection for real-time applications\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENSEMBLE NOVELTY DETECTION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
