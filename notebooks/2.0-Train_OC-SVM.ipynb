{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from collections.abc import Callable\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from json import dumps\n",
    "from time import time\n",
    "from typing import Final, Literal\n",
    "\n",
    "import numpy as np\n",
    "import structlog\n",
    "from numpy import ndarray, where\n",
    "from pandas import DataFrame, Series, concat, read_csv, set_option\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.metrics import (\n",
    "\tauc,\n",
    "\taverage_precision_score,\n",
    "\tf1_score,\n",
    "\tprecision_recall_curve,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "set_option(\"display.max_columns\", None)\n",
    "structlog.configure(\n",
    "\tprocessors=[\n",
    "\t\tstructlog.stdlib.filter_by_level,\n",
    "\t\tstructlog.stdlib.add_logger_name,\n",
    "\t\tstructlog.stdlib.add_log_level,\n",
    "\t\tstructlog.stdlib.PositionalArgumentsFormatter(),\n",
    "\t\tstructlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "\t\tstructlog.processors.StackInfoRenderer(),\n",
    "\t\tstructlog.processors.format_exc_info,\n",
    "\t\tstructlog.processors.UnicodeDecoder(),\n",
    "\t\tstructlog.processors.JSONRenderer(),\n",
    "\t],\n",
    "\tcontext_class=dict,\n",
    "\tlogger_factory=structlog.stdlib.LoggerFactory(),\n",
    "\twrapper_class=structlog.stdlib.BoundLogger,\n",
    "\tcache_logger_on_first_use=True,\n",
    ")\n",
    "type ParamGrid = dict[str, tuple[float | str, ...]]\n",
    "NUM_TRIALS: Final[int] = 20\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "\tmethod: str\n",
    "\tbest_params: dict\n",
    "\tbest_score: float\n",
    "\tcv_scores: list[float]\n",
    "\tfit_time: float\n",
    "\tn_evaluations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = read_csv(\"../data/PAMAP2/x_train_data.csv\")\n",
    "X_test = read_csv(\"../data/PAMAP2/x_test_data.csv\")\n",
    "y_train = read_csv(\"../data/PAMAP2/y_train_data.csv\")\n",
    "y_test = read_csv(\"../data/PAMAP2/y_test_data.csv\")\n",
    "\n",
    "X_train[\"activity\"] = y_train\n",
    "X_test[\"activity\"] = y_test\n",
    "\n",
    "testing_data: DataFrame\n",
    "test_targets: Series\n",
    "\n",
    "# RESULTS: dict[int, dict[str, float | int]] = {}\n",
    "# MODELS: dict[int, dict] = {}\n",
    "MIN_SAMPLES = X_train[\"activity\"].value_counts().sort_values().iloc[0]\n",
    "MAXIMAZED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_file_logger(filepath: str) -> structlog.BoundLogger:\n",
    "\t\"\"\"Configure a file-specific logger using structlog.\"\"\"\n",
    "\n",
    "\t# Create a specific handler for this file\n",
    "\tfile_handler = logging.FileHandler(filepath, mode=\"a\")\n",
    "\tfile_handler.setLevel(logging.INFO)\n",
    "\n",
    "\t# Configure structlog with file output\n",
    "\tstructlog.configure(\n",
    "\t\tprocessors=[\n",
    "\t\t\tstructlog.stdlib.filter_by_level,\n",
    "\t\t\tstructlog.stdlib.add_logger_name,\n",
    "\t\t\tstructlog.stdlib.add_log_level,\n",
    "\t\t\tstructlog.stdlib.PositionalArgumentsFormatter(),\n",
    "\t\t\tstructlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "\t\t\tstructlog.processors.StackInfoRenderer(),\n",
    "\t\t\tstructlog.processors.format_exc_info,\n",
    "\t\t\tstructlog.processors.UnicodeDecoder(),\n",
    "\t\t\tstructlog.processors.JSONRenderer(),\n",
    "\t\t],\n",
    "\t\tcontext_class=dict,\n",
    "\t\tlogger_factory=structlog.stdlib.LoggerFactory(),\n",
    "\t\twrapper_class=structlog.stdlib.BoundLogger,\n",
    "\t\tcache_logger_on_first_use=False,\n",
    "\t)\n",
    "\t# Get the underlying stdlib logger and add the handler\n",
    "\tstdlib_logger = logging.getLogger(\"hyperparameter_search\")\n",
    "\tstdlib_logger.handlers.clear()  # Clear existing handlers\n",
    "\tstdlib_logger.addHandler(file_handler)\n",
    "\tstdlib_logger.setLevel(logging.INFO)\n",
    "\n",
    "\treturn structlog.get_logger(\"hyperparameter_search\")\n",
    "\n",
    "\n",
    "def score_function(model: OneClassSVM, Train: DataFrame, test: Series) -> float:\n",
    "\t\"\"\"\n",
    "\tObjective function to maximize, calcs the F1 score on the test set.\n",
    "\tfollows the format needed by scikit-learn's API.\n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel (OneClassSVM): Model to eval\n",
    "\t\tX_test (DataFrame): train data, only for API compliance\n",
    "\t\ty_true (Series): true targets, only for API compliance\n",
    "\n",
    "\tReturns:\n",
    "\t\tfloat: F1 score\n",
    "\t\"\"\"\n",
    "\tf1 = f1_score(test_targets, where(model.predict(testing_data) == -1, True, False))\n",
    "\n",
    "\t# Get decision scores (higher values = more normal, lower = more anomalous)\n",
    "\t# convert to anomaly scores (higher values = more anomalous) and Negate\n",
    "\t# decision scores since OneClassSVM gives higher scores for inliers\n",
    "\tanomaly_scores = -model.decision_function(testing_data)\n",
    "\tprecision, recall, _ = precision_recall_curve(test_targets, anomaly_scores)\n",
    "\tlogger.info(\n",
    "\t\t{\n",
    "\t\t\t\"target\": f1,\n",
    "\t\t\t\"avg_precision\": average_precision_score(test_targets, anomaly_scores),\n",
    "\t\t\t\"auc_pr\": auc(recall, precision),\n",
    "\t\t\t\"params\": model.get_params(),\n",
    "\t\t\t\"datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "\t\t}\n",
    "\t)\n",
    "\treturn float(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedAnnealingSearch:\n",
    "\t\"\"\"\n",
    "\tCustom Simulated Annealing implementation for hyperparameter optimization.\n",
    "\n",
    "\tSimulated Annealing Search:\n",
    "\t- Temperature-based acceptance: Accepts worse solutions with decreasing probability\n",
    "\t- Adaptive parameter perturbation: Different strategies for continuous (nu, gamma, tol) vs discrete parameters\n",
    "\t- Cooling schedule: Exponential cooling with configurable rate\n",
    "\t- Neighbor generation: Smart parameter space exploration\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tparam_space: dict,\n",
    "\t\tn_iter: int = 100,\n",
    "\t\tinitial_temp: float = 1.0,\n",
    "\t\tcooling_rate: float = 0.95,\n",
    "\t\tmin_temp: float = 0.01,\n",
    "\t\trandom_state: int = 42,\n",
    "\t):\n",
    "\t\tself.param_space = param_space\n",
    "\t\tself.n_iter = n_iter\n",
    "\t\tself.initial_temp = initial_temp\n",
    "\t\tself.cooling_rate = cooling_rate\n",
    "\t\tself.min_temp = min_temp\n",
    "\t\tself.random_state = random_state\n",
    "\t\tself.best_params_ = None\n",
    "\t\tself.best_score_ = -np.inf\n",
    "\t\tself.cv_results_ = {\"mean_test_score\": []}\n",
    "\n",
    "\tdef _sample_params(self) -> dict:\n",
    "\t\t\"\"\"Sample random parameters from the parameter space.\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\tkey: values.rvs(random_state=self.random_state)\n",
    "\t\t\tif hasattr(values, \"rvs\")  # scipy distribution\n",
    "\t\t\telse random.choice(values)\n",
    "\t\t\tfor key, values in self.param_space.items()\n",
    "\t\t}\n",
    "\n",
    "\tdef _neighbor_params(self, current_params: dict) -> dict:\n",
    "\t\t\"\"\"Generate neighboring parameters by slightly modifying current ones.\"\"\"\n",
    "\t\tneighbor = deepcopy(current_params)\n",
    "\n",
    "\t\t# Choose a random parameter to modify\n",
    "\t\tparam_to_modify = random.choice(list(self.param_space.keys()))\n",
    "\n",
    "\t\tif hasattr(self.param_space[param_to_modify], \"rvs\"):  # continuous parameter\n",
    "\t\t\tif param_to_modify == \"nu\":\n",
    "\t\t\t\t# For nu, stay within bounds [0.001, 1.0]\n",
    "\t\t\t\tcurrent_val = neighbor[param_to_modify]\n",
    "\t\t\t\tneighbor[param_to_modify] = np.clip(\n",
    "\t\t\t\t\tcurrent_val + np.random.normal(0, 0.05 * current_val), 0.001, 1.0\n",
    "\t\t\t\t)\n",
    "\t\t\telif param_to_modify == \"gamma\":\n",
    "\t\t\t\t# For gamma, use log-space perturbation\n",
    "\t\t\t\tneighbor[param_to_modify] = 10 ** np.clip(\n",
    "\t\t\t\t\tnp.log10(neighbor[param_to_modify]) + np.random.normal(0, 0.1),\n",
    "\t\t\t\t\t-4,\n",
    "\t\t\t\t\t1,\n",
    "\t\t\t\t)  # 1e-4 to 10\n",
    "\t\t\telif param_to_modify == \"tol\":\n",
    "\t\t\t\t# For tolerance, use log-space perturbation\n",
    "\t\t\t\tneighbor[param_to_modify] = 10 ** np.clip(\n",
    "\t\t\t\t\tnp.log10(neighbor[param_to_modify]) + np.random.normal(0, 0.1),\n",
    "\t\t\t\t\t-6,\n",
    "\t\t\t\t\t-1,\n",
    "\t\t\t\t)  # 1e-6 to 1e-1\n",
    "\t\telse:  # discrete parameter\n",
    "\t\t\tneighbor[param_to_modify] = random.choice(self.param_space[param_to_modify])\n",
    "\n",
    "\t\treturn neighbor\n",
    "\n",
    "\tdef _evaluate_params(self, params: ParamGrid, X: DataFrame, y: Series) -> float:\n",
    "\t\t\"\"\"Evaluate parameter configuration using cross-validation.\"\"\"\n",
    "\t\treturn np.mean(\n",
    "\t\t\tcross_val_score(\n",
    "\t\t\t\tOneClassSVM(**params, kernel=\"rbf\"), X, y, cv=4, scoring=score_function\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y: Series):\n",
    "\t\t\"\"\"Fit the simulated annealing search.\"\"\"\n",
    "\t\trandom.seed(self.random_state)\n",
    "\t\tnp.random.seed(self.random_state)\n",
    "\n",
    "\t\t# Initialize with random parameters\n",
    "\t\tcurrent_params = self._sample_params()\n",
    "\t\tcurrent_score = self._evaluate_params(current_params, X, y)\n",
    "\n",
    "\t\tself.best_params_ = deepcopy(current_params)\n",
    "\t\tself.best_score_ = current_score\n",
    "\n",
    "\t\ttemperature = self.initial_temp\n",
    "\n",
    "\t\tfor iteration in range(self.n_iter):\n",
    "\t\t\t# Generate neighbor, store the score for cv_results and Accept or reject neighbor\n",
    "\t\t\tneighbor_params = self._neighbor_params(current_params)\n",
    "\t\t\tneighbor_score = self._evaluate_params(neighbor_params, X, y)\n",
    "\t\t\tself.cv_results_[\"mean_test_score\"].append(neighbor_score)\n",
    "\n",
    "\t\t\tif neighbor_score > current_score:  # Better solution - always accept\n",
    "\t\t\t\tcurrent_params = neighbor_params\n",
    "\t\t\t\tcurrent_score = neighbor_score\n",
    "\t\t\telse:  # Worse solution - accept with probability\n",
    "\t\t\t\tif (\n",
    "\t\t\t\t\trandom.random()\n",
    "\t\t\t\t\t< np.exp(neighbor_score - current_score / temperature)\n",
    "\t\t\t\t\tif temperature > 0\n",
    "\t\t\t\t\telse 0\n",
    "\t\t\t\t):\n",
    "\t\t\t\t\tcurrent_params = neighbor_params\n",
    "\t\t\t\t\tcurrent_score = neighbor_score\n",
    "\n",
    "\t\t\tif current_score > self.best_score_:  # Update best solution\n",
    "\t\t\t\tself.best_params_ = deepcopy(current_params)\n",
    "\t\t\t\tself.best_score_ = current_score\n",
    "\n",
    "\t\t\t# Cool down\n",
    "\t\t\ttemperature = max(temperature * self.cooling_rate, self.min_temp)\n",
    "\n",
    "\t\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticAlgorithmSearch:\n",
    "\t\"\"\"\n",
    "\tCustom Genetic Algorithm implementation for hyperparameter optimization.\n",
    "\n",
    "\tGenetic Algorithm Search:\n",
    "\t- Population-based optimization: Maintains diverse parameter sets\n",
    "\t- Tournament selection: Robust parent selection mechanism\n",
    "\t- Uniform crossover: Parameter exchange between parents\n",
    "\t- Adaptive mutation: Random parameter changes with configurable rate\n",
    "\t- Elite preservation: Keeps best solutions across generations\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tparam_space: dict,\n",
    "\t\tpopulation_size: int = 20,\n",
    "\t\tn_generations: int = 10,\n",
    "\t\tmutation_rate: float = 0.1,\n",
    "\t\tcrossover_rate: float = 0.8,\n",
    "\t\telite_size: int = 2,\n",
    "\t\trandom_state: int = 42,\n",
    "\t):\n",
    "\t\tself.param_space = param_space\n",
    "\t\tself.population_size = population_size\n",
    "\t\tself.n_generations = n_generations\n",
    "\t\tself.mutation_rate = mutation_rate\n",
    "\t\tself.crossover_rate = crossover_rate\n",
    "\t\tself.elite_size = elite_size\n",
    "\t\tself.random_state = random_state\n",
    "\t\tself.best_params_ = None\n",
    "\t\tself.best_score_ = -np.inf\n",
    "\t\tself.cv_results_ = {\"mean_test_score\": []}\n",
    "\n",
    "\tdef _create_individual(self) -> dict:\n",
    "\t\t\"\"\"Create a random individual (parameter set).\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\tkey: values.rvs(random_state=self.random_state)\n",
    "\t\t\tif hasattr(values, \"rvs\")  # scipy distribution\n",
    "\t\t\telse random.choice(values)\n",
    "\t\t\tfor key, values in self.param_space.items()\n",
    "\t\t}\n",
    "\n",
    "\tdef _crossover(self, parent1: dict, parent2: dict) -> tuple[dict, dict]:\n",
    "\t\t\"\"\"Create two offspring from two parents using uniform crossover.\"\"\"\n",
    "\t\tchild1, child2 = deepcopy(parent1), deepcopy(parent2)\n",
    "\n",
    "\t\tfor key in parent1.keys():\n",
    "\t\t\tif random.random() < 0.5:  # Swap parameter values\n",
    "\t\t\t\tchild1[key], child2[key] = child2[key], child1[key]\n",
    "\n",
    "\t\treturn child1, child2\n",
    "\n",
    "\tdef _mutate(self, individual: dict) -> dict:\n",
    "\t\t\"\"\"Mutate an individual by randomly changing some parameters.\"\"\"\n",
    "\t\tmutated = deepcopy(individual)\n",
    "\n",
    "\t\tfor key in individual.keys():\n",
    "\t\t\tif random.random() < self.mutation_rate:\n",
    "\t\t\t\tmutated[key] = (\n",
    "\t\t\t\t\tself.param_space[key].rvs(random_state=self.random_state)\n",
    "\t\t\t\t\tif hasattr(self.param_space[key], \"rvs\")  # continuous parameter\n",
    "\t\t\t\t\telse random.choice(self.param_space[key])  # discrete parameter\n",
    "\t\t\t\t)\n",
    "\t\treturn mutated\n",
    "\n",
    "\tdef _tournament_selection(\n",
    "\t\tself, population: list, fitness_scores: list, tournament_size: int = 3\n",
    "\t) -> dict:\n",
    "\t\t\"\"\"Select an individual using tournament selection.\"\"\"\n",
    "\t\ttournament_indices = random.sample(\n",
    "\t\t\trange(len(population)), min(tournament_size, len(population))\n",
    "\t\t)\n",
    "\t\treturn population[\n",
    "\t\t\ttournament_indices[\n",
    "\t\t\t\tnp.argmax([fitness_scores[i] for i in tournament_indices])\n",
    "\t\t\t]\n",
    "\t\t]\n",
    "\n",
    "\tdef _evaluate_params(self, params: dict, X: DataFrame, y: Series) -> float:\n",
    "\t\t\"\"\"Evaluate parameter configuration using cross-validation.\"\"\"\n",
    "\t\treturn np.mean(\n",
    "\t\t\tcross_val_score(\n",
    "\t\t\t\tOneClassSVM(**params, kernel=\"rbf\"), X, y, cv=4, scoring=score_function\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y: Series):\n",
    "\t\t\"\"\"Fit the genetic algorithm search.\"\"\"\n",
    "\t\trandom.seed(self.random_state)\n",
    "\t\tnp.random.seed(self.random_state)\n",
    "\t\tpopulation = [self._create_individual() for _ in range(self.population_size)]\n",
    "\n",
    "\t\tfor generation in range(self.n_generations):  # Evaluate fitness\n",
    "\t\t\tprint(f\"Evaluating Generation {generation}\")\n",
    "\t\t\tfitness_scores = []\n",
    "\t\t\tfor individual in population:\n",
    "\t\t\t\tscore = self._evaluate_params(individual, X, y)\n",
    "\t\t\t\tfitness_scores.append(score)\n",
    "\t\t\t\tself.cv_results_[\"mean_test_score\"].append(score)\n",
    "\n",
    "\t\t\t\tif score > self.best_score_:\n",
    "\t\t\t\t\tself.best_params_ = deepcopy(individual)\n",
    "\t\t\t\t\tself.best_score_ = score\n",
    "\n",
    "\t\t\t# Create next generation | Elite selection - keep best individuals\n",
    "\t\t\tnew_population = [\n",
    "\t\t\t\tdeepcopy(population[idx])\n",
    "\t\t\t\tfor idx in np.argsort(fitness_scores)[-self.elite_size :]\n",
    "\t\t\t]\n",
    "\t\t\t# Generate offspring\n",
    "\t\t\twhile len(new_population) < self.population_size:\n",
    "\t\t\t\t# Selection\n",
    "\t\t\t\tparent1 = self._tournament_selection(population, fitness_scores)\n",
    "\t\t\t\tparent2 = self._tournament_selection(population, fitness_scores)\n",
    "\t\t\t\t# Crossover\n",
    "\t\t\t\tif random.random() < self.crossover_rate:\n",
    "\t\t\t\t\tchild1, child2 = self._crossover(parent1, parent2)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tchild1, child2 = deepcopy(parent1), deepcopy(parent2)\n",
    "\t\t\t\t# Mutation\n",
    "\t\t\t\tnew_population.extend([self._mutate(child1), self._mutate(child2)])\n",
    "\t\t\t# Trim to exact population size\n",
    "\t\t\tpopulation = new_population[: self.population_size]\n",
    "\n",
    "\t\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_grid(\n",
    "\tsearch_method: Literal[\n",
    "\t\t\"Grid\", \"Random\", \"SimulatedAnnealing\", \"GeneticAlgorithm\", \"Bayesian\"\n",
    "\t],\n",
    "\tuse_log_dist: bool = False,\n",
    ") -> ParamGrid:\n",
    "\tif search_method == \"Grid\":\n",
    "\t\treturn {\n",
    "\t\t\t\"nu\": [0.01, 0.05, 0.1, 0.25],\n",
    "\t\t\t\"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1],\n",
    "\t\t\t\"tol\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "\t\t}\n",
    "\telif search_method in [\"Random\", \"SimulatedAnnealing\", \"GeneticAlgorithm\"]:\n",
    "\t\tif use_log_dist:  # Log-uniform for better coverage\n",
    "\t\t\treturn {\n",
    "\t\t\t\t\"nu\": loguniform(0.001, 0.3),\n",
    "\t\t\t\t\"gamma\": loguniform(1e-4, 10),\n",
    "\t\t\t\t\"tol\": loguniform(1e-6, 1e-1),\n",
    "\t\t\t}\n",
    "\t\telse:\n",
    "\t\t\treturn {\n",
    "\t\t\t\t\"nu\": [0.01, 0.025, 0.05, 0.75, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "\t\t\t\t\"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1],\n",
    "\t\t\t\t\"tol\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "\t\t\t}\n",
    "\telif search_method == \"Bayesian\":\n",
    "\t\treturn {\"nu\": (0.01, 0.5), \"gamma\": (1e-4, 1), \"tol\": (1e-5, 1e-1)}\n",
    "\n",
    "\n",
    "def update_train_vars(\n",
    "\ti: int, activities: ndarray\n",
    ") -> tuple[DataFrame, Series, DataFrame, Series]:\n",
    "\ttraining = (  # picks the first n samples of each class\n",
    "\t\tX_train[X_train[\"activity\"].isin(activities[:i])]\n",
    "\t\t.groupby(\"activity\")\n",
    "\t\t.head(MIN_SAMPLES)\n",
    "\t)\n",
    "\ttesting = X_test[X_test[\"activity\"] == activities[i]].head(MIN_SAMPLES)\n",
    "\ttraining.loc[:, \"isNovelty\"], testing.loc[:, \"isNovelty\"] = False, True\n",
    "\tnovelty = concat(\n",
    "\t\t[testing, training.sample(n=int(0.15 * len(training)), random_state=42)]\n",
    "\t)\n",
    "\treturn (\n",
    "\t\ttraining.drop(columns=[\"isNovelty\"]),\n",
    "\t\ttraining[\"isNovelty\"],\n",
    "\t\t# only current activity (as novelty)\n",
    "\t\tnovelty.drop(columns=[\"isNovelty\"]),\n",
    "\t\tnovelty[\"isNovelty\"],\n",
    "\t)\n",
    "\n",
    "\n",
    "def train_search_method(\n",
    "\ttraining_data: DataFrame,\n",
    "\ttrain_targets: Series,\n",
    "\tsearch_type: Literal[\"Grid\", \"Random\", \"SimulatedAnnealing\", \"GeneticAlgorithm\"],\n",
    "\tparams: dict[str, list],\n",
    "\tscoring: Callable,\n",
    "\tn_iter: int | None = 100,\n",
    "\tcv: int = 4,\n",
    "\tverbose: int = 1,\n",
    "\trandom_state: int = 42,\n",
    ") -> (\n",
    "\tRandomizedSearchCV\n",
    "\t| GridSearchCV\n",
    "\t| SimulatedAnnealingSearch\n",
    "\t| GeneticAlgorithmSearch\n",
    "):\n",
    "\tif search_type == \"SimulatedAnnealing\":\n",
    "\t\treturn SimulatedAnnealingSearch(\n",
    "\t\t\tparam_space=params,\n",
    "\t\t\tn_iter=n_iter or 100,\n",
    "\t\t\tinitial_temp=1.0,\n",
    "\t\t\tcooling_rate=0.95,\n",
    "\t\t\trandom_state=random_state,\n",
    "\t\t).fit(training_data, train_targets)\n",
    "\n",
    "\telif search_type == \"GeneticAlgorithm\":\n",
    "\t\t# For GA, i use n_iter as total evaluations = population_size * n_generations\n",
    "\t\tpopulation_size = min(20, n_iter // 5) if n_iter else 20\n",
    "\t\treturn GeneticAlgorithmSearch(\n",
    "\t\t\tparam_space=params,\n",
    "\t\t\tpopulation_size=population_size,\n",
    "\t\t\tn_generations=(n_iter // population_size) if n_iter else 5,\n",
    "\t\t\tmutation_rate=0.1,\n",
    "\t\t\tcrossover_rate=0.8,\n",
    "\t\t\trandom_state=random_state,\n",
    "\t\t).fit(training_data, train_targets)\n",
    "\n",
    "\telse:\n",
    "\t\t# Original implementation for Grid and Random search\n",
    "\t\tsearch_cls = RandomizedSearchCV if search_type == \"Random\" else GridSearchCV\n",
    "\t\tsearch_kwargs = {\n",
    "\t\t\tf\"param_{'distributions' if search_type == 'Random' else 'grid'}\": params,\n",
    "\t\t\t\"estimator\": OneClassSVM(kernel=\"rbf\"),\n",
    "\t\t\t\"scoring\": scoring,\n",
    "\t\t\t\"cv\": cv,\n",
    "\t\t\t\"verbose\": verbose,\n",
    "\t\t\t\"error_score\": \"raise\",\n",
    "\t\t}\n",
    "\t\tif search_type == \"Random\" and n_iter:\n",
    "\t\t\tsearch_kwargs.update({\"n_iter\": n_iter, \"random_state\": random_state})\n",
    "\t\treturn search_cls(**search_kwargs).fit(training_data, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search_methods(\n",
    "\tgrid_scores: list[float], random_scores: list[float]\n",
    ") -> dict[str, float]:\n",
    "\t\"\"\"\n",
    "\tStatistical comparison of search methods using Wilcoxon signed-rank test.\n",
    "\t\"\"\"\n",
    "\tif len(grid_scores) != len(random_scores):\n",
    "\t\traise ValueError(\"Score arrays must have equal length\")\n",
    "\n",
    "\tstatistic, p_value = wilcoxon(grid_scores, random_scores, alternative=\"two-sided\")\n",
    "\n",
    "\treturn {\n",
    "\t\t\"wilcoxon_statistic\": statistic,\n",
    "\t\t\"p_value\": p_value,\n",
    "\t\t\"grid_mean\": np.mean(grid_scores),\n",
    "\t\t\"random_mean\": np.mean(random_scores),\n",
    "\t\t\"effect_size\": (np.mean(grid_scores) - np.mean(random_scores))\n",
    "\t\t/ np.std(grid_scores + random_scores),\n",
    "\t}\n",
    "\n",
    "\n",
    "def update_params_grid(\n",
    "\tcv_results: dict[str, tuple], og_param_grid: ParamGrid\n",
    ") -> ParamGrid:\n",
    "\tparams = [\"gamma\", \"nu\", \"tol\"]\n",
    "\ttop_entries = (\n",
    "\t\tDataFrame(\n",
    "\t\t\tzip(\n",
    "\t\t\t\tcv_results[\"rank_test_score\"],\n",
    "\t\t\t\tcv_results[\"param_gamma\"],\n",
    "\t\t\t\tcv_results[\"param_nu\"],\n",
    "\t\t\t\tcv_results[\"param_tol\"],\n",
    "\t\t\t),\n",
    "\t\t\tcolumns=[\"rank_test_score\", \"gamma\", \"nu\", \"tol\"],\n",
    "\t\t)\n",
    "\t\t.sort_values(\"rank_test_score\")\n",
    "\t\t.head(NUM_TRIALS)\n",
    "\t)\n",
    "\t# Check if we're stuck with the same parameter space\n",
    "\tif len(top_entries) == NUM_TRIALS:\n",
    "\t\tprint(\"Detected potential parameter space stagnation, diversifying...\")\n",
    "\n",
    "\t\tcurrent_params = {col: set(top_entries[col]) for col in params}\n",
    "\t\tunused_params = {\n",
    "\t\t\tparam: list(set(og_param_grid[param]) - current_params[param])\n",
    "\t\t\tfor param in params\n",
    "\t\t}\n",
    "\t\t# Replace least effective values with unused ones\n",
    "\t\tfor param in params:\n",
    "\t\t\tif unused_params[param]:  # If there are unused values available\n",
    "\t\t\t\tcurrent_unique = list(dict.fromkeys(top_entries[param]))\n",
    "\t\t\t\t# Remove the least effective (last) value and add an unused one\n",
    "\t\t\t\tif len(current_unique) > 1 and unused_params[param]:\n",
    "\t\t\t\t\tcurrent_unique = current_unique[:-1] + [unused_params[param][0]]\n",
    "\t\t\t\telif unused_params[param]:\n",
    "\t\t\t\t\t# If only one value, replace it partially\n",
    "\t\t\t\t\tcurrent_unique.append(unused_params[param][0])\n",
    "\n",
    "\t\t\t\ttop_entries.loc[  # Update the parameter list\n",
    "\t\t\t\t\ttop_entries[param] == list(dict.fromkeys(top_entries[param]))[-1],\n",
    "\t\t\t\t\tparam,\n",
    "\t\t\t\t] = unused_params[param][0]\n",
    "\n",
    "\texmp = {col: list(dict.fromkeys(top_entries[col])) for col in params}\n",
    "\tcartesian_size = len(exmp[\"gamma\"]) * len(exmp[\"nu\"]) * len(exmp[\"tol\"])\n",
    "\n",
    "\tif cartesian_size > NUM_TRIALS:\n",
    "\t\twhile cartesian_size > NUM_TRIALS:\n",
    "\t\t\t# Find which parameter to reduce (try removing last value from each)\n",
    "\t\t\tbest_reduction = None\n",
    "\t\t\tbest_param = None\n",
    "\n",
    "\t\t\tfor param in params:\n",
    "\t\t\t\tif len(exmp[param]) > 1:  # Only reduce if more than 1 value remains\n",
    "\t\t\t\t\t# Calculate new cartesian size if we remove last value from this param\n",
    "\t\t\t\t\ttemp_sizes = [\n",
    "\t\t\t\t\t\tlen(exmp[p]) if p != param else len(exmp[p]) - 1 for p in params\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t\tnew_size = temp_sizes[0] * temp_sizes[1] * temp_sizes[2]\n",
    "\t\t\t\t\t# Check if this gets us closer to NUM_TRIALS without going under\n",
    "\t\t\t\t\tif new_size >= NUM_TRIALS and (\n",
    "\t\t\t\t\t\tbest_reduction is None or new_size < best_reduction\n",
    "\t\t\t\t\t):\n",
    "\t\t\t\t\t\tbest_reduction = new_size\n",
    "\t\t\t\t\t\tbest_param = param\n",
    "\n",
    "\t\t\t# If no good reduction found, just remove from the param with most values\n",
    "\t\t\tif best_param is None:\n",
    "\t\t\t\tparam_lengths = [(param, len(exmp[param])) for param in params]\n",
    "\t\t\t\tparam_lengths.sort(key=lambda x: x[1], reverse=True)\n",
    "\t\t\t\tbest_param = param_lengths[0][0]\n",
    "\n",
    "\t\t\t# Remove the last (least effective) value from the chosen parameter\n",
    "\t\t\tif len(exmp[best_param]) > 1:\n",
    "\t\t\t\texmp[best_param] = exmp[best_param][:-1]\n",
    "\n",
    "\t\t\tcartesian_size = len(exmp[\"gamma\"]) * len(exmp[\"nu\"]) * len(exmp[\"tol\"])\n",
    "\t\t\t# Safety break to avoid infinite loop\n",
    "\t\t\tif all(len(exmp[param]) == 1 for param in params):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\tassert cartesian_size == NUM_TRIALS, \"reducing the params space failed\"\n",
    "\n",
    "\tprint(f\"dict of len {cartesian_size} :\", exmp)\n",
    "\treturn exmp\n",
    "\n",
    "\n",
    "def eval_search_method(\n",
    "\tactivities: ndarray,\n",
    "\tsearch_name: Literal[\"Grid\", \"Random\", \"SimulatedAnnealing\", \"GeneticAlgorithm\"],\n",
    "\tuse_log_dist: bool = False,\n",
    ") -> SearchResult:\n",
    "\tdist = get_param_grid(search_name, use_log_dist)\n",
    "\tMAXIMAZED = False\n",
    "\tBEST_SCORES = None\n",
    "\n",
    "\tglobal testing_data, test_targets, logger\n",
    "\tstart_time = time()\n",
    "\n",
    "\tfor i in range(1, len(activities)):\n",
    "\t\ttraining_data, train_targets, test_data, testing_targets = update_train_vars(\n",
    "\t\t\ti, activities\n",
    "\t\t)\n",
    "\t\ttesting_data = test_data\n",
    "\t\ttest_targets = testing_targets\n",
    "\t\tprint(f\"Training for activities {activities[:i]}\")\n",
    "\n",
    "\t\tif not MAXIMAZED:\n",
    "\t\t\tsearch_method = train_search_method(\n",
    "\t\t\t\ttraining_data=training_data,\n",
    "\t\t\t\ttrain_targets=train_targets,\n",
    "\t\t\t\tsearch_type=search_name,\n",
    "\t\t\t\tparams=dist,\n",
    "\t\t\t\tscoring=score_function,\n",
    "\t\t\t)\n",
    "\t\t\tBEST_SCORES = search_method\n",
    "\t\t\tMAXIMAZED = True\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"Already maximized, suggesting new {NUM_TRIALS} points\")\n",
    "\t\t\t# For metaheuristics, we don't update param grid like GridSearch\n",
    "\t\t\tif search_name in [\"SimulatedAnnealing\", \"GeneticAlgorithm\"]:\n",
    "\t\t\t\tsearch_method = train_search_method(\n",
    "\t\t\t\t\ttraining_data=training_data,\n",
    "\t\t\t\t\ttrain_targets=train_targets,\n",
    "\t\t\t\t\tsearch_type=search_name,\n",
    "\t\t\t\t\tparams=dist,\n",
    "\t\t\t\t\tscoring=score_function,\n",
    "\t\t\t\t\tn_iter=NUM_TRIALS,\n",
    "\t\t\t\t)\n",
    "\t\t\telse:\n",
    "\t\t\t\tsearch_method = train_search_method(\n",
    "\t\t\t\t\ttraining_data=training_data,\n",
    "\t\t\t\t\ttrain_targets=train_targets,\n",
    "\t\t\t\t\tsearch_type=search_name,\n",
    "\t\t\t\t\tparams=update_params_grid(search_method.cv_results_, dist)\n",
    "\t\t\t\t\tif search_name == \"Grid\"\n",
    "\t\t\t\t\telse dist,\n",
    "\t\t\t\t\tscoring=score_function,\n",
    "\t\t\t\t\tn_iter=NUM_TRIALS if search_name == \"Random\" else None,\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tif search_method.best_score_ > BEST_SCORES.best_score_:\n",
    "\t\t\t\tBEST_SCORES = search_method\n",
    "\n",
    "\t\tprint(f\"{search_name} Search Best Params:\", search_method.best_params_)\n",
    "\n",
    "\treturn SearchResult(\n",
    "\t\tmethod=search_name + \"_search\",\n",
    "\t\tbest_params=BEST_SCORES.best_params_,\n",
    "\t\tbest_score=BEST_SCORES.best_score_,\n",
    "\t\tcv_scores=BEST_SCORES.cv_results_[\"mean_test_score\"].tolist()\n",
    "\t\tif hasattr(BEST_SCORES.cv_results_[\"mean_test_score\"], \"tolist\")\n",
    "\t\telse BEST_SCORES.cv_results_[\"mean_test_score\"],\n",
    "\t\tfit_time=time() - start_time,\n",
    "\t\tn_evaluations=len(BEST_SCORES.cv_results_[\"mean_test_score\"]),\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = X_train[\"activity\"].unique()\n",
    "\n",
    "logger = configure_file_logger(\"../reports/logs_grid.log\")\n",
    "grid_result = eval_search_method(activities, \"Grid\")\n",
    "\n",
    "logger = configure_file_logger(\"../reports/logs_rand.log\")\n",
    "rand_result = eval_search_method(activities, \"Random\")\n",
    "\n",
    "logger = configure_file_logger(\"../reports/logs_rand_log.log\")\n",
    "rand_log_result = eval_search_method(activities, \"Random\", True)\n",
    "\n",
    "# New metaheuristic searches\n",
    "logger = configure_file_logger(\"../reports/logs_sian.log\")\n",
    "sa_result = eval_search_method(activities, \"SimulatedAnnealing\", True)\n",
    "\n",
    "logger = configure_file_logger(\"../reports/logs_geal.log\")\n",
    "ga_result = eval_search_method(activities, \"GeneticAlgorithm\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../conf/test_results.json\", \"w\") as file:\n",
    "\tfile.write(\n",
    "\t\tdumps(\n",
    "\t\t\t[\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"test\": \"Grid x Rand\",\n",
    "\t\t\t\t\t**compare_search_methods(\n",
    "\t\t\t\t\t\tgrid_result.cv_scores, rand_result.cv_scores\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"test\": \"Grid x SA\",\n",
    "\t\t\t\t\t**compare_search_methods(\n",
    "\t\t\t\t\t\tgrid_result.cv_scores, sa_result.cv_scores\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"test\": \"Grid x GA\",\n",
    "\t\t\t\t\t**compare_search_methods(\n",
    "\t\t\t\t\t\tgrid_result.cv_scores, ga_result.cv_scores\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"test\": \"Rand x SA\",\n",
    "\t\t\t\t\t**compare_search_methods(\n",
    "\t\t\t\t\t\trand_result.cv_scores, sa_result.cv_scores\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"test\": \"Rand x GA\",\n",
    "\t\t\t\t\t**compare_search_methods(\n",
    "\t\t\t\t\t\trand_result.cv_scores, ga_result.cv_scores\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"test\": \"SA x GA\",\n",
    "\t\t\t\t\t**compare_search_methods(sa_result.cv_scores, ga_result.cv_scores),\n",
    "\t\t\t\t},\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
