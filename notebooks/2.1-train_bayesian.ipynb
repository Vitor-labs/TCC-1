{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Final\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.util import load_logs\n",
    "from numpy import ndarray, where\n",
    "from pandas import DataFrame, Series, concat, read_csv, set_option\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "set_option(\"display.max_columns\", None)\n",
    "LOGS_PATH: Final[str] = \"../reports/logs_bayesian.log\"\n",
    "NUM_TRIALS: Final[int] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = read_csv(\"../data/PAMAP2/x_train_data.csv\")\n",
    "X_test = read_csv(\"../data/PAMAP2/x_test_data.csv\")\n",
    "y_train = read_csv(\"../data/PAMAP2/y_train_data.csv\")\n",
    "y_test = read_csv(\"../data/PAMAP2/y_test_data.csv\")\n",
    "\n",
    "X_train[\"activity\"] = y_train  # First 80% of the data\n",
    "X_test[\"activity\"] = y_test  # Last 20% of the data\n",
    "\n",
    "# MIN_SAMPLES = X_train[\"activity\"].value_counts().min()\n",
    "MIN_SAMPLES = X_train[\"activity\"].value_counts().sort_values().iloc[0]\n",
    "MAXIMAZED = False\n",
    "\n",
    "training_data: DataFrame\n",
    "testing_data: DataFrame\n",
    "train_targets: Series\n",
    "test_targets: Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(nu: float, gamma: float, tol: float) -> float:\n",
    "    \"\"\"\n",
    "    Objective function to optimize F1-Score using Bayesian Optimization.\n",
    "\n",
    "    Args:\n",
    "        nu (float): nu parameter for the OneClassSVM.\n",
    "        gamma (float): gamma parameter for the OneClassSVM.\n",
    "        tol (float): tol parameter for the OneClassSVM.\n",
    "\n",
    "    Returns:\n",
    "        float: Macro Average F1-Score.\n",
    "    \"\"\"\n",
    "    global training_data, testing_data, train_targets, test_targets\n",
    "    oc_svm = OneClassSVM(kernel=\"rbf\", nu=nu, gamma=gamma, tol=tol).fit(training_data)\n",
    "\n",
    "    return float(\n",
    "        f1_score(test_targets, where(oc_svm.predict(testing_data) == 1, False, True))\n",
    "    )\n",
    "\n",
    "\n",
    "def update_train_vars(\n",
    "    i: int, activities: ndarray\n",
    ") -> tuple[DataFrame, Series, DataFrame, Series]:\n",
    "    training = (  # picks the first n samples of each class\n",
    "        X_train[X_train[\"activity\"].isin(activities[:i])]\n",
    "        .groupby(\"activity\")\n",
    "        .head(MIN_SAMPLES)\n",
    "    )\n",
    "    testing = X_test[X_test[\"activity\"] == activities[i]].head(MIN_SAMPLES)\n",
    "    training.loc[:, \"isNovelty\"], testing.loc[:, \"isNovelty\"] = False, True\n",
    "    novelty = concat(\n",
    "        [testing, training.sample(n=int(0.15 * len(training)), random_state=42)]\n",
    "    )\n",
    "    return (\n",
    "        training.drop(columns=[\"isNovelty\"]),\n",
    "        training[\"isNovelty\"],\n",
    "        # only current activity (as novelty)\n",
    "        novelty.drop(columns=[\"isNovelty\"]),\n",
    "        novelty[\"isNovelty\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    objective_function,\n",
    "    {\"nu\": (0.01, 0.5), \"gamma\": (1e-4, 1), \"tol\": (1e-5, 1e-1)},\n",
    "    random_state=42,\n",
    ")\n",
    "if not os.path.exists(LOGS_PATH):\n",
    "    with open(LOGS_PATH, \"w\") as fp:\n",
    "        pass\n",
    "\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, JSONLogger(LOGS_PATH, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len((activities := X_train[\"activity\"].unique()))):\n",
    "    training_data, train_targets, testing_data, test_targets = update_train_vars(\n",
    "        i, activities\n",
    "    )\n",
    "    print(f\"Activity: {activities[:i]}, with {training_data.shape[0]} samples\")\n",
    "    load_logs(optimizer, logs=[LOGS_PATH])\n",
    "    print(\"New optimizer is now aware of {} points.\".format(len(optimizer.space)))\n",
    "\n",
    "    if not MAXIMAZED:\n",
    "        print(\"Maximizing for the first time (100 iterations)...\")\n",
    "        optimizer.maximize(init_points=25, n_iter=75)\n",
    "        MAXIMAZED = True\n",
    "    else:\n",
    "        load_logs(optimizer, logs=[LOGS_PATH])\n",
    "        print(f\"Already maximized, sugesting new {NUM_TRIALS} points\")\n",
    "\n",
    "        iters = 0\n",
    "        for j in range(NUM_TRIALS):\n",
    "            MAX = optimizer.max[\"target\"]  # type: ignore\n",
    "            next_point_to_probe = optimizer.suggest()\n",
    "            target = objective_function(**next_point_to_probe)\n",
    "            optimizer.register(params=next_point_to_probe, target=target)\n",
    "            if target > MAX:\n",
    "                MAX = target\n",
    "                print(\n",
    "                    f\"New best points found: {next_point_to_probe}. Max: {MAX}, continuing optimization\"\n",
    "                )\n",
    "                iters += j\n",
    "                j = 0\n",
    "        print(f\"Maximized for {iters} iterations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
