{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Final, Literal\n",
    "\n",
    "import numpy as np\n",
    "import structlog\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from numpy import argmax, concatenate, ndarray, vstack, where\n",
    "from pandas import DataFrame, Series, concat, read_csv, set_option, to_datetime\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import loguniform, uniform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import (\n",
    "\tauc,\n",
    "\taverage_precision_score,\n",
    "\tclassification_report,\n",
    "\tf1_score,\n",
    "\tprecision_recall_curve,\n",
    "\troc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "set_option(\"display.max_columns\", None)\n",
    "logger: Final[structlog.stdlib.BoundLogger] = structlog.get_logger(__name__)\n",
    "type ParamGrid = dict[str, tuple[float | str, ...]]\n",
    "\n",
    "COLUMNS: Final[list[str]] = [\n",
    "\t\"timestamp\",\n",
    "\t\"activity\",\n",
    "\t\"heart_rate\",\n",
    "\t*[\n",
    "\t\tf\"IMU_{body_part}_{suffix}\"\n",
    "\t\tfor body_part in [\"hand\", \"chest\", \"ankle\"]\n",
    "\t\tfor suffix in [\n",
    "\t\t\t\"temp_C\",\n",
    "\t\t\t*[\n",
    "\t\t\t\tf\"{scalar}_{axis}\"\n",
    "\t\t\t\tfor scalar in [\"acc16g_ms^-2\", \"acc6g_ms^-2\", \"gyro_rad/s\", \"mag_Î¼T\"]\n",
    "\t\t\t\tfor axis in [\"x\", \"y\", \"z\"]\n",
    "\t\t\t],\n",
    "\t\t\t*[f\"orient_{x}\" for x in range(1, 5)],\n",
    "\t\t]\n",
    "\t],\n",
    "]\n",
    "IMU_COLUMNS: Final[list[str]] = [\n",
    "\tcol\n",
    "\tfor col in COLUMNS\n",
    "\tif col.startswith(\"IMU_\") and \"acc6g_ms^-2\" not in col and \"orient\" not in col\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "\t\"\"\"LSTM-based Autoencoder for time-series novelty detection.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tn_features: int,\n",
    "\t\tsequence_length: int,\n",
    "\t\thidden_size: int = 64,\n",
    "\t\tn_layers: int = 2,\n",
    "\t\tdropout: float = 0.2,\n",
    "\t):\n",
    "\t\tsuper(LSTMAutoencoder, self).__init__()\n",
    "\t\tself.n_features = n_features\n",
    "\t\tself.sequence_length = sequence_length\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.encoder = nn.LSTM(\n",
    "\t\t\tinput_size=n_features,\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tnum_layers=n_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=dropout if n_layers > 1 else 0,\n",
    "\t\t)\n",
    "\t\tself.decoder = nn.LSTM(\n",
    "\t\t\tinput_size=hidden_size,\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tnum_layers=n_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=dropout if n_layers > 1 else 0,\n",
    "\t\t)\n",
    "\t\tself.output_layer = nn.Linear(hidden_size, n_features)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# Encode\n",
    "\t\t_, (hidden, cell) = self.encoder(x)\n",
    "\t\t# Repeat hidden state for decoder\n",
    "\t\tdecoder_input = hidden[-1].unsqueeze(1).repeat(1, self.sequence_length, 1)\n",
    "\t\t# Decode\n",
    "\t\tdecoder_output, _ = self.decoder(decoder_input, (hidden, cell))\n",
    "\t\t# Output reconstruction\n",
    "\t\treturn self.output_layer(decoder_output)\n",
    "\n",
    "\n",
    "class LSTMAutoencoderWrapper:\n",
    "\t\"\"\"Wrapper for LSTM Autoencoder to match sklearn API.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tn_features: int,\n",
    "\t\tsequence_length: int = 100,\n",
    "\t\thidden_size: int = 64,\n",
    "\t\tn_layers: int = 2,\n",
    "\t\tdropout: float = 0.2,\n",
    "\t\tepochs: int = 50,\n",
    "\t\tbatch_size: int = 64,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\tdevice: str | None = None,\n",
    "\t):\n",
    "\t\tself.n_features = n_features\n",
    "\t\tself.sequence_length = sequence_length\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.n_layers = n_layers\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.epochs = epochs\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\t\tself.model = None\n",
    "\t\tself.threshold = None\n",
    "\n",
    "\tdef _create_sequences(self, data: ndarray) -> ndarray:\n",
    "\t\t\"\"\"Create sliding windows from flat data.\"\"\"\n",
    "\t\tdata = data.astype(np.float32)\n",
    "\t\treturn np.array(\n",
    "\t\t\t[\n",
    "\t\t\t\tdata[i : i + self.sequence_length]\n",
    "\t\t\t\tfor i in range(len(data) - self.sequence_length + 1)\n",
    "\t\t\t],\n",
    "\t\t\tdtype=np.float32,  # Explicitly set dtype\n",
    "\t\t)\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y=None):\n",
    "\t\t\"\"\"Train the LSTM Autoencoder on normal data.\"\"\"\n",
    "\t\t# Convert to numpy\n",
    "\t\tsequences = self._create_sequences(X[IMU_COLUMNS].values.astype(np.float32))\n",
    "\t\tif len(sequences) == 0:\n",
    "\t\t\tlogger.warning(\n",
    "\t\t\t\t\"Not enough data to create sequences. Skipping LSTM-AE training.\"\n",
    "\t\t\t)\n",
    "\t\t\treturn self\n",
    "\n",
    "\t\tself.model = LSTMAutoencoder(\n",
    "\t\t\tn_features=self.n_features,\n",
    "\t\t\tsequence_length=self.sequence_length,\n",
    "\t\t\thidden_size=self.hidden_size,\n",
    "\t\t\tn_layers=self.n_layers,\n",
    "\t\t\tdropout=self.dropout,\n",
    "\t\t).to(self.device)\n",
    "\n",
    "\t\t# Training setup\n",
    "\t\tcriterion = nn.MSELoss()\n",
    "\t\toptimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\t\tdataloader = torch.utils.data.DataLoader(\n",
    "\t\t\ttorch.utils.data.TensorDataset(torch.FloatTensor(sequences)),\n",
    "\t\t\tbatch_size=self.batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t)\n",
    "\t\tself.model.train()\n",
    "\t\tfor epoch in range(self.epochs):  # Training loop\n",
    "\t\t\tepoch_loss = 0\n",
    "\t\t\tfor batch in dataloader:\n",
    "\t\t\t\tbatch_data = batch[0].to(self.device)\n",
    "\t\t\t\t# Forward pass\n",
    "\t\t\t\treconstruction = self.model(batch_data)\n",
    "\t\t\t\tloss = criterion(reconstruction, batch_data)\n",
    "\t\t\t\t# Backward pass\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\tepoch_loss += loss.item()\n",
    "\n",
    "\t\t\tif (epoch + 1) % 10 == 0:\n",
    "\t\t\t\tlogger.info(\n",
    "\t\t\t\t\tf\"LSTM-AE Epoch {epoch + 1}/{self.epochs} \"\n",
    "\t\t\t\t\t+ f\"Loss: {epoch_loss / len(dataloader):.4f}\",\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t# Treshold on training data (95th percentile of reconstruction errors)\n",
    "\t\tself.threshold = np.percentile(self._calc_reconstruction_errors(sequences), 95)\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef _calc_reconstruction_errors(self, sequences: ndarray) -> ndarray:\n",
    "\t\t\"\"\"Compute reconstruction error for sequences.\"\"\"\n",
    "\t\tassert self.model is not None, \"Model is not trained yet.\"\n",
    "\t\tself.model.eval()\n",
    "\t\terrors = []\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i in range(0, len(sequences), self.batch_size):\n",
    "\t\t\t\tbatch_tensor = torch.FloatTensor(sequences[i : i + self.batch_size]).to(\n",
    "\t\t\t\t\tself.device\n",
    "\t\t\t\t)\n",
    "\t\t\t\terrors.extend(  # MSE per sequence\n",
    "\t\t\t\t\ttorch.mean(\n",
    "\t\t\t\t\t\t(batch_tensor - self.model(batch_tensor)) ** 2, dim=(1, 2)\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\t.cpu()\n",
    "\t\t\t\t\t.numpy()\n",
    "\t\t\t\t)\n",
    "\t\treturn np.array(errors)\n",
    "\n",
    "\tdef predict(self, X: DataFrame) -> ndarray:\n",
    "\t\t\"\"\"Predict: 1 for normal, -1 for anomaly.\"\"\"\n",
    "\t\treturn where(self.decision_function(X) > self.threshold, -1, 1)\n",
    "\n",
    "\tdef decision_function(self, X: DataFrame) -> ndarray:\n",
    "\t\t\"\"\"Return anomaly scores (higher = more anomalous).\"\"\"\n",
    "\t\tsequences = self._create_sequences(X[IMU_COLUMNS].values.astype(np.float32))\n",
    "\t\t# Data is already scaled - no transformation needed\n",
    "\t\tif len(sequences) == 0:\n",
    "\t\t\t# If not enough data for a sequence, return high anomaly score\n",
    "\t\t\treturn np.array([self.threshold * 2 if self.threshold else 1.0] * len(X))\n",
    "\n",
    "\t\terrors = self._calc_reconstruction_errors(sequences)\n",
    "\t\t# Pad to match original length (for sequences lost at the end)\n",
    "\t\tif (padding := len(X) - len(errors)) > 0:\n",
    "\t\t\terrors = np.concatenate([errors, errors[-1:].repeat(padding)])\n",
    "\n",
    "\t\treturn errors\n",
    "\n",
    "\tdef get_params(self, deep=True):\n",
    "\t\t\"\"\"\n",
    "\t\tGet parameters for sklearn compatibility.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tdeep (bool): Ignored, present for sklearn compatibility. Default to True.\n",
    "\t\t\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t\"n_features\": self.n_features,\n",
    "\t\t\t\"sequence_length\": self.sequence_length,\n",
    "\t\t\t\"hidden_size\": self.hidden_size,\n",
    "\t\t\t\"n_layers\": self.n_layers,\n",
    "\t\t\t\"dropout\": self.dropout,\n",
    "\t\t\t\"epochs\": self.epochs,\n",
    "\t\t\t\"batch_size\": self.batch_size,\n",
    "\t\t\t\"learning_rate\": self.learning_rate,\n",
    "\t\t}\n",
    "\n",
    "\tdef set_params(self, **params):\n",
    "\t\t\"\"\"Set parameters for sklearn compatibility.\"\"\"\n",
    "\t\tfor key, value in params.items():\n",
    "\t\t\tsetattr(self, key, value)\n",
    "\t\treturn self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoveltyDetectionEnsemble:\n",
    "\t\"\"\"Ensemble combining LSTM-AE, Isolation Forest, and One-Class SVM.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tn_features: int,\n",
    "\t\t# LSTM-AE params\n",
    "\t\tsequence_length: int = 100,\n",
    "\t\thidden_size: int = 64,\n",
    "\t\tn_layers: int = 2,\n",
    "\t\tdropout: float = 0.2,\n",
    "\t\tepochs: int = 50,\n",
    "\t\tbatch_size: int = 64,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\t# Isolation Forest params\n",
    "\t\tn_estimators: int = 200,\n",
    "\t\tmax_samples: int = 256,\n",
    "\t\tcontamination: float = 0.05,\n",
    "\t\t# OC-SVM params\n",
    "\t\tnu: float = 0.05,\n",
    "\t\tgamma: float | Literal[\"scale\", \"auto\"] = \"scale\",\n",
    "\t\t# Ensemble params\n",
    "\t\t# weights: tuple[float, float, float] = (0.4, 0.3, 0.3),\n",
    "\t\tweights: tuple[float, float] = (0.3, 0.3),\n",
    "\t):\n",
    "\t\tself.n_features = n_features\n",
    "\t\tself.weights = np.array(weights)\n",
    "\n",
    "\t\tself.lstm_ae = LSTMAutoencoderWrapper(\n",
    "\t\t\tn_features=n_features,\n",
    "\t\t\tsequence_length=sequence_length,\n",
    "\t\t\thidden_size=hidden_size,\n",
    "\t\t\tn_layers=n_layers,\n",
    "\t\t\tdropout=dropout,\n",
    "\t\t\tepochs=epochs,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tlearning_rate=learning_rate,\n",
    "\t\t)\n",
    "\t\tself.isolation_forest = IsolationForest(\n",
    "\t\t\tn_estimators=n_estimators,\n",
    "\t\t\tmax_samples=max_samples,\n",
    "\t\t\tcontamination=contamination,\n",
    "\t\t\trandom_state=42,\n",
    "\t\t)\n",
    "\t\tself.ocsvm = OneClassSVM(nu=nu, gamma=gamma, kernel=\"rbf\")\n",
    "\n",
    "\t\tself.models = [self.isolation_forest, self.ocsvm]\n",
    "\t\tself.model_names = [\"IsolationForest\", \"OC-SVM\"]\n",
    "\n",
    "\tdef fit(self, X: DataFrame, y=None):\n",
    "\t\t\"\"\"Train all models in the ensemble.\"\"\"\n",
    "\t\tlogger.info(\"Training ensemble models...\")\n",
    "\t\tfor name, model in zip(self.model_names, self.models, strict=True):\n",
    "\t\t\tlogger.info(f\"Training {name}...\")\n",
    "\t\t\tmodel.fit(X, y)\n",
    "\t\tlogger.info(\"Ensemble training complete.\")\n",
    "\t\treturn self\n",
    "\n",
    "\tdef predict(self, X: DataFrame) -> ndarray:\n",
    "\t\t\"\"\"Predict: 1 for normal, -1 for anomaly.\"\"\"\n",
    "\t\tscores = self.decision_function(X)\n",
    "\t\treturn where(scores > np.median(scores), -1, 1)\n",
    "\n",
    "\tdef decision_function(self, X: DataFrame) -> ndarray:\n",
    "\t\t\"\"\"Return combined anomaly scores (higher = more anomalous).\"\"\"\n",
    "\t\treturn np.average(\n",
    "\t\t\t[\n",
    "\t\t\t\t# self._normalize_scores(self.lstm_ae.decision_function(X)),\n",
    "\t\t\t\tself._normalize_scores(\n",
    "\t\t\t\t\t-self.isolation_forest.decision_function(X.values)\n",
    "\t\t\t\t),\n",
    "\t\t\t\tself._normalize_scores(-self.ocsvm.decision_function(X.values)),\n",
    "\t\t\t],\n",
    "\t\t\taxis=0,\n",
    "\t\t\tweights=self.weights,\n",
    "\t\t)\n",
    "\n",
    "\tdef _normalize_scores(self, scores: ndarray) -> ndarray:\n",
    "\t\t\"\"\"Normalize scores to [0, 1] range using min-max scaling.\"\"\"\n",
    "\t\tif (max_score := np.max(scores)) - (min_score := np.min(scores)) == 0:\n",
    "\t\t\treturn np.zeros_like(scores)\n",
    "\t\treturn (scores - min_score) / (max_score - min_score)\n",
    "\n",
    "\tdef get_params(self, deep=True):\n",
    "\t\t\"\"\"Get all ensemble parameters.\"\"\"\n",
    "\t\tparams = {\"n_features\": self.n_features, \"weights\": tuple(self.weights)}\n",
    "\t\tfor model in self.models:\n",
    "\t\t\tparams.update({f\"{k}\": v for k, v in model.get_params(deep=deep).items()})\n",
    "\t\treturn params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_iteration_results(\n",
    "\tlog_file: Path,\n",
    "\titeration: int,\n",
    "\tmetrics: dict,\n",
    "\thpo_method: str,\n",
    ") -> None:\n",
    "\t\"\"\"\n",
    "\tLog results from a single HPO iteration using structured logging format.\n",
    "\n",
    "\tArgs:\n",
    "\t\tlog_file: Path to log file\n",
    "\t\titeration: Current iteration number\n",
    "\t\tmetrics: Dictionary containing metrics, params, and datetime\n",
    "\t\thpo_method: Name of HPO method being used\n",
    "\t\"\"\"\n",
    "\twith open(log_file, \"a\") as f:\n",
    "\t\tf.write(\n",
    "\t\t\tjson.dumps(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"event\": {\n",
    "\t\t\t\t\t\t\"iteration\": iteration,\n",
    "\t\t\t\t\t\t\"fold\": metrics.get(\"fold\", 0),\n",
    "\t\t\t\t\t\t\"target\": metrics.get(\"f1_score\", 0.0),\n",
    "\t\t\t\t\t\t\"avg_precision\": metrics.get(\"avg_precision\", 0.0),\n",
    "\t\t\t\t\t\t\"auc_pr\": metrics.get(\"auc_pr\", 0.0),\n",
    "\t\t\t\t\t\t\"auc_roc\": metrics.get(\"auc_roc\", 0.0),\n",
    "\t\t\t\t\t\t\"params\": metrics.get(\"params\", {}),\n",
    "\t\t\t\t\t\t\"datetime\": metrics.get(\"datetime\", \"\"),\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"logger\": hpo_method,\n",
    "\t\t\t\t\t\"level\": \"info\",\n",
    "\t\t\t\t\t\"timestamp\": datetime.now().isoformat() + \"Z\",\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\t\t\t+ \"\\n\"\n",
    "\t\t)\n",
    "\n",
    "\n",
    "def score_function(\n",
    "\tmodel: NoveltyDetectionEnsemble,\n",
    "\ttrain: DataFrame,\n",
    "\ttest: Series,\n",
    "\ttesting_data: DataFrame,\n",
    ") -> dict[str, float | int | str | dict]:\n",
    "\t\"\"\"\n",
    "\tObjective function to maximize, calcs the F1 score on the test set.\n",
    "\tNow works with ensemble model.\n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel (NoveltyDetectionEnsemble): Ensemble model to eval\n",
    "\t\tTrain (DataFrame): train data, only for API compliance\n",
    "\t\ttest (Series): true targets (True for novel, False for normal)\n",
    "\t\ttesting_data (DataFrame): test feature data\n",
    "\n",
    "\tReturns:\n",
    "\t\tdict: Evaluation metrics\n",
    "\t\"\"\"\n",
    "\t# Get predictions and anomaly scores\n",
    "\tf1 = f1_score(test, where(model.predict(testing_data) == -1, True, False))\n",
    "\t# Get decision scores (higher values = more anomalous)\n",
    "\tanomaly_scores = model.decision_function(testing_data)\n",
    "\tprecision, recall, _ = precision_recall_curve(test, anomaly_scores)\n",
    "\tmetrics = {\n",
    "\t\t\"f1_score\": float(f1),\n",
    "\t\t\"avg_precision\": float(average_precision_score(test, anomaly_scores)),\n",
    "\t\t\"auc_pr\": float(auc(recall, precision)),\n",
    "\t\t\"auc_roc\": float(roc_auc_score(test, anomaly_scores)),\n",
    "\t\t\"params\": model.get_params(),\n",
    "\t\t\"datetime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "\t}\n",
    "\tlogger.info(f\"Evaluation metrics: {metrics}\")\n",
    "\treturn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalDataset:\n",
    "\t\"\"\"Manages data splits for incremental/continual learning experiments.\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tpath: Path,\n",
    "\t\tinitial_activities: int = 3,\n",
    "\t\ttest_size: int = 2,\n",
    "\t\tactivity_order: list[int] | None = None,\n",
    "\t\tordering_method: Literal[\"pca\", \"statistical\", \"variance\", \"frequency\"] = \"pca\",\n",
    "\t):\n",
    "\t\tself.data, self.labels = self.load_data(path)\n",
    "\t\tself.df = self.data.merge(self.labels, how=\"left\", on=\"id\")\n",
    "\n",
    "\t\tself.initial_activities = initial_activities\n",
    "\t\tself.test_size = test_size\n",
    "\n",
    "\t\tself.activity_order = (\n",
    "\t\t\tself.get_activity_order(self.data, self.labels, method=ordering_method)\n",
    "\t\t\tif activity_order is None\n",
    "\t\t\telse activity_order\n",
    "\t\t)\n",
    "\t\tself.n_activities = len(self.activity_order)\n",
    "\t\tself.n_folds = (self.n_activities - initial_activities) // test_size\n",
    "\n",
    "\tdef read_w_log(self, path: Path, filename: str) -> tuple[DataFrame, str]:\n",
    "\t\t\"\"\"Read and preprocess PAMAP2 data file.\"\"\"\n",
    "\t\tprint(f\"Reading: {filename}\", end=\"\\r\")\n",
    "\t\tdf = read_csv(os.path.join(path, filename), sep=r\"\\s+\", header=None)\n",
    "\t\tdf.columns = COLUMNS\n",
    "\t\treturn (\n",
    "\t\t\tdf.loc[:, ~df.columns.str.contains(r\"orient|acc6g\", regex=True)],\n",
    "\t\t\tfilename.split(\".\")[0][-2:],\n",
    "\t\t)\n",
    "\n",
    "\tdef handle_nans(self, df: DataFrame) -> DataFrame:\n",
    "\t\t\"\"\"Handles NaN values in the sensor data with a time-series-aware strategy.\"\"\"\n",
    "\t\tdf = df.copy()\n",
    "\t\tfor col in IMU_COLUMNS:\n",
    "\t\t\tdf.loc[:, col] = (\n",
    "\t\t\t\tdf[col]\n",
    "\t\t\t\t.ffill(limit=2)\n",
    "\t\t\t\t.interpolate(\"linear\", limit=5, limit_direction=\"both\")\n",
    "\t\t\t)\n",
    "\t\treturn df.dropna(subset=IMU_COLUMNS)\n",
    "\n",
    "\tdef normalize_features(\n",
    "\t\tself,\n",
    "\t\tX_train: DataFrame,\n",
    "\t\tX_val: DataFrame | None = None,\n",
    "\t\tX_test: DataFrame | None = None,\n",
    "\t\tscaler: RobustScaler | None = None,\n",
    "\t\tforce_refit: bool = False,\n",
    "\t) -> tuple[DataFrame, DataFrame | None, DataFrame | None, RobustScaler]:\n",
    "\t\t\"\"\"Normalizes IMU features using RobustScaler.\"\"\"\n",
    "\t\tif scaler is None or force_refit:\n",
    "\t\t\tscaler = RobustScaler().fit(X_train[IMU_COLUMNS])\n",
    "\n",
    "\t\tX_train = X_train.copy()\n",
    "\t\tX_train.loc[:, IMU_COLUMNS] = scaler.transform(X_train[IMU_COLUMNS])\n",
    "\n",
    "\t\tX_val_norm = None\n",
    "\t\tif X_val is not None:\n",
    "\t\t\tX_val_norm = X_val.copy()\n",
    "\t\t\tX_val_norm.loc[:, IMU_COLUMNS] = scaler.transform(X_val[IMU_COLUMNS])\n",
    "\n",
    "\t\tX_test_norm = None\n",
    "\t\tif X_test is not None:\n",
    "\t\t\tX_test_norm = X_test.copy()\n",
    "\t\t\tX_test_norm.loc[:, IMU_COLUMNS] = scaler.transform(X_test[IMU_COLUMNS])\n",
    "\n",
    "\t\treturn X_train, X_val_norm, X_test_norm, scaler\n",
    "\n",
    "\tdef load_data(self, base_path: Path) -> tuple[DataFrame, DataFrame]:\n",
    "\t\t\"\"\"Load and preprocess all PAMAP2 protocol data.\"\"\"\n",
    "\t\tlogger.info(\"Loading PAMAP2 data...\")\n",
    "\t\tDATA_DIR = Path(\"../data/PAMAP2/data.csv\")\n",
    "\t\tLABELS_DIR = Path(\"../data/PAMAP2/labels.csv\")\n",
    "\t\tif DATA_DIR.exists() and LABELS_DIR.exists():\n",
    "\t\t\tlogger.info(\"Found preprocessed data files. Loading...\")\n",
    "\t\t\treturn read_csv(DATA_DIR), read_csv(LABELS_DIR)\n",
    "\n",
    "\t\tdata, labels = [], []\n",
    "\t\tfor df, subject in [\n",
    "\t\t\tself.read_w_log(base_path, filename)\n",
    "\t\t\tfor filename in os.listdir(base_path)\n",
    "\t\t\tif filename.endswith(\".dat\")\n",
    "\t\t]:\n",
    "\t\t\tdf = self.handle_nans(df[~df[\"activity\"].isin([0, 24])])\n",
    "\t\t\tdf[\"subject\"] = str(subject)\n",
    "\t\t\tdf[\"timestamp\"] = to_datetime(df[\"timestamp\"], unit=\"s\").dt.time\n",
    "\t\t\tdf[\"id\"] = df[\"subject\"] + \"_\" + df[\"timestamp\"].astype(str)\n",
    "\n",
    "\t\t\tdata.append(\n",
    "\t\t\t\tdf.drop(columns=[\"timestamp\", \"heart_rate\", \"activity\", \"subject\"])\n",
    "\t\t\t)\n",
    "\t\t\tlabels.append(df[[\"id\", \"timestamp\", \"activity\", \"subject\"]])\n",
    "\n",
    "\t\tdata, labels = concat(data), concat(labels)\n",
    "\t\tdata[\"subject\"] = data[\"subject\"].astype(\"category\")\n",
    "\t\tlabels[\"activity\"] = labels[\"activity\"].astype(\"category\")\n",
    "\n",
    "\t\tdata.to_csv(DATA_DIR, index=False)\n",
    "\t\tlabels.to_csv(LABELS_DIR, index=False)\n",
    "\n",
    "\t\treturn data, labels\n",
    "\n",
    "\tdef get_activity_order(\n",
    "\t\tself,\n",
    "\t\tdata: DataFrame,\n",
    "\t\tlabels: DataFrame,\n",
    "\t\tmethod: Literal[\"pca\", \"statistical\", \"variance\", \"frequency\"] = \"pca\",\n",
    "\t) -> list[int]:\n",
    "\t\t\"\"\"Order activities by distinctiveness for incremental learning.\"\"\"\n",
    "\t\tdf = data.merge(labels, how=\"left\", on=\"id\")\n",
    "\n",
    "\t\tif method == \"frequency\":\n",
    "\t\t\treturn df[\"activity\"].value_counts().index.tolist()\n",
    "\n",
    "\t\tactivity_stats, activity_counts = {}, {}\n",
    "\t\tfor activity in df[\"activity\"].unique():\n",
    "\t\t\tactivity_data = df[df[\"activity\"] == activity][IMU_COLUMNS]\n",
    "\t\t\tactivity_counts[activity] = len(activity_data)\n",
    "\n",
    "\t\t\tif method == \"pca\":\n",
    "\t\t\t\tactivity_stats[activity] = (\n",
    "\t\t\t\t\tPCA(n_components=min(10, len(IMU_COLUMNS), len(activity_data)))\n",
    "\t\t\t\t\t.fit_transform(activity_data)\n",
    "\t\t\t\t\t.mean(axis=0)\n",
    "\t\t\t\t)\n",
    "\t\t\telif method == \"statistical\":\n",
    "\t\t\t\tactivity_stats[activity] = concatenate(\n",
    "\t\t\t\t\t[\n",
    "\t\t\t\t\t\tactivity_data.mean().values,\n",
    "\t\t\t\t\t\tactivity_data.std().values,\n",
    "\t\t\t\t\t\tactivity_data.quantile(0.25).values,\n",
    "\t\t\t\t\t\tactivity_data.quantile(0.75).values,\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t)\n",
    "\t\t\telif method == \"variance\":\n",
    "\t\t\t\tactivity_stats[activity] = concatenate(\n",
    "\t\t\t\t\t[\n",
    "\t\t\t\t\t\tactivity_data.var().values,\n",
    "\t\t\t\t\t\tactivity_data.abs().mean().values,\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t)\n",
    "\t\tactivities = list(activity_stats.keys())\n",
    "\t\tdistances = squareform(\n",
    "\t\t\tpdist(\n",
    "\t\t\t\tvstack([activity_stats[act] for act in activities]), metric=\"euclidean\"\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\tordered = []\n",
    "\t\tremaining = set(range(len(activities)))\n",
    "\t\tavg_distances = distances.mean(axis=1)\n",
    "\t\tfirst_idx = argmax(\n",
    "\t\t\t[\n",
    "\t\t\t\t-avg_distances[i] if i in remaining else float(\"-inf\")\n",
    "\t\t\t\tfor i in range(len(activities))\n",
    "\t\t\t]\n",
    "\t\t).astype(int)\n",
    "\t\tordered.append(first_idx)\n",
    "\t\tremaining.remove(first_idx)\n",
    "\n",
    "\t\twhile remaining:\n",
    "\t\t\tmin_dists = [\n",
    "\t\t\t\tmin([distances[idx, sel_idx] for sel_idx in ordered])\n",
    "\t\t\t\tif idx in remaining\n",
    "\t\t\t\telse float(\"inf\")\n",
    "\t\t\t\tfor idx in range(len(activities))\n",
    "\t\t\t]\n",
    "\t\t\tnext_idx = argmax(\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t-min_dists[i] if i in remaining else float(\"-inf\")\n",
    "\t\t\t\t\tfor i in range(len(activities))\n",
    "\t\t\t\t]\n",
    "\t\t\t).astype(int)\n",
    "\t\t\tordered.append(next_idx)\n",
    "\t\t\tremaining.remove(next_idx)\n",
    "\n",
    "\t\treturn [activities[i] for i in ordered]\n",
    "\n",
    "\tdef get_fold(\n",
    "\t\tself,\n",
    "\t\tfold_idx: int,\n",
    "\t\tval_split: float = 0.2,\n",
    "\t\tnormalize: bool = True,\n",
    "\t\tscaler: RobustScaler | None = None,\n",
    "\t\tpollution_rate: float = 0.3,  # 30% normal activities in val/test\n",
    "\t) -> tuple[\n",
    "\t\tDataFrame,\n",
    "\t\tDataFrame | None,\n",
    "\t\tDataFrame | None,\n",
    "\t\tDataFrame,\n",
    "\t\tDataFrame | None,\n",
    "\t\tDataFrame,\n",
    "\t\tRobustScaler | None,\n",
    "\t]:\n",
    "\t\t\"\"\"\n",
    "\t\tGet train/val/test split for a specific fold with polluted val/test sets.\n",
    "\n",
    "\t\tPollution means including some normal (train) activities in val/test sets\n",
    "\t\tto make evaluation more realistic.\n",
    "\t\t\"\"\"\n",
    "\t\tif fold_idx >= self.n_folds:\n",
    "\t\t\traise ValueError(f\"fold_idx {fold_idx} >= n_folds {self.n_folds}\")\n",
    "\n",
    "\t\tn_train_activities = self.initial_activities + fold_idx * self.test_size\n",
    "\t\ttrain_activities = self.activity_order[:n_train_activities]\n",
    "\t\ttest_activities = self.activity_order[\n",
    "\t\t\tn_train_activities : n_train_activities + self.test_size\n",
    "\t\t]\n",
    "\t\t# Get train/val data (normal activities)\n",
    "\t\ttrain_val_df = self.df[self.df[\"activity\"].isin(train_activities)].copy()\n",
    "\t\t# Get test data (novel activities)\n",
    "\t\tnovel_df = self.df[self.df[\"activity\"].isin(test_activities)].copy()\n",
    "\t\t# Split train activities into train/val\n",
    "\t\ttrain_data, val_normal_data = [], []\n",
    "\t\tfor activity in train_val_df[\"activity\"].unique():\n",
    "\t\t\tactivity_data = train_val_df[train_val_df[\"activity\"] == activity]\n",
    "\t\t\tsplit_idx = int(len(activity_data) * (1 - val_split))\n",
    "\t\t\ttrain_data.append(activity_data.iloc[:split_idx])\n",
    "\t\t\tval_normal_data.append(activity_data.iloc[split_idx:])\n",
    "\n",
    "\t\ttrain_df = concat(train_data).reset_index(drop=True)\n",
    "\t\t# Create polluted validation set: mix of normal and novel\n",
    "\t\tval_normal = concat(val_normal_data).reset_index(drop=True)\n",
    "\n",
    "\t\t# Sample novel activities for validation\n",
    "\t\tval_novel = novel_df.sample(\n",
    "\t\t\tn=min(\n",
    "\t\t\t\tint(len(val_normal) * (1 - pollution_rate) / pollution_rate),\n",
    "\t\t\t\tlen(novel_df) // 2,\n",
    "\t\t\t),\n",
    "\t\t\trandom_state=42,\n",
    "\t\t).reset_index(drop=True)\n",
    "\n",
    "\t\tval_df = (\n",
    "\t\t\tconcat([val_normal, val_novel])\n",
    "\t\t\t.sample(frac=1, random_state=42)\n",
    "\t\t\t.reset_index(drop=True)\n",
    "\t\t)\n",
    "\t\t# polluted test set: mix of normal and novel usinfg remaining novel\n",
    "\t\t# samples not used in validation\n",
    "\t\ttest_novel = (\n",
    "\t\t\tnovel_df[~novel_df.index.isin(val_novel.index)].reset_index(drop=True)\n",
    "\t\t\tif val_df is not None\n",
    "\t\t\telse novel_df\n",
    "\t\t)\n",
    "\t\t# Sample normal activities for test (from train set, not val)\n",
    "\t\ttest_normal = train_df.sample(\n",
    "\t\t\tn=min(\n",
    "\t\t\t\tint(len(test_novel) * pollution_rate / (1 - pollution_rate)),\n",
    "\t\t\t\tlen(train_df) // 10,\n",
    "\t\t\t),\n",
    "\t\t\trandom_state=42,\n",
    "\t\t).reset_index(drop=True)\n",
    "\t\ttest_df = (\n",
    "\t\t\tconcat([test_normal, test_novel])\n",
    "\t\t\t.sample(frac=1, random_state=42)\n",
    "\t\t\t.reset_index(drop=True)\n",
    "\t\t)\n",
    "\t\tfeature_cols = [col for col in self.data.columns if col != \"id\"]\n",
    "\n",
    "\t\tX_train = train_df[feature_cols].reset_index(drop=True)\n",
    "\t\tX_val = (\n",
    "\t\t\tval_df[feature_cols].reset_index(drop=True) if val_df is not None else None\n",
    "\t\t)\n",
    "\t\tX_test = test_df[feature_cols].reset_index(drop=True)\n",
    "\n",
    "\t\ty_train = train_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\t\ty_train[\"isNovelty\"] = False  # all normal\n",
    "\n",
    "\t\tif val_df is not None:\n",
    "\t\t\ty_val = val_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\t\t\t# Mark as novel if not in train activities\n",
    "\t\t\ty_val[\"isNovelty\"] = ~y_val[\"activity\"].isin(train_activities)\n",
    "\t\telse:\n",
    "\t\t\ty_val = None\n",
    "\n",
    "\t\ty_test = test_df[[\"id\", \"activity\"]].reset_index(drop=True)\n",
    "\t\t# Mark as novel if not in train activities\n",
    "\t\ty_test[\"isNovelty\"] = ~y_test[\"activity\"].isin(train_activities)\n",
    "\n",
    "\t\tif normalize:\n",
    "\t\t\tX_train, X_val, X_test, scaler = self.normalize_features(\n",
    "\t\t\t\tX_train, X_val, X_test, scaler=scaler\n",
    "\t\t\t)\n",
    "\n",
    "\t\tif y_val is not None:\n",
    "\t\t\tlogger.info(\n",
    "\t\t\t\tf\"Fold {fold_idx} - Val set: {(~y_val['isNovelty']).sum()} normal, \"\n",
    "\t\t\t\tf\"{y_val['isNovelty'].sum()} novel ({y_val['isNovelty'].mean():.2%} novel)\"\n",
    "\t\t\t)\n",
    "\t\tlogger.info(\n",
    "\t\t\tf\"Fold {fold_idx} - Test set: {(~y_test['isNovelty']).sum()} normal, \"\n",
    "\t\t\tf\"{y_test['isNovelty'].sum()} novel ({y_test['isNovelty'].mean():.2%} novel)\"\n",
    "\t\t)\n",
    "\t\treturn X_train, X_val, X_test, y_train, y_val, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_space(\n",
    "\tuse_log_dist: bool = False,\n",
    ") -> dict[str, list[int | float | tuple[float]]]:\n",
    "\t\"\"\"\n",
    "\tGet hyperparameter search space for the ensemble model.\n",
    "\n",
    "\tArgs:\n",
    "\t\tuse_log_dist: If to log-uniform distributions for parameters that span orders of magnitude\n",
    "\n",
    "\tReturns:\n",
    "\t\tDictionary with parameter distributions\n",
    "\t\"\"\"\n",
    "\treturn {\n",
    "\t\t# LSTM-AE params\n",
    "\t\t\"sequence_length\": [50, 100, 150, 200],\n",
    "\t\t\"hidden_size\": [32, 64, 128, 256],\n",
    "\t\t\"n_layers\": [1, 2, 3],\n",
    "\t\t\"dropout\": uniform(0.0, 0.5),\n",
    "\t\t\"epochs\": [20, 30, 50, 75, 100],\n",
    "\t\t\"batch_size\": [32, 64, 128, 256],\n",
    "\t\t\"learning_rate\": loguniform(1e-4, 1e-2)\n",
    "\t\tif use_log_dist\n",
    "\t\telse uniform(1e-4, 1e-2),\n",
    "\t\t# Isolation Forest params\n",
    "\t\t\"n_estimators\": [100, 200, 300, 500],\n",
    "\t\t\"max_samples\": [128, 256, 512, 1024],\n",
    "\t\t\"contamination\": loguniform(0.001, 0.1)\n",
    "\t\tif use_log_dist\n",
    "\t\telse uniform(0.001, 0.1),\n",
    "\t\t# OC-SVM params\n",
    "\t\t\"nu\": loguniform(0.01, 0.2) if use_log_dist else uniform(0.01, 0.2),\n",
    "\t\t\"gamma\": loguniform(1e-4, 1.0) if use_log_dist else uniform(1e-4, 1.0),\n",
    "\t\t# Ensemble weights - using uniform for weights\n",
    "\t\t# \"weights\": [\n",
    "\t\t# \t(0.5, 0.3, 0.2),\n",
    "\t\t# \t(0.4, 0.3, 0.3),\n",
    "\t\t# \t(0.3, 0.4, 0.3),\n",
    "\t\t# \t(0.3, 0.3, 0.4),\n",
    "\t\t# \t(0.33, 0.33, 0.34),\n",
    "\t\t# ],\n",
    "\t\t\"weights\": [\n",
    "\t\t\t(0.3, 0.2),\n",
    "\t\t\t(0.3, 0.3),\n",
    "\t\t\t(0.4, 0.3),\n",
    "\t\t\t(0.3, 0.4),\n",
    "\t\t\t(0.33, 0.34),\n",
    "\t\t],\n",
    "\t}  # type: ignore\n",
    "\n",
    "\n",
    "def get_search_method(\n",
    "\tsearch_name: Literal[\"random_search\", \"random_search_log\"],\n",
    "\tn_iter: int = 10,\n",
    "\trandom_state: int = 42,\n",
    ") -> ParameterSampler:\n",
    "\t\"\"\"\n",
    "\tGet the appropriate search method with corresponding parameter space.\n",
    "\n",
    "\tArgs:\n",
    "\t\tsearch_name: Name of the search method to use\n",
    "\t\tn_iter: Number of iterations for the search\n",
    "\t\trandom_state: Random state for reproducibility\n",
    "\n",
    "\tReturns:\n",
    "\t\tIterator over parameter dictionaries (ParameterSampler)\n",
    "\t\"\"\"\n",
    "\tparam_distributions = get_search_space(use_log_dist=\"log\" in search_name.lower())\n",
    "\n",
    "\tif search_name in [\"random_search\", \"random_search_log\"]:\n",
    "\t\treturn ParameterSampler(  # This is an iterator over parameter settings\n",
    "\t\t\tparam_distributions, n_iter=n_iter, random_state=random_state\n",
    "\t\t)\n",
    "\traise ValueError(f\"Unknown search method: {search_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_fold(\n",
    "\tdataset: IncrementalDataset,\n",
    "\tfold_idx: int,\n",
    "\tmodel_params: dict[str, float | int | tuple[float, ...]],\n",
    ") -> dict:\n",
    "\t\"\"\"\n",
    "\tTrain and evaluate the ensemble model on a specific fold.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdataset: IncrementalDataset instance\n",
    "\t\tfold_idx: Fold index to train/evaluate\n",
    "\t\tmodel_params: Optional model hyperparameters (used if hpo_method=\"none\")\n",
    "\n",
    "\tReturns:\n",
    "\t\tDictionary with evaluation metrics from validation set\n",
    "\t\"\"\"\n",
    "\tX_train, X_val, X_test, y_train, y_val, y_test, scaler = dataset.get_fold(\n",
    "\t\tfold_idx, val_split=0.2, normalize=True\n",
    "\t)\n",
    "\tassert X_train is not None, \"Train set can't be None\"\n",
    "\tassert X_val is not None and y_val is not None, \"val set & labels can't be None\"\n",
    "\tassert X_test is not None, \"Test set can't be None\"\n",
    "\n",
    "\tmodel_params[\"n_features\"] = len(IMU_COLUMNS)  # sanity check\n",
    "\n",
    "\tlogger.info(f\"Training model on fold {fold_idx}...\")\n",
    "\tmodel = NoveltyDetectionEnsemble(**model_params).fit(X_train)  # type: ignore\n",
    "\n",
    "\tmetrics = score_function(model, X_train, y_val[\"isNovelty\"], X_val)\n",
    "\tmetrics[\"split\"] = \"validation\"\n",
    "\tmetrics[\"fold\"] = fold_idx\n",
    "\n",
    "\tlogger.info(f\"Fold {fold_idx} - Validation F1: {metrics['f1_score']:.4f}\")\n",
    "\tprint(\n",
    "\t\tclassification_report(\n",
    "\t\t\ty_val[\"isNovelty\"],\n",
    "\t\t\twhere(model.predict(X_test) == -1, True, False),\n",
    "\t\t\ttarget_names=[\"Normal\", \"Novel\"],\n",
    "\t\t)\n",
    "\t)\n",
    "\treturn metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hpo_search(\n",
    "\tdataset: IncrementalDataset,\n",
    "\thpo_method: Literal[\"random_search\", \"random_search_log\"],\n",
    "\tn_iter: int = 10,\n",
    "\toutput_dir: Path = Path(\"../results\"),\n",
    ") -> dict[int, dict[str, float | dict]]:\n",
    "\t\"\"\"\n",
    "\tRun HPO search across all folds for a given method.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdataset: IncrementalDataset instance\n",
    "\t\thpo_method: HPO method to use\n",
    "\t\tn_iter: Number of iterations per fold\n",
    "\t\toutput_dir: Directory to save results\n",
    "\n",
    "\tReturns:\n",
    "\t\tDictionary with best results per fold\n",
    "\t\"\"\"\n",
    "\tif (log_file := output_dir / f\"{hpo_method}.log\").exists():\n",
    "\t\tlog_file.unlink()\n",
    "\n",
    "\tbest_results_per_fold: dict[int, dict[str, float | dict]] = {}\n",
    "\n",
    "\tfor fold_idx in range(dataset.n_folds):\n",
    "\t\tparam_sampler = get_search_method(\n",
    "\t\t\tsearch_name=hpo_method, n_iter=n_iter, random_state=42 + fold_idx\n",
    "\t\t)\n",
    "\t\tbest_f1: float = -1.0\n",
    "\t\tbest_params: dict | None = None\n",
    "\t\tbest_metrics: dict | None = None\n",
    "\t\t# Run HPO iterations for this fold\n",
    "\t\tfor iteration, params in enumerate(param_sampler):\n",
    "\t\t\tlogger.info(f\"Fold {fold_idx} - Iteration {iteration + 1}/{n_iter}\")\n",
    "\t\t\ttry:\n",
    "\t\t\t\tmetrics = train_and_evaluate_fold(\n",
    "\t\t\t\t\tdataset=dataset, fold_idx=fold_idx, model_params=params\n",
    "\t\t\t\t)\n",
    "\t\t\t\tmetrics[\"iteration\"] = iteration + 1\n",
    "\t\t\t\tlog_iteration_results(\n",
    "\t\t\t\t\tlog_file=log_file,\n",
    "\t\t\t\t\titeration=iteration + 1,\n",
    "\t\t\t\t\tmetrics=metrics,\n",
    "\t\t\t\t\thpo_method=hpo_method,\n",
    "\t\t\t\t)\n",
    "\t\t\t\tif metrics[\"f1_score\"] > best_f1:\n",
    "\t\t\t\t\tbest_f1 = metrics[\"f1_score\"]\n",
    "\t\t\t\t\tbest_params = params.copy()\n",
    "\t\t\t\t\tbest_metrics = metrics.copy()\n",
    "\n",
    "\t\t\t\t\tlogger.info(f\"New best for fold {fold_idx}! F1: {best_f1:.4f}\")\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tlogger.error(f\"Error in fold {fold_idx}: {str(e)}\")\n",
    "\t\t\t\traise e\n",
    "\n",
    "\t\tif best_metrics is not None and best_params is not None:\n",
    "\t\t\tbest_results_per_fold[fold_idx] = {\n",
    "\t\t\t\t\"best_f1\": best_f1,\n",
    "\t\t\t\t\"best_params\": best_params,\n",
    "\t\t\t\t\"best_metrics\": best_metrics,\n",
    "\t\t\t}\n",
    "\t\t\tlogger.info(f\"\\nFold {fold_idx} complete - Best F1: {best_f1:.4f}\")\n",
    "\n",
    "\twith open(output_dir / f\"{hpo_method}_summary.json\", \"w\") as f:\n",
    "\t\tjson.dump(best_results_per_fold, f, indent=2, default=str)\n",
    "\n",
    "\treturn best_results_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:07:24 [info     ] Loading PAMAP2 data...        \n",
      "2025-11-16 12:07:24 [info     ] Found preprocessed data files. Loading...\n",
      "2025-11-16 12:07:31 [info     ] Fold 0 - Iteration 1/10       \n",
      "2025-11-16 12:07:32 [info     ] Fold 0 - Val set: 103792 normal, 144022 novel (58.12% novel)\n",
      "2025-11-16 12:07:32 [info     ] Fold 0 - Test set: 41516 normal, 266340 novel (86.51% novel)\n",
      "2025-11-16 12:07:33 [info     ] Training model on fold 0...   \n",
      "2025-11-16 12:07:33 [info     ] Training ensemble models...   \n",
      "2025-11-16 12:07:33 [info     ] Training IsolationForest...   \n",
      "2025-11-16 12:07:39 [info     ] Training OC-SVM...            \n"
     ]
    }
   ],
   "source": [
    "dataset = IncrementalDataset(\n",
    "\tPath(\"../data/PAMAP2_Dataset/Protocol/\"),\n",
    "\tinitial_activities=3,\n",
    "\ttest_size=2,\n",
    "\tordering_method=\"pca\",\n",
    ")\n",
    "results_random = run_hpo_search(\n",
    "\tdataset=dataset,\n",
    "\thpo_method=\"random_search_log\",\n",
    "\tn_iter=10,\n",
    "\toutput_dir=Path(\"../reports\"),\n",
    ")\n",
    "results_random_log = run_hpo_search(\n",
    "\tdataset=dataset,\n",
    "\thpo_method=\"random_search\",\n",
    "\tn_iter=10,\n",
    "\toutput_dir=Path(\"../reports\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
