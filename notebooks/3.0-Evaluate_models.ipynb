{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n",
    "from json import dump as json_dump\n",
    "from json import load as json_load\n",
    "from json import loads\n",
    "from os import environ, makedirs\n",
    "from os.path import isfile, join\n",
    "from random import sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from numpy import ndarray, where\n",
    "from pandas import DataFrame, concat, json_normalize, read_csv, to_datetime\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "json_pattern = re.compile(r\"\\[INFO\\] (.+)$\")\n",
    "log_pattern = re.compile(r\"({.*})\")\n",
    "\n",
    "MODELS_UPDATED = environ[\"MODELS_UPDATED\"] == \"true\"\n",
    "TECHNIQUES = [\"Grid Search\", \"Random Search\", \"Bayesian Opt\"]\n",
    "POLLUTIONS = [0.2, 0.4, 0.75, 0.9]\n",
    "RESULTS: dict[str, dict[float, dict[str, float]]] = {\n",
    "    technique: {} for technique in TECHNIQUES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = read_csv(\"../data/PAMAP2/x_train_data.csv\")\n",
    "X_test = read_csv(\"../data/PAMAP2/x_test_data.csv\")\n",
    "y_train = read_csv(\"../data/PAMAP2/y_train_data.csv\")\n",
    "y_test = read_csv(\"../data/PAMAP2/y_test_data.csv\")\n",
    "\n",
    "x_label = [f\"{num[0]}\" for num in y_test.value_counts().index.sort_values().to_list()][\n",
    "    1:\n",
    "]\n",
    "\n",
    "X_train[\"activity\"] = y_train  # First 80% of the data\n",
    "X_test[\"activity\"] = y_test  # Last 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_result(file_name: str) -> DataFrame:\n",
    "    records = []\n",
    "    with open(f\"../reports/{file_name}.log\", \"r\") as file:\n",
    "        for line in file:\n",
    "            if match := log_pattern.search(line):\n",
    "                raw_data = match.group(1).strip()\n",
    "                data = (\n",
    "                    literal_eval(raw_data)\n",
    "                    if raw_data.startswith(\"{'\")\n",
    "                    or raw_data.startswith('{\"')\n",
    "                    and \"'\" in raw_data\n",
    "                    else loads(raw_data)\n",
    "                )\n",
    "                # Extract datetime from JSON and normalize datetime to 'YYYY-MM-DD HH:MM:SS'\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"target\": data[\"target\"],\n",
    "                        \"datetime\": datetime.strptime(\n",
    "                            data[\"datetime\"][:19]\n",
    "                            if isinstance(data[\"datetime\"], str)\n",
    "                            else data[\"datetime\"][\"datetime\"],\n",
    "                            \"%Y-%m-%d %H:%M:%S\",\n",
    "                        ).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"gamma\": data[\"params\"][\"gamma\"],\n",
    "                        \"nu\": data[\"params\"][\"nu\"],\n",
    "                        \"tol\": data[\"params\"][\"tol\"],\n",
    "                    }\n",
    "                )\n",
    "    return json_normalize(records)\n",
    "\n",
    "\n",
    "def normalize_result(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.groupby(df.index // 4)\n",
    "        .agg(\n",
    "            {\n",
    "                \"target\": \"mean\",\n",
    "                \"datetime\": \"first\",\n",
    "                \"gamma\": \"first\",\n",
    "                \"nu\": \"first\",\n",
    "                \"tol\": \"first\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_datetime_part(dt_str):\n",
    "    # This will capture the first 'YYYY-MM-DD HH:MM' part\n",
    "    match = re.match(r\"^(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2})\", dt_str)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def split_dataframe(df, chunk_sizes):\n",
    "    assert sum(chunk_sizes) <= len(df), \"Chunk sizes exceed DataFrame length\"\n",
    "\n",
    "    grid_chunks = []\n",
    "    start = 0\n",
    "    for size in chunk_sizes:\n",
    "        end = start + size\n",
    "        grid_chunks.append(df.iloc[start:end].reset_index(drop=True))\n",
    "        start = end\n",
    "\n",
    "    return grid_chunks\n",
    "\n",
    "\n",
    "def pick_best_target(chunks):\n",
    "    best_targets = []\n",
    "    labels = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        best_row = chunk.loc[chunk[\"target\"].idxmax()]\n",
    "        best_targets.append(best_row[\"target\"])\n",
    "        labels.append(\n",
    "            f\"γ={best_row['gamma']}\\nν={best_row['nu']}\\ntol={best_row['tol']}\"\n",
    "        )\n",
    "    return best_targets, labels\n",
    "\n",
    "\n",
    "def get_top_params(df):\n",
    "    return {\n",
    "        key: val[0]\n",
    "        for key, val in df.sort_values(\"target\", ascending=False)\n",
    "        .head(1)[[\"gamma\", \"nu\", \"tol\"]]\n",
    "        .to_dict(\"list\")\n",
    "        .items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid = normalize_result(extract_result(\"logs_grid\"))\n",
    "df_grid[\"datetime\"] = to_datetime(\n",
    "    df_grid[\"datetime\"].apply(extract_datetime_part), format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "print(\n",
    "    f\"Total Time Grid Search: {df_grid['datetime'].max() - df_grid['datetime'].min()}\"\n",
    ")\n",
    "grid_chunks = split_dataframe(df_grid, [100] + [30] * 10)\n",
    "\n",
    "df_rand = normalize_result(extract_result(\"logs_rand\"))\n",
    "df_rand[\"datetime\"] = to_datetime(\n",
    "    df_rand[\"datetime\"].apply(extract_datetime_part), format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "print(\n",
    "    f\"Total Time Random Search: {df_rand['datetime'].max() - df_rand['datetime'].min()}\"\n",
    ")\n",
    "rand_chunks = split_dataframe(df_rand, [100] + [20] * 10)\n",
    "\n",
    "df_bayes = extract_result(\"logs_bayesian\")\n",
    "df_bayes[\"datetime\"] = to_datetime(\n",
    "    df_bayes[\"datetime\"].apply(extract_datetime_part), format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "print(\n",
    "    f\"Total Time Bayesian Search: {df_bayes['datetime'].max() - df_bayes['datetime'].min()}\"\n",
    ")\n",
    "bayes_chunks = split_dataframe(df_bayes, [100] + [5] * 10)\n",
    "\n",
    "with open(\"../conf/top_results.json\", \"w\") as f:\n",
    "    json_dump(\n",
    "        {\n",
    "            \"Grid Search\": get_top_params(df_grid),\n",
    "            \"Random Search\": get_top_params(df_rand),\n",
    "            \"Bayesian Opt\": get_top_params(df_bayes),\n",
    "        },\n",
    "        f,\n",
    "        indent=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_targets_grid, labels = pick_best_target(grid_chunks)\n",
    "best_targets_rand, _ = pick_best_target(rand_chunks)\n",
    "best_targets_bayes, _ = pick_best_target(bayes_chunks)\n",
    "\n",
    "methods = [\n",
    "    (\"Grid Search\", best_targets_grid, \"b\", labels, (0, 10), \"black\"),\n",
    "    (\"Random Search\", best_targets_rand, \"orange\", labels, (0, -15), \"orange\"),\n",
    "    (\"Bayesian Opt.\", best_targets_bayes, \"green\", labels, (0, -15), \"green\"),\n",
    "]\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for name, targets, color, lbls, offset, ann_color in methods:\n",
    "    plt.plot(x_label, targets, marker=\"o\", linestyle=\"-\", color=color, label=name)\n",
    "    for xi, yi, lbl in zip(x_label, targets, lbls):\n",
    "        plt.annotate(\n",
    "            lbl,\n",
    "            (xi, yi),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=offset,\n",
    "            ha=\"center\",\n",
    "            fontsize=9,\n",
    "            color=ann_color,\n",
    "        )\n",
    "\n",
    "plt.title(\"Comparação dos Resultados\")\n",
    "plt.xlabel(\"Classe de Teste\")\n",
    "plt.ylabel(\"Melhor Valor F1\")\n",
    "plt.xticks(x_label)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_train_test(data: DataFrame) -> tuple[list, DataFrame, DataFrame]:\n",
    "    classes: list[int] = [\n",
    "        num for num in sample(X_train[\"activity\"].value_counts().index.to_list(), 6)\n",
    "    ]\n",
    "    classes.sort()\n",
    "    mask = data[\"activity\"].isin(classes)\n",
    "    return (classes, data[mask], data[~mask])\n",
    "\n",
    "\n",
    "def test_ocsvm_with_pollution(\n",
    "    train: DataFrame, test: DataFrame, model: OneClassSVM, percent: float\n",
    ") -> dict[str, float]:\n",
    "    novelty = concat([test, train.sample(n=int(percent * len(train)), random_state=42)])\n",
    "    preds = where(model.predict(novelty.drop(columns=[\"isNovelty\"])) == -1, True, False)\n",
    "    precision, recall, _ = precision_recall_curve(novelty[\"isNovelty\"], preds)\n",
    "\n",
    "    return {\n",
    "        \"f1\": float(f1_score(novelty[\"isNovelty\"], preds)),\n",
    "        \"mcc\": float(matthews_corrcoef(novelty[\"isNovelty\"], preds)),\n",
    "        \"acc\": float(accuracy_score(novelty[\"isNovelty\"], preds)),\n",
    "        \"pr_auc\": float(auc(recall, precision)),\n",
    "        \"roc_auc\": float(roc_auc_score(novelty[\"isNovelty\"], preds)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, training, testing = filter_train_test(concat([X_train, X_test]))\n",
    "training.loc[:, \"isNovelty\"], testing.loc[:, \"isNovelty\"] = False, True\n",
    "\n",
    "print(f\"Training with: {classes}\")\n",
    "with open(\"../conf/top_results.json\", \"r\") as f:\n",
    "    top_results = json_load(f)\n",
    "\n",
    "# Extract training data once as numpy array (lighter to share)\n",
    "X_training = training.drop(columns=[\"isNovelty\"]).values\n",
    "\n",
    "\n",
    "def create_ocsvm(params: dict, technique: str, train_data: ndarray = X_training):\n",
    "    print(f\"fitting model for {technique}\")\n",
    "    return OneClassSVM(\n",
    "        kernel=\"rbf\", gamma=params[\"gamma\"], tol=params[\"tol\"], nu=params[\"nu\"]\n",
    "    ).fit(train_data)\n",
    "\n",
    "\n",
    "def load_or_train_models(techniques, top_results, X_training, model_dir=\"./models\"):\n",
    "    \"\"\"Load models from disk if available, otherwise train and save them.\"\"\"\n",
    "    makedirs(model_dir, exist_ok=True)\n",
    "    model_paths = {\n",
    "        technique: join(model_dir, f\"{technique.replace(' ', '_')}.joblib\")\n",
    "        for technique in TECHNIQUES\n",
    "    }\n",
    "    models = {}\n",
    "\n",
    "    if all(isfile(path) for path in model_paths.values()) and MODELS_UPDATED:\n",
    "        print(\"Loading models from disk...\")\n",
    "        for technique, path in model_paths.items():\n",
    "            models[technique] = load(path)\n",
    "            print(f\"Loaded model for {technique} from {path}\")\n",
    "    else:\n",
    "        print(\"Training models...\")\n",
    "        models = {\n",
    "            technique: result\n",
    "            for technique, result in zip(\n",
    "                TECHNIQUES,\n",
    "                Parallel(n_jobs=-1)(\n",
    "                    delayed(create_ocsvm)(top_results[technique], technique, X_training)\n",
    "                    for technique in TECHNIQUES\n",
    "                ),\n",
    "            )\n",
    "        }\n",
    "        for technique, model in models.items():\n",
    "            dump(model, f\"./models/{technique.replace(' ', '_')}.joblib\")  # type: ignore\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_or_train_models(TECHNIQUES, top_results, X_training)\n",
    "results_list = Parallel(n_jobs=-1)(\n",
    "    delayed(test_ocsvm_with_pollution)(training, testing, model, pollution)\n",
    "    for pollution in POLLUTIONS\n",
    "    for _, model in models.items()\n",
    ")\n",
    "\n",
    "# Assign results back to RESULTS\n",
    "idx = 0\n",
    "for pollution in POLLUTIONS:\n",
    "    print(f\"Testing with {pollution * 100}% pollution\")\n",
    "    for name in models.keys():\n",
    "        result = {pollution: results_list[idx]}\n",
    "        RESULTS[name].update(result)\n",
    "        idx += 1\n",
    "\n",
    "with open(\"../conf/result_polution.json\", \"w\") as fp:\n",
    "    json_dump(RESULTS, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technique_translation = {\n",
    "    \"Grid Search\": \"Busca em grade\",\n",
    "    \"Random Search\": \"Busca Aleatória\",\n",
    "    \"Bayesian Opt\": \"Otimização Bayesiana\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 8), sharey=True)\n",
    "\n",
    "for i, (technique_name, df) in enumerate(\n",
    "    (\n",
    "        (name, DataFrame(RESULTS[name]).transpose())\n",
    "        for name in [\"Grid Search\", \"Random Search\", \"Bayesian Opt\"]\n",
    "    )\n",
    "):\n",
    "    for metric in df.columns.tolist():\n",
    "        axes[i].plot(df.index.tolist(), df[metric], marker=\"o\", label=metric)\n",
    "\n",
    "    axes[i].set_ylabel(\"Resultados\" if i == 0 else \"\")\n",
    "    axes[i].set_xlabel(\"% de novidade\")\n",
    "    axes[i].set_title(technique_translation[technique_name])\n",
    "    axes[i].tick_params(axis=\"x\", rotation=0)\n",
    "    # 'upper right', 'upper left', 'lower left', 'lower right', 'right', 'center left', 'center right', 'lower center', 'upper center', 'center'\n",
    "    if i == 0:\n",
    "        axes[i].legend(loc=\"lower left\", fontsize=18)\n",
    "\n",
    "fig.tight_layout(rect=(0.0, 0.03, 1.0, 0.95))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
