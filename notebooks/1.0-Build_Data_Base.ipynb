{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Final, Literal\n",
    "\n",
    "from numpy import argmax, concatenate, unravel_index, vstack\n",
    "from pandas import DataFrame, concat, read_csv, set_option, to_datetime\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "set_option(\"display.max_columns\", None)\n",
    "\n",
    "COLUMNS: Final[list[str]] = [\n",
    "\t\"timestamp\",\n",
    "\t\"activity\",\n",
    "\t\"heart_rate\",\n",
    "\t*[\n",
    "\t\tf\"IMU_{body_part}_{suffix}\"\n",
    "\t\tfor body_part in [\"hand\", \"chest\", \"ankle\"]\n",
    "\t\tfor suffix in [\n",
    "\t\t\t\"temp_C\",\n",
    "\t\t\t*[\n",
    "\t\t\t\tf\"{scalar}_{axis}\"\n",
    "\t\t\t\tfor scalar in [\"acc16g_ms^-2\", \"acc6g_ms^-2\", \"gyro_rad/s\", \"mag_ŒºT\"]\n",
    "\t\t\t\tfor axis in [\"x\", \"y\", \"z\"]\n",
    "\t\t\t],\n",
    "\t\t\t*[f\"orient_{x}\" for x in range(1, 5)],\n",
    "\t\t]\n",
    "\t],\n",
    "]\n",
    "IMU_COLUMNS: Final[list[str]] = [\n",
    "\tcol\n",
    "\tfor col in COLUMNS\n",
    "\tif col.startswith(\"IMU_\") and \"acc6g_ms^-2\" not in col and \"orient\" not in col\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_w_log(path: Path, filename: str) -> tuple[DataFrame, str]:\n",
    "\t\"\"\"\n",
    "\tThe IMU sensory data contains the following columns:\n",
    "\t- 1 temperature (¬∞C)\n",
    "\t- 2...4 3D-acceleration data (ms^-2), scale: ¬±16g, resolution: 13-bit\n",
    "\t- 5...7 3D-acceleration data (ms^-2), scale: ¬±6g, resolution: 13-bit*\n",
    "\t- 8...10 3D-gyroscope data (rad/s)\n",
    "\t- 11...13 3D-magnetometer data (ŒºT)\n",
    "\t- 14...17 orientation (invalid in this data collection)\n",
    "\n",
    "\t* This accelerometer is not precisely calibrated with the first one. Moreover, due\n",
    "\tto high impacts caused by certain movements (e.g. during running) with acceleration\n",
    "\tover 6g, it gets saturated sometimes. Therefore, the use of the data from the first\n",
    "\taccelerometer (with the scale of ¬±16g) is recommended.\n",
    "\n",
    "\tArgs:\n",
    "\t\tpath: Directory path containing the data file.\n",
    "\t\tfilename: Name of the file to read.\n",
    "\n",
    "\tReturns:\n",
    "\t\tTuple containing the cleaned DataFrame and subject ID (last 2 chars of filename).\n",
    "\t\"\"\"\n",
    "\tprint(f\"Reading: {filename}\", end=\"\\r\")\n",
    "\tdf = read_csv(os.path.join(path, filename), sep=r\"\\s+\", header=None)\n",
    "\tdf.columns = COLUMNS\n",
    "\treturn (\n",
    "\t\tdf.loc[\n",
    "\t\t\t:,\n",
    "\t\t\t~df.columns.str.contains(\"orient\") & ~df.columns.str.contains(\"acc6g\"),\n",
    "\t\t],\n",
    "\t\tfilename.split(\".\")[0][-2:],\n",
    "\t)\n",
    "\n",
    "\n",
    "def handle_nans(df: DataFrame) -> DataFrame:\n",
    "\t\"\"\"\n",
    "\tHandles NaN values in the sensor data with a time-series-aware strategy.\n",
    "\n",
    "\t- First, forward-fills to propagate the last valid observation.\n",
    "\t- Then, uses linear interpolation for short gaps.\n",
    "\t- Finally, drops any rows where sensor data is still missing.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdf: The input DataFrame with potential NaN values.\n",
    "\n",
    "\tReturns:\n",
    "\t\tDataFrame with NaNs handled.\n",
    "\t\"\"\"\n",
    "\tdf = df.copy()\n",
    "\t# For IMU data: linear interpolation for short gaps, drop for long gaps\n",
    "\tfor col in IMU_COLUMNS:\n",
    "\t\t# Forward fill first (sensor readings typically persist briefly)\n",
    "\t\tdf.loc[:, col] = df[col].ffill(limit=2)\n",
    "\t\t# Only interpolate if gap is ‚â§ 5 samples (0.05s at 100Hz)\n",
    "\t\t# IMU gaps can be interpolated without significant information loss.\n",
    "\t\tdf.loc[:, col] = df[col].interpolate(\"linear\", limit=5, limit_direction=\"both\")\n",
    "\t# Drop rows where ANY IMU sensor still has NaN (likely sensor disconnection)\n",
    "\treturn df.dropna(subset=IMU_COLUMNS)\n",
    "\n",
    "\n",
    "def normalize_features(\n",
    "\tX_train: DataFrame, X_val: DataFrame, X_test: DataFrame, force_refit: bool = False\n",
    ") -> tuple[DataFrame, DataFrame, DataFrame]:\n",
    "\t\"\"\"Normalizes IMU features using RobustScaler fitted on training data only.\n",
    "\n",
    "\tArgs:\n",
    "\t\tX_train: Training feature DataFrame.\n",
    "\t\tX_val: Validation feature DataFrame.\n",
    "\t\tX_test: Test feature DataFrame.\n",
    "\t\tforce_refit: If to ignores existing scaler and refits. Defaults to False.\n",
    "\n",
    "\tReturns:\n",
    "\t\t(normalized X_train, normalized X_val, normalized X_test, fitted scaler).\n",
    "\n",
    "\tRaises:\n",
    "\t\tFileNotFoundError: If scaler_path directory doesn't exist when trying to save.\n",
    "\t\tpickle.UnpicklingError: If saved scaler file is corrupted.\n",
    "\t\"\"\"\n",
    "\tscaler_path = Path(\"../data/PAMAP2/splits/robust_scaler.joblib\")\n",
    "\n",
    "\tif scaler_path and scaler_path.exists() and not force_refit:\n",
    "\t\twith open(scaler_path, \"rb\") as f:\n",
    "\t\t\tscaler = pickle.load(f)\n",
    "\telse:\n",
    "\t\tscaler = RobustScaler().fit(X_train[IMU_COLUMNS])\n",
    "\t\tif scaler_path:\n",
    "\t\t\tscaler_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\t\t\twith open(scaler_path, \"wb\") as f:\n",
    "\t\t\t\tpickle.dump(scaler, f)\n",
    "\n",
    "\tX_train = X_train.copy()\n",
    "\tX_val = X_val.copy()\n",
    "\tX_test = X_test.copy()\n",
    "\n",
    "\tX_train.loc[:, IMU_COLUMNS] = scaler.transform(X_train[IMU_COLUMNS])\n",
    "\tX_val.loc[:, IMU_COLUMNS] = scaler.transform(X_val[IMU_COLUMNS])\n",
    "\tX_test.loc[:, IMU_COLUMNS] = scaler.transform(X_test[IMU_COLUMNS])\n",
    "\n",
    "\treturn X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Mod proposal_: **Activity-Based Splitting**\n",
    "\n",
    "> In novelty detection, you want to detect unseen patterns. If the same subject appears in both train and test, the model learns subject-specific characteristics, which won't generalize to new users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: Path) -> tuple[DataFrame, DataFrame]:\n",
    "\tdata, labels = [], []\n",
    "\tfor df, subject in [  # all protocol files\n",
    "\t\tread_w_log(path, filename)\n",
    "\t\tfor filename in os.listdir(path)\n",
    "\t\tif filename.endswith(\".dat\")\n",
    "\t]:  # droping rope jumping (24) cause only subject 9 does this activity\n",
    "\t\tdf = handle_nans(df[~df[\"activity\"].isin([0, 24])])\n",
    "\t\tdf[\"subject\"] = str(subject)\n",
    "\t\tdf[\"timestamp\"] = to_datetime(df[\"timestamp\"], unit=\"s\").dt.time\n",
    "\t\tdf[\"id\"] = df[\"subject\"] + \"_\" + df[\"timestamp\"].astype(str)\n",
    "\n",
    "\t\tdata.append(df.drop(columns=[\"activity\", \"heart_rate\"]))\n",
    "\t\tlabels.append(df[[\"id\", \"activity\"]])  # Index & Activity\n",
    "\n",
    "\tdata, labels = concat(data), concat(labels)\n",
    "\tdata[\"subject\"] = data[\"subject\"].astype(\"category\")\n",
    "\tlabels[\"activity\"] = labels[\"activity\"].astype(\"category\")\n",
    "\n",
    "\treturn data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: subject109.dat\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>IMU_hand_temp_C</th>\n",
       "      <th>IMU_hand_acc16g_ms^-2_x</th>\n",
       "      <th>IMU_hand_acc16g_ms^-2_y</th>\n",
       "      <th>IMU_hand_acc16g_ms^-2_z</th>\n",
       "      <th>IMU_hand_gyro_rad/s_x</th>\n",
       "      <th>IMU_hand_gyro_rad/s_y</th>\n",
       "      <th>IMU_hand_gyro_rad/s_z</th>\n",
       "      <th>IMU_hand_mag_ŒºT_x</th>\n",
       "      <th>IMU_hand_mag_ŒºT_y</th>\n",
       "      <th>IMU_hand_mag_ŒºT_z</th>\n",
       "      <th>IMU_chest_temp_C</th>\n",
       "      <th>IMU_chest_acc16g_ms^-2_x</th>\n",
       "      <th>IMU_chest_acc16g_ms^-2_y</th>\n",
       "      <th>IMU_chest_acc16g_ms^-2_z</th>\n",
       "      <th>IMU_chest_gyro_rad/s_x</th>\n",
       "      <th>IMU_chest_gyro_rad/s_y</th>\n",
       "      <th>IMU_chest_gyro_rad/s_z</th>\n",
       "      <th>IMU_chest_mag_ŒºT_x</th>\n",
       "      <th>IMU_chest_mag_ŒºT_y</th>\n",
       "      <th>IMU_chest_mag_ŒºT_z</th>\n",
       "      <th>IMU_ankle_temp_C</th>\n",
       "      <th>IMU_ankle_acc16g_ms^-2_x</th>\n",
       "      <th>IMU_ankle_acc16g_ms^-2_y</th>\n",
       "      <th>IMU_ankle_acc16g_ms^-2_z</th>\n",
       "      <th>IMU_ankle_gyro_rad/s_x</th>\n",
       "      <th>IMU_ankle_gyro_rad/s_y</th>\n",
       "      <th>IMU_ankle_gyro_rad/s_z</th>\n",
       "      <th>IMU_ankle_mag_ŒºT_x</th>\n",
       "      <th>IMU_ankle_mag_ŒºT_y</th>\n",
       "      <th>IMU_ankle_mag_ŒºT_z</th>\n",
       "      <th>subject</th>\n",
       "      <th>id</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:37.660000</td>\n",
       "      <td>30.375</td>\n",
       "      <td>2.21530</td>\n",
       "      <td>8.27915</td>\n",
       "      <td>5.58753</td>\n",
       "      <td>-0.004750</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>-0.011145</td>\n",
       "      <td>8.93200</td>\n",
       "      <td>-67.9326</td>\n",
       "      <td>-19.9755</td>\n",
       "      <td>32.1875</td>\n",
       "      <td>0.124482</td>\n",
       "      <td>9.65003</td>\n",
       "      <td>-1.65181</td>\n",
       "      <td>0.036668</td>\n",
       "      <td>0.016559</td>\n",
       "      <td>-0.052791</td>\n",
       "      <td>0.567566</td>\n",
       "      <td>-50.7269</td>\n",
       "      <td>44.2728</td>\n",
       "      <td>30.75</td>\n",
       "      <td>9.73855</td>\n",
       "      <td>-1.84761</td>\n",
       "      <td>0.095156</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>-0.027714</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>-61.1081</td>\n",
       "      <td>-36.8636</td>\n",
       "      <td>-58.3696</td>\n",
       "      <td>01</td>\n",
       "      <td>01_00:00:37.660000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00:37.670000</td>\n",
       "      <td>30.375</td>\n",
       "      <td>2.29196</td>\n",
       "      <td>7.67288</td>\n",
       "      <td>5.74467</td>\n",
       "      <td>-0.171710</td>\n",
       "      <td>0.025479</td>\n",
       "      <td>-0.009538</td>\n",
       "      <td>9.58300</td>\n",
       "      <td>-67.9584</td>\n",
       "      <td>-20.9091</td>\n",
       "      <td>32.1875</td>\n",
       "      <td>0.200711</td>\n",
       "      <td>9.64980</td>\n",
       "      <td>-1.65043</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>-0.024304</td>\n",
       "      <td>-0.059843</td>\n",
       "      <td>0.904990</td>\n",
       "      <td>-50.5080</td>\n",
       "      <td>43.5427</td>\n",
       "      <td>30.75</td>\n",
       "      <td>9.69762</td>\n",
       "      <td>-1.88438</td>\n",
       "      <td>-0.020804</td>\n",
       "      <td>0.020882</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>-60.8916</td>\n",
       "      <td>-36.3197</td>\n",
       "      <td>-58.3656</td>\n",
       "      <td>01</td>\n",
       "      <td>01_00:00:37.670000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00:37.680000</td>\n",
       "      <td>30.375</td>\n",
       "      <td>2.29090</td>\n",
       "      <td>7.14240</td>\n",
       "      <td>5.82342</td>\n",
       "      <td>-0.238241</td>\n",
       "      <td>0.011214</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>9.05516</td>\n",
       "      <td>-67.4017</td>\n",
       "      <td>-19.5083</td>\n",
       "      <td>32.1875</td>\n",
       "      <td>0.270277</td>\n",
       "      <td>9.72331</td>\n",
       "      <td>-1.88174</td>\n",
       "      <td>-0.001428</td>\n",
       "      <td>0.038466</td>\n",
       "      <td>-0.046464</td>\n",
       "      <td>0.455480</td>\n",
       "      <td>-50.7209</td>\n",
       "      <td>44.0259</td>\n",
       "      <td>30.75</td>\n",
       "      <td>9.69633</td>\n",
       "      <td>-1.92203</td>\n",
       "      <td>-0.059173</td>\n",
       "      <td>-0.035392</td>\n",
       "      <td>-0.052422</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>-60.3407</td>\n",
       "      <td>-35.7842</td>\n",
       "      <td>-58.6119</td>\n",
       "      <td>01</td>\n",
       "      <td>01_00:00:37.680000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00:37.690000</td>\n",
       "      <td>30.375</td>\n",
       "      <td>2.21800</td>\n",
       "      <td>7.14365</td>\n",
       "      <td>5.89930</td>\n",
       "      <td>-0.192912</td>\n",
       "      <td>0.019053</td>\n",
       "      <td>0.013374</td>\n",
       "      <td>9.92698</td>\n",
       "      <td>-67.4387</td>\n",
       "      <td>-20.5602</td>\n",
       "      <td>32.1875</td>\n",
       "      <td>0.236737</td>\n",
       "      <td>9.72447</td>\n",
       "      <td>-1.72746</td>\n",
       "      <td>0.017277</td>\n",
       "      <td>-0.048547</td>\n",
       "      <td>-0.074946</td>\n",
       "      <td>0.324284</td>\n",
       "      <td>-50.1544</td>\n",
       "      <td>43.6570</td>\n",
       "      <td>30.75</td>\n",
       "      <td>9.66370</td>\n",
       "      <td>-1.84714</td>\n",
       "      <td>0.094385</td>\n",
       "      <td>-0.032514</td>\n",
       "      <td>-0.018844</td>\n",
       "      <td>0.026950</td>\n",
       "      <td>-60.7646</td>\n",
       "      <td>-37.1028</td>\n",
       "      <td>-57.8799</td>\n",
       "      <td>01</td>\n",
       "      <td>01_00:00:37.690000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:00:37.700000</td>\n",
       "      <td>30.375</td>\n",
       "      <td>2.30106</td>\n",
       "      <td>7.25857</td>\n",
       "      <td>6.09259</td>\n",
       "      <td>-0.069961</td>\n",
       "      <td>-0.018328</td>\n",
       "      <td>0.004582</td>\n",
       "      <td>9.15626</td>\n",
       "      <td>-67.1825</td>\n",
       "      <td>-20.0857</td>\n",
       "      <td>32.1875</td>\n",
       "      <td>0.352225</td>\n",
       "      <td>9.72437</td>\n",
       "      <td>-1.68665</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>-0.013352</td>\n",
       "      <td>-0.039315</td>\n",
       "      <td>0.462317</td>\n",
       "      <td>-50.7110</td>\n",
       "      <td>42.9228</td>\n",
       "      <td>30.75</td>\n",
       "      <td>9.77578</td>\n",
       "      <td>-1.88582</td>\n",
       "      <td>0.095775</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>-0.048878</td>\n",
       "      <td>-0.006328</td>\n",
       "      <td>-60.2040</td>\n",
       "      <td>-37.1225</td>\n",
       "      <td>-57.8847</td>\n",
       "      <td>01</td>\n",
       "      <td>01_00:00:37.700000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp  IMU_hand_temp_C  IMU_hand_acc16g_ms^-2_x  \\\n",
       "0  00:00:37.660000           30.375                  2.21530   \n",
       "1  00:00:37.670000           30.375                  2.29196   \n",
       "2  00:00:37.680000           30.375                  2.29090   \n",
       "3  00:00:37.690000           30.375                  2.21800   \n",
       "4  00:00:37.700000           30.375                  2.30106   \n",
       "\n",
       "   IMU_hand_acc16g_ms^-2_y  IMU_hand_acc16g_ms^-2_z  IMU_hand_gyro_rad/s_x  \\\n",
       "0                  8.27915                  5.58753              -0.004750   \n",
       "1                  7.67288                  5.74467              -0.171710   \n",
       "2                  7.14240                  5.82342              -0.238241   \n",
       "3                  7.14365                  5.89930              -0.192912   \n",
       "4                  7.25857                  6.09259              -0.069961   \n",
       "\n",
       "   IMU_hand_gyro_rad/s_y  IMU_hand_gyro_rad/s_z  IMU_hand_mag_ŒºT_x  \\\n",
       "0               0.037579              -0.011145            8.93200   \n",
       "1               0.025479              -0.009538            9.58300   \n",
       "2               0.011214               0.000831            9.05516   \n",
       "3               0.019053               0.013374            9.92698   \n",
       "4              -0.018328               0.004582            9.15626   \n",
       "\n",
       "   IMU_hand_mag_ŒºT_y  IMU_hand_mag_ŒºT_z  IMU_chest_temp_C  \\\n",
       "0           -67.9326           -19.9755           32.1875   \n",
       "1           -67.9584           -20.9091           32.1875   \n",
       "2           -67.4017           -19.5083           32.1875   \n",
       "3           -67.4387           -20.5602           32.1875   \n",
       "4           -67.1825           -20.0857           32.1875   \n",
       "\n",
       "   IMU_chest_acc16g_ms^-2_x  IMU_chest_acc16g_ms^-2_y  \\\n",
       "0                  0.124482                   9.65003   \n",
       "1                  0.200711                   9.64980   \n",
       "2                  0.270277                   9.72331   \n",
       "3                  0.236737                   9.72447   \n",
       "4                  0.352225                   9.72437   \n",
       "\n",
       "   IMU_chest_acc16g_ms^-2_z  IMU_chest_gyro_rad/s_x  IMU_chest_gyro_rad/s_y  \\\n",
       "0                  -1.65181                0.036668                0.016559   \n",
       "1                  -1.65043                0.019343               -0.024304   \n",
       "2                  -1.88174               -0.001428                0.038466   \n",
       "3                  -1.72746                0.017277               -0.048547   \n",
       "4                  -1.68665                0.000275               -0.013352   \n",
       "\n",
       "   IMU_chest_gyro_rad/s_z  IMU_chest_mag_ŒºT_x  IMU_chest_mag_ŒºT_y  \\\n",
       "0               -0.052791            0.567566            -50.7269   \n",
       "1               -0.059843            0.904990            -50.5080   \n",
       "2               -0.046464            0.455480            -50.7209   \n",
       "3               -0.074946            0.324284            -50.1544   \n",
       "4               -0.039315            0.462317            -50.7110   \n",
       "\n",
       "   IMU_chest_mag_ŒºT_z  IMU_ankle_temp_C  IMU_ankle_acc16g_ms^-2_x  \\\n",
       "0             44.2728             30.75                   9.73855   \n",
       "1             43.5427             30.75                   9.69762   \n",
       "2             44.0259             30.75                   9.69633   \n",
       "3             43.6570             30.75                   9.66370   \n",
       "4             42.9228             30.75                   9.77578   \n",
       "\n",
       "   IMU_ankle_acc16g_ms^-2_y  IMU_ankle_acc16g_ms^-2_z  IMU_ankle_gyro_rad/s_x  \\\n",
       "0                  -1.84761                  0.095156                0.002908   \n",
       "1                  -1.88438                 -0.020804                0.020882   \n",
       "2                  -1.92203                 -0.059173               -0.035392   \n",
       "3                  -1.84714                  0.094385               -0.032514   \n",
       "4                  -1.88582                  0.095775                0.001351   \n",
       "\n",
       "   IMU_ankle_gyro_rad/s_y  IMU_ankle_gyro_rad/s_z  IMU_ankle_mag_ŒºT_x  \\\n",
       "0               -0.027714                0.001752            -61.1081   \n",
       "1                0.000945                0.006007            -60.8916   \n",
       "2               -0.052422               -0.004882            -60.3407   \n",
       "3               -0.018844                0.026950            -60.7646   \n",
       "4               -0.048878               -0.006328            -60.2040   \n",
       "\n",
       "   IMU_ankle_mag_ŒºT_y  IMU_ankle_mag_ŒºT_z subject                  id activity  \n",
       "0            -36.8636            -58.3696      01  01_00:00:37.660000        1  \n",
       "1            -36.3197            -58.3656      01  01_00:00:37.670000        1  \n",
       "2            -35.7842            -58.6119      01  01_00:00:37.680000        1  \n",
       "3            -37.1028            -57.8799      01  01_00:00:37.690000        1  \n",
       "4            -37.1225            -57.8847      01  01_00:00:37.700000        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = load_data(Path(\"../data/PAMAP2_Dataset/Protocol/\"))\n",
    "\n",
    "data.to_csv(\"../data/PAMAP2/data.csv\", index=False)\n",
    "labels.to_csv(\"../data/PAMAP2/labels.csv\", index=False)\n",
    "\n",
    "df = data.merge(labels, how=\"left\", on=\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_distinct_activities(\n",
    "\tdata: DataFrame,\n",
    "\tlabels: DataFrame,\n",
    "\tn_activities: int = 3,\n",
    "\tmethod: Literal[\"pca\", \"statistical\", \"variance\"] = \"pca\",\n",
    ") -> list[int]:\n",
    "\t\"\"\"\n",
    "\tSelect the most distinct activities based on pairwise feature distances.\n",
    "\n",
    "\tUses a greedy algorithm to maximize separation between selected activities:\n",
    "\t1. Start with the two activities that are furthest apart\n",
    "\t2. If selecting 3, add the activity with maximum minimum distance to the first two\n",
    "\n",
    "\tArgs:\n",
    "\t\tdata: Feature DataFrame\n",
    "\t\tlabels: Labels DataFrame with 'id' and 'activity' columns\n",
    "\t\tn_activities: Number of activities to select (2 or 3)\n",
    "\t\tmethod: Method to use for activity representation:\n",
    "\t\t\t- 'pca': Use principal components (captures main movement patterns)\n",
    "\t\t\t- 'statistical': Use mean, std, and quartiles (robust statistics)\n",
    "\t\t\t- 'variance': Use variance and energy (good for dynamic activities)\n",
    "\n",
    "\tReturns:\n",
    "\t\tList of activity IDs representing the most distinct activities.\n",
    "\n",
    "\tRaises:\n",
    "\t\tValueError: If method is not one of 'pca', 'statistical', or 'variance'.\n",
    "\t\"\"\"\n",
    "\tprint(f\"\\nüîç Selecting {n_activities} most distinct activities using '{method}'...\")\n",
    "\tdf = data.merge(labels, how=\"left\", on=\"id\")\n",
    "\n",
    "\tactivity_stats = {}\n",
    "\tfor activity in df[\"activity\"].unique():\n",
    "\t\tactivity_data = df[df[\"activity\"] == activity][IMU_COLUMNS]\n",
    "\t\tif method == \"pca\":  # Reduce dimensionality and capture main characteristics\n",
    "\t\t\tactivity_stats[activity] = (\n",
    "\t\t\t\tPCA(n_components=min(10, len(IMU_COLUMNS), len(activity_data)))\n",
    "\t\t\t\t.fit_transform(activity_data)  # PCA components as activity signature\n",
    "\t\t\t\t.mean(axis=0)\n",
    "\t\t\t)\n",
    "\t\telif method == \"statistical\":\n",
    "\t\t\tactivity_stats[activity] = concatenate(\n",
    "\t\t\t\t[  # Statistical features (more robust to outliers)\n",
    "\t\t\t\t\tactivity_data.mean().values,\n",
    "\t\t\t\t\tactivity_data.std().values,\n",
    "\t\t\t\t\tactivity_data.quantile(0.25).values,\n",
    "\t\t\t\t\tactivity_data.quantile(0.75).values,\n",
    "\t\t\t\t]\n",
    "\t\t\t)\n",
    "\t\telif method == \"variance\":\n",
    "\t\t\tactivity_stats[activity] = concatenate(\n",
    "\t\t\t\t[  # Variance and energy characteristics (good for dynamic activities)\n",
    "\t\t\t\t\tactivity_data.var().values,\n",
    "\t\t\t\t\tactivity_data.abs().mean().values,  # Mean absolute value (energy)\n",
    "\t\t\t\t]\n",
    "\t\t\t)\n",
    "\tactivities = list(activity_stats.keys())\n",
    "\tdistances = squareform(  # Calculate pairwise distances between activities\n",
    "\t\tpdist(vstack([activity_stats[act] for act in activities]), metric=\"euclidean\")\n",
    "\t)\n",
    "\tselected_indices = []  # Greedy selection algorithm\n",
    "\t# Start with the pair that has maximum distance\n",
    "\tmax_dist_idx = unravel_index(distances.argmax(), distances.shape)\n",
    "\tselected_indices.extend([max_dist_idx[0], max_dist_idx[1]])\n",
    "\t# If a third activity is needed, select the one with maximum minimum distance\n",
    "\tif n_activities == 3:\n",
    "\t\tremaining_indices = [\n",
    "\t\t\ti for i in range(len(activities)) if i not in selected_indices\n",
    "\t\t]\n",
    "\t\t# Select the activity with maximum minimum distance (most distinct from both)\n",
    "\t\tselected_indices.append(\n",
    "\t\t\tremaining_indices[\n",
    "\t\t\t\targmax(\n",
    "\t\t\t\t\t[  # Find minimum distance to any already selected activity\n",
    "\t\t\t\t\t\tmin(distances[idx, sel_idx] for sel_idx in selected_indices)\n",
    "\t\t\t\t\t\tfor idx in remaining_indices\n",
    "\t\t\t\t\t]\n",
    "\t\t\t\t)\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\tprint(f\"\\n‚úÖ Selected {n_activities} most distinct activities:\")\n",
    "\tfor i, act in enumerate(\n",
    "\t\tselected_activities := [activities[i] for i in selected_indices], 1\n",
    "\t):\n",
    "\t\tprint(f\"  {i}. Activity {act}: {(df['activity'] == act).sum():,} samples\")\n",
    "\n",
    "\tprint(\"\\nüìè Pairwise distances between selected activities:\")\n",
    "\tfor i in range(len(selected_indices)):\n",
    "\t\tfor j in range(i + 1, len(selected_indices)):\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf\"Activity {selected_activities[i]} ‚Üî Activity {selected_activities[j]}\"\n",
    "\t\t\t\tf\": {distances[selected_indices[i], selected_indices[j]]:.2f}\"\n",
    "\t\t\t)\n",
    "\treturn selected_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_based_split(\n",
    "\tdata: DataFrame,\n",
    "\tlabels: DataFrame,\n",
    "\ttest_activities: list[int] | None = None,\n",
    "\tn_distinct: int = 2,\n",
    "\tmethod: Literal[\"pca\", \"statistical\", \"variance\"] = \"pca\",\n",
    "\tval_size: float = 0.2,\n",
    ") -> tuple[DataFrame, DataFrame, DataFrame, DataFrame, DataFrame, DataFrame]:\n",
    "\t\"\"\"\n",
    "\tSplit data into train/val/test sets based on activities for novelty detection.\n",
    "\n",
    "\tStrategy:\n",
    "\t- Test set: Contains ONLY the most distinct activities (novel/unseen)\n",
    "\t- Train set: Contains all other activities (normal behavior)\n",
    "\t- Val set: Last 20% of train set in temporal order (time-series split)\n",
    "\n",
    "\tThis ensures the model is evaluated on truly novel activities during testing.\n",
    "\n",
    "\tArgs:\n",
    "\t\tdata: Feature DataFrame\n",
    "\t\tlabels: Labels DataFrame with 'id' and 'activity' columns\n",
    "\t\ttest_activities: Optional list of activity IDs to use as test set.\n",
    "\t\t\t\tIf None, automatically selects using distinctiveness algorithm.\n",
    "\t\tn_distinct: Number of activities to auto-select (if test_activities=None)\n",
    "\t\tmethod: How to calculate distinctiveness. 'pca', 'statistical' or 'variance'\n",
    "\t\tval_size: Proportion of train data to use for validation (e.g., 0.2 = 20%)\n",
    "\n",
    "\tReturns:\n",
    "\t\ttuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\t\"\"\"\n",
    "\tif test_activities is None:  # Auto-select distinct activities if not provided\n",
    "\t\ttest_activities = select_most_distinct_activities(\n",
    "\t\t\tdata, labels, n_activities=n_distinct, method=method\n",
    "\t\t)\n",
    "\tdf = data.merge(labels, how=\"left\", on=\"id\")  # Merge data and labels\n",
    "\ttest_mask = df[\"activity\"].isin(test_activities)  # Split based on activities\n",
    "\t# Use temporal split for train/val: last 20% becomes validation\n",
    "\ttrain_val_df = df[~test_mask].copy()\n",
    "\tsplit_idx = int(len(train_val_df) * (1 - val_size))\n",
    "\n",
    "\ttrain_df = train_val_df.iloc[:split_idx].copy()\n",
    "\tval_df = train_val_df.iloc[split_idx:].copy()\n",
    "\ttest_df = df[test_mask].copy()\n",
    "\t# Separate features and labels\n",
    "\tfeature_cols = [col for col in data.columns if col != \"id\"]\n",
    "\n",
    "\treturn (\n",
    "\t\ttrain_df[feature_cols].reset_index(drop=True),\n",
    "\t\tval_df[feature_cols].reset_index(drop=True),\n",
    "\t\ttest_df[feature_cols].reset_index(drop=True),\n",
    "\t\ttrain_df[[\"id\", \"activity\"]].reset_index(drop=True),\n",
    "\t\tval_df[[\"id\", \"activity\"]].reset_index(drop=True),\n",
    "\t\ttest_df[[\"id\", \"activity\"]].reset_index(drop=True),\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Selecting 3 most distinct activities using 'pca'...\n",
      "\n",
      "‚úÖ Selected 3 most distinct activities:\n",
      "  1. Activity 7: 188,107 samples\n",
      "  2. Activity 6: 164,600 samples\n",
      "  3. Activity 12: 117,216 samples\n",
      "\n",
      "üìè Pairwise distances between selected activities:\n",
      "Activity 7 ‚Üî Activity 6: 0.00\n",
      "Activity 7 ‚Üî Activity 12: 0.00\n",
      "Activity 6 ‚Üî Activity 12: 0.00\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = activity_based_split(\n",
    "\tdata=data, labels=labels, n_distinct=3, method=\"pca\", val_size=0.2\n",
    ")\n",
    "# Normalize features using training data statistics only\n",
    "X_train, X_val, X_test = normalize_features(X_train, X_val, X_test)\n",
    "\n",
    "output_dir = Path(\"../data/PAMAP2/splits/\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "X_train.to_csv(output_dir / \"X_train.csv\", index=False)\n",
    "X_val.to_csv(output_dir / \"X_val.csv\", index=False)\n",
    "X_test.to_csv(output_dir / \"X_test.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(output_dir / \"y_train.csv\", index=False)\n",
    "y_val.to_csv(output_dir / \"y_val.csv\", index=False)\n",
    "y_test.to_csv(output_dir / \"y_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
